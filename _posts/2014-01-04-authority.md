---
layout: post
title: "I and the Story of Authority"
---


Every person doesn't have authority inside them as children, just like every person doesn't believe in advertisements when they're children. But authority is like advertising, it's hidden in your subconscious. You may not believe it after listening once, or twice,...., until you've listened a thousand times, and suddenly it starts to work, and the effect gets stronger.

The best way to dispel the illusion created by advertisements is to try and examine it directly. Some illusory things will crumble like soap bubbles when you try them for the first time. But if you don't actively engage with it, it will continue to create a beautiful, sacred illusion in your mind. The more inaccessible it is, the more beautiful it seems. A fascinating phenomenon is that the influence of authority on people's thoughts is similar to that of advertisements.

Before going to college, people don't idolize authority much because they don't have a professional field, so they chase after singers, actors, and athletes. But after entering college, they start to idolize the authorities in their field. They hear the names of certain "geniuses" or "masters" being worshipped by their classmates, even though they've never met them in person, and unconsciously develop a reverence for this person, then feel ashamed of themselves. Unconsciously, they start to agree with these statements, and unknowingly mention the names of these masters and quote their words as their own guiding principles.

Donald Knuth, Dennis Ritchie, Ken Thompson, Rob Pike, ... became authorities for many computer science students through these channels. And decades later, their historical legacies, including their poor designs and incorrect ideas, are still revered as sacred by many people. I, like many others (including myself), have had deep reverence for Knuth and his "The Art of Computer Programming" (TAOCP) during my college and graduate school years. Some classmates spent a lot of money buying the expensive sets of TAOCP in three volumes, saying they probably wouldn't read it but wanted it on their shelves as a status symbol. Following the principle "a book that cannot be borrowed cannot be read," and considering that moving books is the most laborious task, I decided not to buy it. Instead, I borrowed TAOCP from the library. To be honest, I couldn't get past the part where the programs were written in MIX processor assembly language and filled with mathematical formulas and proofs. However, I kept hearing stories about people who had actually read through TAOCP and became masters. Bill Gates even declared, "If anyone has read through TAOCP, please send your resume to me!" In the face of such calls and encouragement, I borrowed TAOCP several times, determined to finish it this time. Each time, I started with great enthusiasm, but I never managed to get past the initial section on MIX machine language and mathematical formulas.

I couldn't understand TAOCP and felt like a failure because not understanding TAOCP meant I couldn't become a "master," but I still believed that Knuth was a god in computer science and that I could learn something from him. That's why I started experimenting with his other works. I borrowed Knuth's TeXbook, read it from cover to cover twice, completed all the exercises, including the most difficult "double bend" exercises. Then I started learning MetaPost for graphics using MetaFont Book. I was quite proud of my achievements at the beginning, but I soon found that the TeX techniques I had learned didn't help me when I needed them. I forgot everything I had learned. I read TeXbook twice but still forgot most of it.

My seniors and seniors laughed at me for messing with such outdated technology. I was angry at them and the academic community in China for using Word to write papers. I wrote a series of articles promoting TeX and criticizing "what-you-see-is-what-you-get" (WYSIWYG) software. I even started experimenting with Knuth's MMIX processor design and believed that MMIX's register file was the most advanced design in the world. I found some insignificant errors and emailed Knuth, who sent me two legendary "Knuth checks." I was thrilled and even considered cashing them in, but I didn't go that far. I kept the checks in their original envelopes. Years later, when I wanted to cash in the checks in the United States, I found that they had expired.

Once you have this kind of authority in your heart, no one else's words can penetrate it, even if they are actually wiser than you in reality. During my time at Tsinghua, I sometimes attended talks given by visiting professors from foreign universities. One time, a professor from a famous American university came to give a talk on algorithms. I somehow started talking to him about TAOCP, probably hoping to learn how to study algorithms from him. He told me that Knuth's books were outdated and suggested I read MIT's "Algorithmic Algorithms." But I didn't listen to him, still believing that TAOCP held the highest secrets in the algorithm community, the eternal treasure. I enjoyed a course called "Computational Geometry" at Tsinghua University. I often exchanged ideas with its teacher. Once in an email, I mentioned that Donald Knuth was my idol. The teacher replied in a polite way: "Having an idol is good. Knuth was once my idol too." I was surprised about the word "once": Did this mean Knuth was no longer his idol? After my persistent questioning, he told me that there were many smarter people in the world, and Knuth was not the be-all and end-all of computer science. He suggested I should look into other people's work, especially mathematicians'.

Looking back, these words had a profound impact on me. Although this teacher was considered a weak researcher in the eyes of the "giants" in the department, he had a significant impact on my life. He guided me to pursue my true interests instead of chasing after fame. I found that many people were doing things not because they were interested, but because others thought they were impressive or "cool." I hope they encountered teachers like mine.

Now, looking back, Knuth's TAOCP is what people call a "white elephant" (white elephant). Everyone keeps it around, but few actually read it, pretending they have. This makes those who try to understand it feel inadequate and anxious, even questioning their intelligence. "Why can't I understand it if everyone else can?" In reality, most of the algorithms in TAOCP were not designed by Knuth himself, and his explanations of others' algorithms often made simple problems seem complex. Additionally, his insistence on using assembly language made understanding the programs even more difficult.

There's a saying: "Learn from the true masters, not their disciples." If you really want to learn an algorithm, you should go directly to the inventor's research papers, not second-hand knowledge. Second-hand knowledge often strips away the inventor's motivations and thought processes, leaving only a bland, uninspiring "final result." Years later, when I saw the new volumes of Knuth's TAOCP plans, I found that I had learned most of the content through easier methods, as I had found the sources of this knowledge.

So, the professor who visited Tsinghua was speaking the truth: Knuth had fallen from grace, but few in America knew or acknowledged this. During an interview with some of the world's acknowledged great programmers, including well-known giants and ML's designer Robin Milner, Haskell's designer Simon Peyton Jones, among others, the interviewer asked each person, "Have you read TAOCP?" Most replied that it was an impressive and important work, but Robin Milner (if I remember correctly) was the most amusing. He said he wished he had read it, but unfortunately, he didn't have the time. I kept TAOCP under my monitor so I could always look at it while working.: Knuth stated "premature optimization is the root of all evil", yet he himself was quite fond of using premature optimization. His code was filled with mysterious tricks and intricacies, making it hard to understand, yet it didn't result in significant performance gains. I once encountered an odd piece of code in a MMIX processor simulator, which Knuth used to count the number of "1's" in a register. Instead of writing a simple loop or using the common method of subtracting one from the least significant bit, he employed a strange trick from Programming Pearls, which took me hours to comprehend, only to find out later that it was less effective than the simplest method. Such intricate yet obscure techniques obscured the global view of Knuth's code, leaving me confused about how a processor simulator should function. It wasn't until I studied programming language theory at Indiana that I realized the simplicity of processor simulators (and processors themselves), as they essentially act as machine code interpreters. Using a similar structure as high-level language interpreters, it's relatively easy to write simulators like MMIX.

Knuth's most significant contribution was likely to programming language parsing (syntax analysis), such as LR parsing. However, parsing is essentially a non-issue in terms of human-made problems. It exists due to misunderstandings, as people believed that programming languages needed human-like syntax, making them unnecessarily complex and difficult. If you simplify the syntax, there's no need for LR, LALR, or any other complex parsing techniques. I recently wrote a parser for a language I designed, which took only two hours and consisted of 500 lines of Java code, from lexer to AST data structure. It was entirely handwritten code, without using any complex parsing techniques or YACC-like tools, even avoiding regular expressions. This was possible because my syntax design made parsing effortless, simpler than Lisp. Knuth overemphasized parsing. His misguided emphasis led many to spend decades researching parsing, introducing new techniques to create increasingly complex syntax. Why? This only makes programming and compilers more painful. If these people had devoted their time to genuine problems, computer science would be much more beautiful today.

Nearly every compiler textbook dedicates extensive space to discussing DFA, NFA, lexing, LL, LR, LALR, and so on. Nearly every compiler course in school spends at least 30% of the time on parsing, tinkering with LEX, YACC, and other tools, while neglecting the essential aspects of compilers. This is why Kent Dybvig's compiler course was so effective, as Scheme's syntax is extremely simple, and we didn't spend time on parsing. Instead, we focused on optimization, tail recursion, higher-order functions, and other issues. Many desired features of languages remain unrealized. Such courses allowed me to showcase my potential, as my compiler contained numerous innovative approaches, and my X64 machine code generator produced incredibly compact code, leaving Kent Dybvig puzzled. These advancements might still be the most cutting-edge technology today.

A person's thought process seems to determine the design of all their creations. Knuth's other significant invention, Literate Programming (Literate Programming), was unnecessary and created confusion. The error in Literate Programming lies in assuming that programming languages should be like human languages and adapt to so-called "human thinking." However, programming languages are superior in many aspects to human languages and should not be influenced by their shortcomings. Splitting programs into separate sections of literary text made it difficult to understand the relationships between the code, and it was easy to make mistakes. This error is similar to the flaw in the "Unix philosophy," treating programs as lines of text instead of data structures akin to circuit diagrams. I won't delve deeper into this issue here, as I have written a separate article explaining why Literate Programming is not a good idea.

TeX is exceptionally poorly designed. Its excessive complexity makes it difficult for most people to understand how to configure it. Often, simple effects require significant effort, only to be forgotten shortly after, forcing repeated efforts to relearn the process. The reason for this is TeX's lack of consistency, numerous special cases, and weak composition abilities. As a result, you need to learn a great deal, while a game like chess only requires learning a few simple rules that can be combined to create infinite possibilities. In the view of programming language designers, TeX's language is one of the worst designs in the world, but without it, it might be even worse. The reason TeX has an "extension language" is due to a little-known story about Scheme's inventor Guy Steele. In the early stages of TeX design, Knuth's TeX didn't have a language. It was due to Steele's suggestion that Knuth design a language to handle future extension issues. Under Steele's strong persuasion and flattery, Knuth adopted this suggestion. Unfortunately, Steele couldn't directly participate in the language's design, leaving after a short summer stint at Stanford.

Knuth's works contain his contributions and value, and TeX's typesetting algorithms (not the language) may still be decent things. But if we revere his chaotic designs as sacred, our own designs will be trapped in the same thought patterns, making simple things complex. Those still in awe of TeX should check out TeXmacs and its creator, who silently surpassed TeX and Knuth.

In my opinion, Knuth is a typical elitist. He believed everything he did was the best, most refined. He used his authority and independence to make users submit to his complex design, rather than designing tools that were more user-friendly. The version number of TeX updates approaches pi, meaning it's perfect, without bugs. He rewards large checks to those who find bugs in TeX code, showing off his confidence in the code, but he "froze" TeX's code, refusing to add anything new or simplify its design. Of course, if the code isn't changed, there won't be new bugs, but its design remains stagnant, stuck in the past. It's even more strange that "TeX" is pronounced differently than regular English pronunciation. Whenever someone mispronounces it, "experts" smirk inside, considering you a novice, and then correct you: "That word isn't pronounced 'teks,' but 'tech,' like the Greek chi, or Scottish loch, German ach, Spanish j, or Russian kh." Maybe this is called "affected elegance," I'm a purebred European!;-) When a software's name is so awkwardly pronounced and hard to master, what will using it be like? Every time you say TeX is too obscure, someone corrects you: "TeX is all-encompassing, much better than your 'what-you-see-is-what-you-get'!" Is that really true? Check out TeXmacs and understand what "what-you-see-is-what-you-get+what-you-think-is-what-you-get" means.

My last contact with Knuth was when I was about to leave Tsinghua. I emailed him, expressing my concern about the chaotic research environment in China and seeking his advice. To my surprise, he replied with a letter saying: "Why do I see so many outstanding research from Chinese scholars? Computer science isn't for everyone. If you've tried for so long and still can't make it, then you're not cut out for this field." Fortunately, I never believed his words, and I've spent years proving him wrong. Today, I can confidently say, Knuth, you were wrong, because I've surpassed you in several areas you're famous for.

### Unix

Unix is a widely used multi-user, multi-tasking operating system. It was developed at the Bell Laboratories by Ken Thompson and Dennis Ritchie in the late 1960s and early 1970s. Unix's design philosophy emphasizes simplicity, modularity, and flexibility. It has had a significant impact on the development of modern operating systems and computer networking. Unix's popularity led to the creation of various derivatives, such as Linux and macOS, which have further extended its reach and influence. Unix's influence can be seen in various aspects of modern computing, including file systems, shell scripting, and system administration tools.makers of Unix are similar in authority to Knuth in my mind, occupying important positions that led me to write an article ten years ago called "Working Completely with Linux," enthusiastically promoting Unix's "philosophy," even pointing out that Linux cannot do certain things that are unnecessary. I also introduced numerous difficult Unix tools, leading many people to tinker. However, if you knew my current attitude towards Unix, you would be surprised, as I have successfully "forgotten" almost everything about Unix, to the point that new graduates would think I am incompetent and would show off their Linux skills to me. They would not understand that Unix/Linux design is the root cause of many problems in the computer software industry, and what they present to me is just the Unix way of thinking and elitism inflicted on programmers. I do not forget Linux design, but I have unconsciously taken pride in being familiar with its quirks, so I often pretend not to know. I am the master of the machine, not its slave, so I always look for ways to make the machine do more for me, help me remember mundane details, and not be bound by its designers' "philosophy."

Discussing Unix and its descendants is always an awkward situation because mentioning any of their shortcomings will be seen as a positive by many. The meaning of GNU is "GNU is Not Unix," but unfortunately, GNU and Linux design have never fully escaped the Unix mindset. Unix's memory management, processes, threads, shell, inter-process communication, file systems, databases, and so on, are all clumsy designs. The so-called "Unix philosophy," which is inter-process communication primarily through unstructured character strings, led to a multitude of excessively complex tools and languages: AWK, sed, Perl, and so on. Unix's memory management allocates memory in "pages" rather than "structures," which is like entrusting every person with a flintlock gun. But then they want "safety," creating a contradiction. As a result, they were forced to enforce complete isolation of process data spaces, making it impossible for processes to directly exchange data structures. Context switching overhead for processes and threads is high, leading to bottlenecks in large-scale parallel or distributed computing, resulting in the emergence of "workarounds" like goroutines and node.js. Storing unstructured data in files makes it impossible to effectively query data, leading to the development of overly complex relational databases. Additionally, the design of the web is largely a patchwork, with websites being a collection of hacks.

The "Unix philosophy" seems to have good parts, such as "each program should do one thing, and do it well," but this philosophy is actually just modular design in programming languages like Lisp. It's a good thing, but the ideas were stolen by Unix and lacked substance. Few Unix programs truly do one thing, and due to the unreliable nature of string communication, they cannot effectively cooperate. Sometimes switching to a new version of make or sed, your build may mysteriously break. This is why some companies hire so-called "build engineers," as high-level programmers do not want to deal with these issues. Lisp programmers have long understood this, which is why they designed S-expressions for structured data transfer. In reality, S-expressions were not "designed," they were just the simplest way to represent tree structures in code. Lisp's design principles include "do not encode." It means, as much as possible, do not encode useful data into strings. Unix's world of tinkering led to XML, CORBA, and eventually JSON, but in reality, JSON is far less simple and powerful than S-expressions. Unix is like a brain tumor, making people abandon the best solutions for decades and create unnecessary complexities. These junks have a significant brainwashing effect on people. Recently, I mentioned that S-expressions are simpler than JSON, and someone argued that JSON has "unordered fields." This left me speechless, as the order of an encoding method depends on how you interpret it. From this perspective, S-expressions can be ordered or unordered.

Unix likes to wave the "freedom" and "open source" flags, but its history is filled with politics, religious conflicts, and struggles over "textbooks." Almost all operating system textbooks begin by mentioning Unix's predecessor Multics, which is used to highlight Unix's "simplicity" and greatness, followed by a by-the-book explanation of Unix design, as if Unix is the only operating system in the world. The books will tell you that Multics failed due to its complex design trying to cover too many aspects, but if you delve into Multics' history, you'll find that the last Multics machine was still running in 2000, possessing advanced features that Unix/Linux still lacks, and was beloved by its users. Multics' design was not perfect (compare it to Lisp Machines and Oberon), but compared to Unix, it was far from simple. Unix borrowed the best ideas from Multics, but some were not copied well, and many unnecessary quirks were added. However, Unix's charm lay in its infectious characteristics, allowing it to quickly capture the market. Unix was free and open-source at the beginning, but AT&T discovered potential profits and took back usage rights, even suing many people. AT&T's evilness surpassed that of Microsoft.

Unix's designs are so quirky that many people were forced to use it due to bureaucratic reasons. This led to early complaints about Unix, with some even creating a mailing list called "Unix Haters." You might dismiss these people as novices, but they had used better operating systems and even designed better operating systems and programming languages themselves. Their complaints were eventually compiled into a book called the "Unix Hater's Handbook." It's surprising that the book has an "anti-foreword" written by one of the designers of Unix and C, Dennis Ritchie. This anti-foreword states that the inconsistent design prison of Unix will continue to imprison you, but clever prisoners will find loopholes. Unfortunately, the Free Software Foundation (FSF) will build a compatible prison, just with more features. People with MIT degrees, Microsoft researchers, and Apple's senior scientists may contribute to the prison's "rules." From these rules, I see a boastful despot, raw power, authoritarianism, and legalism. Sadly, in the software world, any poorly designed software can become popular if you have good advertising and enough evangelists. People who are half-informed, like me a decade ago, love to seek out the "novel" and then tout their benefits, becoming its evangelists. Adding to this, the computer science department's tradition of following the market at universities led to an unfortunate situation: Unix and its descendants virtually monopolized the server operating system market. Due to Unix's monopoly, the software world is now built on a pile of hacks and has solidified into "pearls." In companies and schools, there are plenty of people who are praised for knowing some Unix tricks, unaware that they only know how to avoid some clunky designs. Programmers have to remember too many special cases and details, not complaining, but boasting instead. Few people consider how to solve problems fundamentally, and the lessons of history are seldom learned, leading to design errors being repeated decades later. Unix's greatest contribution may have been creating numerous job positions—because the problems are numerous and complicated, requiring a large workforce to maintain their operation.

Now it's clear that Unix initially gained acceptance through the "Emperor's New Clothes" approach. The weavers said, "Fools or incompetents cannot see this cloth." Dennis Ritchie said, "Unix is simple, but only a genius can understand this simplicity." Do you see it now? You dare not say that Unix's design is too chaotic and complex because, as soon as you utter that, someone will quote Dennis and claim that you're not a genius, so you don't understand.

Dennis Ritchie has passed away. May he rest in peace, but some of his devotees were still fanning the flames at that time, comparing his death to Steve Jobs' and circulating such photos everywhere, as if Steve died at the wrong time and stole Dennis' limelight. Then some wrote articles attributing every system and language to Dennis and Unix. I realized then that such "geniuses" are created in this way. This is a ridiculous and fallacious argument, like saying someone made a beautiful garment using a dull pair of scissors, so the scissors deserve the credit. In fact, this person was sewing and cursing the scissors, thinking, "Damn it, make this garment quickly and buy a better pair!"

I've used Apple products for a long time, and while they're not perfect, they're not Unix clones. They made efforts to free themselves from Unix's thought constraints. They followed the principle of "machine for the user," not making the user a machine's slave. Mac's internal design is fundamentally different from Unix's. Yet, these systems were disparaged by Dennis Ritchie in his counter-speech as "systems based on Sonic the Hedgehog as an intelligence theme and interaction design foundation."

Do you know that during the same period, Lisp's inventor John McCarthy and ML's inventor Robin Milner both passed away? I only noticed this when someone posted a brief message on a mailing list, and I silently mourned the inspiration they had given me. We didn't feel that Steve Jobs' death stole their limelight because they didn't need it. Death is about resting peacefully, and the memories of the wise are enough. It's unlikely that Dennis Ritchie himself is to blame for this situation, but his Unix devotees should certainly reflect on their actions.: The designers of Unix once held a prominent place in my heart, but now I feel they represent reactionary forces. They use their influence to perpetuate these poor designs and leverage people's vanity to silence most, creating a rigid orthodoxy that makes you believe Unix design is a necessary thing to learn. Many have become Unix evangelists and sycophants, lacking real expertise, and blindly parroting the words of the Unix designers in their books. However, their authority and reputation are so immense that I am left speechless in the face of many.

Now, the same Unix "gurus" have designed Go language, relying on their authority and Google's reputation to heavily promote it. These sycophants start praising it, and the aura is as if Go can rule the world. Real language experts know that the designers of Go haven't even scratched the surface of language design. This isn't arrogance from the experts; they wouldn't scoff or mock a child who, through their effort, created an ugly stool. They scoff, they mock, because the creators of this ugly stool are not innocent children, but rather blind people, relying on a blind corporation. They wave banners, trying to make the entire human race sit on this ugly stool.

The most striking feature of Go is goroutines, yet this thing is known to language experts as something else. Some languages like Scheme and ML provide first-class continuations (call/cc), allowing you to easily implement things like goroutines, even hardware interrupt "lightweight threads". As for Go's "type system design based on interfaces", I experimented with it many years ago and held great hope. However, after extensive research and consideration, I found problems and abandoned the idea. It's clear that I'm not the first to fail in this regard, many language experts have attempted to design in this way using parametric types before, only to find it was not a good thing and abandoned it. However, Go's designers didn't learn from these failure lessons, instead treating it as a treasure. A major issue is that in Go, you often need to use "empty interfaces" (interface{}), used to represent all types. This is similar to using C's void pointer, with the hassles of a static type system but without its benefits.: Every time you mention that Go lacks parametric types, Go's supporters say "I don't see the use in that."; They use people's hatred towards Java's complexity and design patterns to make you abandon its few good things. Parametric types in Java were not the first to appear in Java. Their main designers actually include one of Haskell's designers, Philip Wadler. This parametric type has been around in ML and Haskell for a long time and is a very useful thing.

Whenever criticized, Go's supporters retort by saying, "Go is a 'system language.'"; The underlying assumption here is that Unix is the only 'system,' and C was the only 'system language' before Go. It seems that other languages can't write 'systems' like that. However, people were using high-level languages like Algol 60 to write operating systems ten years before C was born. Due to its inherent weaknesses yet aggressive promotion, many of Go's flaws can no longer be patched. Such a language, once popular, will become an endless patch heap like Unix. If Java or Haskell deserve criticism, I can only say to Go's designers, "Go back to school."

---

Cornell:

However, the power and prestige of authority is still significant. Although Knuth no longer holds the monopolistic position in my mind, there are still many things and people that can occupy that place. After leaving Tsinghua, I applied to American universities. Coincidentally or not, only two universities gave me offers: Cornell and Indiana. Surprisingly, I went to both of these universities to study.

To be honest, Indiana gave me a better offer. Cornell gave me a half-time teaching assistant position, while Indiana gave me a fellowship that didn't require any work and paid me in full. To be honest, I never understood why Cornell, a prestigious university, gave me such an offer, given my mediocre GPA and unremarkable papers. Indiana, on the other hand, genuinely cared about me. Indiana's fellowship came from its author, Doug Hofstadter. After learning about my situation and my thirst for knowledge through email, he decided to give it to me, a stranger, and even provided the funding for it. Indyana and Hofstadter's reputation cannot compare to Cornell's renowned "CS top 5" status. Indyana's offer came late. When I received Indyana's offer, I had already accepted Cornell. Hofstadter was surprised and disappointed, as he believed I would be his student. But when he heard I had accepted Cornell's offer, he didn't know what to do. He vaguely recalled telling me that a school's ranking is not the most important thing...

The power of reputation and influence is so great, it prevented me from choosing the person who truly appreciated me and could give me genuine knowledge. When I look back, was I really seeking true knowledge at that time? Did I truly understand what true knowledge meant?

What did Cornell give me? Looking back, it probably only gave me lessons, many lessons. Their work was not easy, it was basically hard labor. You would even doubt they accepted you to exploit your cheap labor. My first time being a TA was for a class of over 200 students in a lecture hall, teaching the most basic Java programming. Although there were several TAs, the tasks were still heavy. The person lecturing was not a professor, but a full-time lecturer. These lecturers usually relied on undergraduate students' good evaluations to make a living, so although they had little academic clout, they were very attentive to students. This left the TAs feeling bitter, as they had to design assignments, prepare standard answers, grade papers, and grade papers in a way that left their brains numb, monitor exams, and then grade exams, and hold office hours to answer students' questions. On top of that, they had their own classes, their own homework, and their own exams. Every time an exam came around, they were anxious because they had to prepare their own exams and do a lot of work for students' exams.

If I had truly learned something, all this hard work might have been worth it, but did the professors really want to teach me? Someone gave an analogy, saying that Cornell promised to teach you to swim, then pushed you into the pool and waited for you to struggle to get out. When you were about to get out, they hit you on the head with a club and pushed you back in. When you were about to get out again, they threw a big rock at your head. This way, you could drown, but Cornell still waited for you to swim to the shore... This is a very accurate analogy for my experience at Cornell.

In an old blog post, I mentioned that Cornell students, including PhD students, took notes in class and worked on assignments day and night. However, Cornell was not just a paradise for note-taking students, but also a temple for those who worshipped authority. Even if you were not particularly reverent towards authority, you couldn't escape being surrounded by a crowd of people who worshipped certain individuals. No matter which professor you asked about, the answer was always: "Wow, he's amazing!" Then you went to their classes, feeling underwhelmed, but they said it was because you didn't understand their value. I felt this atmosphere somewhere else too? Oh, I remember, that was at Google. This atmosphere may not have been accidental. Most PhD students at Cornell at the time dreamed of graduating and working at Google. Later, Facebook rose to become their top choice. It's worth noting that Indyana was a more personal place. My Indiana classmates generally considered working for Google as one of their last options. Once, a new student asked, "How can I get a job at Google?" A senior professor said, "That's easy, Google hires anyone who can do their tasks!"1. Cornell's research can be described as "keeping up with the times," focusing on what's popular. At that time, Facebook and social networks were rising, so the most popular professor in the department was one researching social networks. I attended a few of his lectures, where he analyzed some social network data using the simplest graph theory algorithms and drew some "conclusions." Some of these conclusions were quite obvious, I felt that street vendors could guess them, it would be more interesting to research interstellar warfare instead. However, Facebook's reputation was so large, and there were people whispering in his ear, so many students were eager to be his PhD students. Every time a new professor came, they were praised as extremely intelligent, even called a genius. Many students went to attend their lectures, trying to become their students. However, every class was spent with the professor's back to the students, muttering to himself, writing down formulas and proofs, barely acknowledging the students. Some students took copious notes, while others brought recording devices, afraid of missing a word. It would be better to just print out the textbooks and let students study at home. With more students came more competition. The students in class began to scheme and plot, using Three Kingdoms tactics. Those who couldn't finish their homework came to ask for help, only to find that the helper hadn't finished it either. Those who didn't understand pretended to, creating pressure. The students I liked the most were the ones who said they weren't good, and the ones I didn't like were the ones who said they were brilliant but didn't want me. Until I left Cornell, there were still students who couldn't find advisors and were anxious. Because they hadn't found an advisor for their PhD within two years, they were essentially forced to drop out.

2. After I left Cornell, a domestic student emailed me (found my address from the department website), asking about the situation there. I told him I had already left and shared my feelings, including the constant note-taking and homework rushing. He then asked about a recently graduated PhD student's situation, and I said his work was mediocre, and I had read his dissertation, which was quite trivial, solving a non-existent problem. He was surprised by my comments but still took them with a grain of salt. To ensure no mistakes, he visited Cornell during visiting day and emailed me again, saying he had met many excellent people and broadened his horizons, but it wasn't as bad as I had described. He had also spoken with the PhD student's advisor, who was a world-class expert, and his dissertation was indeed world-class. I had nothing to say, each to their own, go your way.

3. Two years later, I received another email from that student, saying he still hadn't found an advisor at Cornell and was at a loss.

### Turing Award

When someone might ask this question, they might wonder if I, too, was one of those students who couldn't find an advisor and were at a loss. The answer is yes, I indeed didn't find anyone at Cornell who could be my advisor. Then someone might say, "Ah, Wang Jing's abilities aren't up to par, couldn't find an advisor, and was forced to drop out, ha-ha!" But the situation was not as simple as they imagined. As a PhD student, one not only needs to be well-versed in academics but also needs to understand politics and the market. Oh, no, it's not necessary to be well-versed in academics, but one definitely needs to understand politics and the market! However, due to the infighting among students, their information exchange was not on par with that between professors. This resulted in the "student class" being at a disadvantage in this information war, always being passive and selected by professors, rather than effectively selecting suitable professors for themselves. I entered Cornell and took a course on a programming language, which ignited my interest in these things. However, due to Cornell's unbalanced development, its research directions were not well-rounded. Experts in the programming language field had already been overlooked, leaving only a few professors who talked about trivial theories with pens and papers. Historically, Cornell had been renowned for its programming language research, producing some impressive results. However, during my time there, only two lesser-known professors remained, an assistant professor, and an associate professor. Robert Constable was also there, but unfortunately, he had become a dean and no longer had time for students. I didn't know about Cornell's history then, nor did I see the trend of its shifting research focus.

I didn't like the associate professor's projects, which mainly involved adding functional language features to Java. However, since it was a popular language, he received funding and was well-received in the department. When I met one of his students for the first time, I shared my idea with him, and he replied, "Your thoughts aren't even research! Wait till I show you what real research is!" I didn't consider my idea as research, just a casual conversation. But what about your Java projects? I couldn't cooperate with such a person, so I worked on a static analysis project with the assistant professor instead. Our analysis focused on Fortran MPI programs. Despite its lackluster nature, the assistant professor was quite insightful, and some of his advice still guides me today. He once told me, "Those theories are useless. It's best to focus on your own problems and think for yourself." He was a humble and kind person, but good people don't always have good fortune. Unfortunately, he didn't receive tenure and left Cornell to join industry, leaving me without a potential advisor in the programming language field.

I started exploring other related fields, such as database and system design, to see if their professors were interested in language design. Unfortunately, they weren't, and they even told me that the programming language field was too narrow. I initially believed them, but now I'm convinced they were clueless. If these people had humbly sought advice from programming language experts, the design of databases and operating systems wouldn't be so subpar. Relational, SQL, NoSQL, and so on - one disappointment after another. I began exploring other directions, learning about computer graphics and numerical analysis, and making good progress. However, I couldn't stand the languages used in these fields. I wanted to create better programming languages to solve these problems. But when I discussed this with professors, I felt like I was playing a violin to deaf ears; they couldn't understand. I later discovered that professors didn't like students with their own ideas; they preferred students who were willing to "get their hands dirty" and help them realize their own ideas.

I found myself in a similar situation as the student who had asked me about Cornell. I thought about approaching some esteemed professors, like Turing Award winners, for guidance. I contacted one such professor and shared my concerns about Cornell's apparent disregard for my area of interest. I expressed my doubts about the academic world's commitment to intellectual pursuits. However, he failed to understand anything I said and began questioning my grades and test scores instead. Essentially, he was trying to figure out if I was a struggling student. It was a disappointing conversation, and I didn't anticipate that it would lead to my eventual departure from Cornell.

In the midst of students not communicating with each other and the department professors being "in the know," they seemed to have a secret network. They didn't know how to teach effectively and instead relied on assigning homework and tests to students. The survivors picked their advisors, while the unlucky ones were let go. It was a clever power play. I didn't understand their mechanism, how each student felt about which professors, or how they expressed their interests. The department professors seemed to have an uncanny ability to know about this uninterested student who dared to criticize the academic world!: The sky was exceptionally beautiful before the earthquake. I unexpectedly saw that professor with the Turing Award in the corridor nodding and smiling at me. Previously, that professor who made me come and go horizontally and vertically unsatisfied during my time with him also welcomed me with a smiling face. I felt that I had moved that respected professor with that speech, and with the solid progress in computer graphics and numerical computation, perhaps my academic career had taken a turn. But, what I truly understood then was the meaning of the so-called "smile with a dagger behind it."

Due to the good grades in graphics and matrix computation courses that semester, I thought I could find one of the professors for these courses to be my advisor. Plus, those seemingly friendly smiles... So, I didn't think much about it, and I had a very happy winter break. Without any warning, without any direct notification (email, phone), a letter quietly entered my mailbox in the department—a box I rarely checked, used mainly for advertising materials in the department—until the next semester started (in February). The letter was from the department head, saying something like, "Because of your performance, we feel that Cornell is not the right place for you..."

Indeed, I also felt that Cornell was not the right place for me. I had already been considering leaving, but I'm generally lazy to move. If you had told me earlier, say by December, I could have applied to other schools. But it was already February, and Cornell, what am I supposed to do now? I thought I could accuse you of being unjust.

At this most desperate moment, I remembered Hofstadter, who had once cared for me but was ultimately disappointed. I told him I was unhappy at Cornell and wanted to study programming languages, but Cornell didn't understand or care about that field. He replied, "It doesn't matter. You should find people you want to work with." Indiana's Dan Friedman happened to be working on programming languages. You can contact him and tell him I referred you.

So, I emailed Friedman, and I received a quick response saying, "Yin, we looked at your materials two years ago, and we thought you were an exceptional student. Unfortunately, you didn't choose us. You need to understand that the most important things in life are not wealth and fame, but finding people you want to collaborate with. Your materials are still here." My story with Dan Friedman began from there. I encountered a humiliating and secretive experience at Cornell that seems unmentionable, but today I can reveal it to the world because Cornell no longer has the qualification to judge me. Relying on my own efforts and the guidance of teachers at Indiana, my abilities have surpassed those of most professors in Cornell's computer science department. Now I feel like the person who went to Cornell to study swimming essence, who was born to swim, but every time I reached the shore, Cornell picked up a big stone to throw at me, saying I couldn't swim. So I dove into the water and dug a hole, drained it.

Due to some unpleasant encounters with several Turing Award laureates and repeated theoretical misguidance from others, not to mention the fact that many of their major contributions have brought confusion to the software industry, the Turing Award, which many computer science students revere as a god, has lost all meaning in my heart. Many people may find this hard to imagine, but I'm not the only one with this attitude towards the Turing Award. Most programming language experts I know don't take it seriously, and many of them don't even take Turing himself seriously, considering him to have designed some rather ugly things. I admit that Turing's research achievements are valuable, but due to the reasons above, receiving the Turing Award has become a joke for me. I even think ACM should stop issuing this award because it is a very illusory and political thing. Every time people talk about these "big awards" with awe, I can't help but see their naivete.

My experience at Cornell was not accidental, and it was not because I was special. Many of the PhD students who entered Cornell at the same time as me have left without a degree. One of them was a very intelligent young man who entered a PhD program at 18 and could barely understand the theories I could barely follow. But four years later, he dropped out to join Facebook, saying it was too hard to graduate. Some undergraduates also told me similar stories, saying they had been "handled" by a professor called "smiling tiger." Cornell's suicide rate is among the highest in American universities. After leaving, I suddenly saw news reports of three Cornell students jumping off a bridge from the nearby waterfall within a week, and the police were patrolling the bridge day and night. I felt the pressure I experienced at Cornell was beyond imagination, and it was possible to push people to the brink. Looking back, it's laughable, because I unconsciously valued authority and reputation, giving these people who had no right to educate me immense power, allowing them to exert unnecessary pressure on me.

It's worth noting that this phenomenon is not unique to Cornell. I've learned about students at Tsinghua, Princeton, Harvard, MIT, Stanford, Berkeley, and CMU, among others, who are often referred to as "world-class universities" or "wannabe world-class universities." You push your way in through their reputation and "network," and every day someone in your ear exclaims: "Wow, he's great! He's published so many papers and won the XX award." It's like joining a pyramid scheme, making you question whether these people have any self-respect. Then comes the rote learning, endless homework and exams, making you feel they're not educating you, but selecting you. This selection always eliminates the worst and the best. The best students realize what you're doing and don't give you a chance to select them. Once they find out they haven't learned anything, they drop out to start their own businesses. So what's left are the most average, obedient ones. In such an environment, you don't feel the presence of true wisdom and knowledge. The "critical thinking" promoted by the GRE exam is actually quite lacking in American universities. Students are being trained to be someone else's tool, with fixed thinking patterns, like a mold being filled. They are not true creators and pioneers.m people felt similarly during their time at these universities, but once they had graduated, they would never speak of it. Wearing the gold pins of these schools on themselves, how could they damage their own brand, carry someone else's aura? So whenever I criticize Cornell, some of my former classmates look anxious, as if they hadn't experienced that hardship themselves.

Though I gained intellectual freedom at Indiana, this freedom came at a cost of loneliness. I am not an arrogant or unconventional person, but I don't like being around a crowd of people who behave like groupies. In my days at Indiana, the shadow of authoritarianism was a common sight. Indiana students' authority was quite unique, not Dan Friedman, not Kent Dybvig. Friedman was always surrounded by a group of students who believed themselves to be geniuses, flattering him, boasting in front of others. PhD students seemed cool at first, but later discovered they had similar tendencies, eager to present themselves, with weaker research abilities the more they wanted to show off. So you found that someone started by trying to fit into this clique by flattering you, but after two years they became arrogant, and often tried to dominate you with, "Kent said..." I respect Dan and Kent, but I surpass them in many ways. I see some of their thought processes as not entirely correct, and I never use their words as theoretical foundations. Worshiping authority reveals a psychological weakness. If you have confidence in yourself, your own ideas, and judgment abilities, why do you need to bring up a famous person to suppress others?

In my own mind, I am the most formidable PL student at Indiana. Due to my constant experimentation with new ideas, no one's research has escaped my exploration. I don't keep track of my half-baked ideas and failures (because there are too many), and my standards are exceptionally high. So I often find that people giving talks or writing papers contain ideas I tried and abandoned long ago. Sometimes I attend others' talks and immediately spot the flaws, ask questions they can't answer. I often doubt myself, trying to give those ideas another chance to prove their worth, but such questions are unwelcome, so my classmates and even some assistant professors look at me with alarm when I'm present. At meal times, I don't like discussing problems with people because they often display a worshipful attitude towards the theory proposer, and unfortunately, those are often theories I knew were useless long ago. They sometimes know it's trivial, but fear I'll burst their bubble, so they hide their heads in the sand like ostriches.

I wanted to fit in, but I couldn't, so I basically did my research alone. Initially, it was out of necessity, but later, I grew to enjoy it. This is unavoidable because creativity and solitude are virtually inseparable. Since I freed myself from the time spent arguing with people, I had ample time for my exploration. Then I discovered that the common room I had longed for had little use, as no one would understand what I was saying there. Now, even if there was such a place, I wouldn't go. I entered Indiana without intending to get a doctorate degree from the start, I was just playing around with the system to fulfill my curiosity. I disregarded the school's requirements for the degree, unless it threatened my existence. I often served as an RA, but was frequently asked to research seemingly fruitless topics, which clashed with my own thoughts. I eventually became a TA instead, which was less mentally taxing. In a short period of time, I managed to explore the major theories in this field on my own. I seldom read books or papers, and could often solve problems in a week that took others ten years to complete. People were surprised not because I was particularly brilliant, but because these researchers had spent ten years on trivial matters. I never thought I was smarter than others, I just believed many people's minds were confined. I have a simple mind, I cannot understand complex formulas or deep terminology. Yet, it was precisely because of this that I was able to escape the constraints of established theories.

This field has had few escapes from my insight and intuition in the past century. The earliest research in this area can be traced back to the 1870s. I rarely read papers because I believe I can clarify a problem without spending much time on it. Most papers are dull and tedious, so it's better to think for myself than to spend hours reading. When I read papers, I mostly check if someone else has already solved the problem I've thought of. I quickly determine if something is useful or not. I often find that theories considered profound are actually addressing non-existent problems or even creating them, while the real problems remain unsolved. Many problems are caused by the shadow of authority. People are afraid to question these giants' ideas, let alone expose and abandon them, instead choosing to parasitize on them. Therefore, much time is spent on solving historical legacy problems rather than real ones. That's why my English blog title is "Surely I Am Joking," as it documents problems I believe don't exist or were artificially created.

Criticizing problems in the PL domain does not mean other domains are better. Quite the opposite, I believe the system and database domain has more idol worship and nonsense. Sometimes, language experts seem to have obvious problems, but database and operating system people don't notice, they argue and debate without clarity, and even believe they understand PL concepts.

The theory of programming languages is the essence of computer science, but programming language experts have their own issues: they worship logicians. At least one page in every PL paper is filled with constellation-like strange logic symbols, yet they represent nothing more than interpreters and data structures that can be easily implemented in programming languages. People like Kent Dybvig discovered this pattern early on and created tools to convert programming languages into LaTeX format for logic formulas, to help with paper writing.: Some people think those formulas have "mathematical beauty," but in reality, they have design flaws. If you look at Gottlob Frege's original works in modern logic, you'll find that logic wasn't represented by formulas at the beginning. Frege's groundbreaking paper Begriffsschrift only had about 20 pages, and it was quite easy to understand since it was full of diagrams like circuit diagrams. However, someone changed the diagrams into strange symbols. The goal was to represent those diagrams with symbols, but later descendants forgot about the diagrams behind the symbols, leading to today's chaotic situation. After reading Frege's paper, I once again understood the old saying: learn from the true master, not their disciples.

ACM SIGPLAN's chairman Philip Wadler wrote a paper introducing Curry-Howard correspondence, mentioning that excellent logicians always come up with good ideas before us. However, he didn't realize that the capabilities of programming languages have long surpassed those of logic. Logicians have struggled with some problems for over a century without fully understanding them, and they came up with special cases and patches. They have a heap of logical "theorems," but they can't be sure they're correct, and there are paradoxes and other unsolvable issues. So they came up with model theory and other methods to verify their correctness. Logicians have been wrestling with similar issues for over a century without questioning their ancestors' designs.

The most famous logician today is Per Martin-Löf. His type theory, Martin-Löf Type Theory, is revered by many PL people. I've never fully understood what's special about this type theory until one day I read Martin-Löf's 1980 paper (actually a lecture script). I then realized he was essentially talking about how to write a partial evaluator. It's not a particularly magical thing; you just need to make a few changes to a regular interpreter. However, someone (like Neil Jones) wrote over 400 pages of books and numerous papers about it.

Curry-Howard correspondence is also revered by many, originating from mathematician Haskell Curry and logician W.A. Howard's early discoveries. They found that there's a correspondence between some programs and their proofs. Then PL experts started going overboard, saying "programs are proofs, a program's type is a theorem." However, they didn't realize this statement can't explain operating systems, as they're designed to run forever and can't satisfy a proof's basic properties. Moreover, many programs were designed not to prove any theorem but to help people accomplish tasks. So I think "programs are proofs" is a forced association. Reversing it, saying "proofs are programs," isn't much different.

However, from these discoveries, I'm glad to see the value of a programmer. Many people look down on programmers, calling them "coders," but if a program is well-written, it's stronger than those deep logicians and philosophers, as programming languages are more powerful than logical systems. As one mathematician put it, to truly understand something, you should write a program for it. Another person said, "programming is just a forgotten art, and this art is called 'thinking'." I agree. I have gone through turbulent academic years, which gave me exceptionally strong power. My power does not only come from teachers' instructions but also from my own relentless pursuit, as opportunities only favor prepared minds.

Knuth was once the only authority figure in my heart, but later I succumbed to Cornell and the Ivy League alliance's authority and reputation. After being tricked several times in a row, I finally kicked out all the authority figures from my mind. Although I may have been a bit too forceful at times, it was generally beneficial. Not being influenced by authorities anymore doesn't mean I disrespect or disregard them. I merely gained the freedom not to bow down to them or join the mindless cheering crowd. I disrespect only those with inflated egos or their sycophants. In general, the authorities lost their revered positions in my mind, the positions that could be used to override rational analysis with "XX said...". Now they are just a bunch of ordinary, protein-based organisms, some with good intentions or bad intentions, arrogant, humble, or hypocritical. I won't eat humble pie, but if their thoughts are good, I will certainly use them. I deeply understand the dangers of accepting incorrect ideas, so I also hope everyone has a critical mind and doesn't blindly accept what I say. I dislike being called "genius" or "geniuses" and also dislike those who blindly worship me, as they are the very people perpetuating authoritarianism.

American authoritarianism surpasses Europe's, but not everyone in America is overly reverent of authorities, whereas China is the epicenter of authoritarianism. Titles like "Nobel Prize winner XX" are only seen in China. I hope Chinese students don't blindly worship foreign so-called "masters," "elite universities," or "elite companies." I wish you all to eliminate the various authorities in your minds and overcome your fear of them, recognizing your own worth and power.

### Postscript (Regarding IU)

(Note: IU refers to a specific person or entity, but it is not mentioned in the provided text.) some people told me they applied to IU after reading my article about my experiences there. I feel it necessary to make a disclaimer: I didn't intend for anyone to apply to IU based on my article, YMMV. I found Friedman's teaching to be a good fit for me, but some students have told me they found it laborious and barely passed. I only highlighted the good aspects of IU and overlooked the not-so-good. I had difficult times at IU as well. Currently, Kent Dybvig has left IU and joined Cisco. His companies Cadence Research Systems and Chez Scheme have also been absorbed by Cisco. Dan Friedman may or may not still be bringing students. Recently, some seemingly competent assistant professors have been introduced, but I am not familiar with them. My experience is that assistant professors often do things against their will for research funding and to become tenured professors. I can't say for sure what level IU is in other CS directions, so I encourage students to make their own judgments. However, I can unequivocally say that IU has a very beautiful campus, larger than Tsinghua, Peking University, Cornell, Stanford, and MIT.