---
layout: post
title: "Self-Driving Car"
---

I've been following autonomous driving technology.
Google's self-driving cars and Tesla's autopilot have been in the news headlines recently. People are passionately discussing autonomous driving technology, filled with wonder, curiosity, and even fear. Google says: "Self-driving cars are very safe. Human beings are terrible drivers." Many people have accepted this viewpoint without hesitation, believing that they will soon be replaced by self-driving cars. So today, I want to share my thoughts on these "self-driving cars."

From my previous article, you should have already seen that Tesla's autopilot is not even close to being a "self-driving" system. Tesla pushed this immature software into users' cars just to compete with Google and build its own reputation. Look, we were the first to release a self-driving car! However, Tesla's thing is at most an "adaptive cruise control." It's still very far from real self-driving technology. Unfortunately, for Tesla's sake, some people are cheering it on.

Even Google's self-driving cars are still far from being able to be used. I mean "far" not like some people predict, in 10 years or 20 years, but at least 100 years, 1000 years... perhaps even never achievable. Why is that? Google claims that it makes its self-driving cars "learn" from over a hundred million miles of driving records every day. So, shouldn't this car be as smart as a human after learning such massive data?

If you think that way, then you probably don't understand artificial intelligence (AI). Learning from over a hundred million miles doesn't make self-driving cars very smart. Quite the opposite, it shows that they are very dumb. Just ask yourself one question: how many miles does a person need to learn to drive? A normal person only needs 12 lessons, each lasting an hour. Even if you spend the entire hour on the highway, it's only about 80 miles. Twelve hours is 960 miles. That means a normal person only needs to drive less than 1000 miles to become a relatively reliable driver. Compare Google's self-driving cars, they analyze and learn from a hundred thousand miles of "virtual miles" every day and often collect data outside accumulating hundreds of thousands of miles. However, these self-driving cars can only operate in Mountain View, a small town with only a few roads and hardly any pedestrians. I have never seen a Google self-driving car on a freeway going over 50mph or in a traffic complex city.

Recently, Google's self-driving cars have had 272 instances in the past year where human intervention was required due to errors. If humans don't take control in time, many accidents would have occurred. With such simple conditions, the need for so much human intervention is questionable. If the environment is slightly more complex, self-driving cars would likely be clueless.

There is also a "special care" issue. Due to the clear markings on Google's self-driving cars, pedestrians and other drivers can see it and are a bit apprehensive, not daring to make sudden moves for fear of colliding with it. Once Google removes the car's markings, no one will know it's a self-driving car, and they won't provide special care, just minding their own business, and the accident rate might skyrocket.

Therefore, Google's self-driving cars are still far from being able to be used in earnest. It's premature to claim "self-driving cars are very safe," "humans are terrible drivers," ... isn't it a bit too early? How big is the gap between self-driving cars and humans? Heaven and earth. A normal person only needs to drive a thousand miles to learn how to drive, while these self-driving cars have learned several hundred thousand, several million, several billion miles, yet they still can't tell the difference. This shows that the motion sensors of self-driving cars and humans have fundamental differences.

When a person moves, their brain immediately flashes with the corresponding "concept" for the object they see, then quickly brings out the characteristics and countermeasures for this thing. In contrast, a self-driving car cannot accurately identify what the object is: is it a car, a person, a tree, a construction barrier, a big hole, or the car in front of it that has fallen apart? A self-driving car is like a stupid child who has learned for so long but doesn't know what anything is, yet people expect them to cross the United States in ten years.: The car is equipped with GPS, lasers, radars, and so on. Its "senses" receive a lot of data, some of which humans cannot perceive. However, the car's "brain" (computer) lacks cognitive abilities. Even if it collects a large amount of data, it still doesn't know what that thing is or what the relationship is between them. The computer doesn't have this "common sense," so it cannot make correct judgments for people. In critical situations, it may make decisions that endanger passenger safety. "Cognition" is a fundamental problem that AI has not yet solved, let alone started researching it.

The so-called "machine learning" technology used in cars is completely different from human "learning." For example, a child who has never seen a cat before only needs to be shown one and told it's a "kitten." The next time she sees any colored cat, no matter what pose it makes, she knows it's a "kitten." Modern computers, however, have much less cognitive ability than a child or even other animals. If you let the computer analyze hundreds of thousands of cat photos of various colors, poses, and angles, and then show it a cat for a year, it still won't understand what a cat is or make accurate judgments about whether something is a cat. If we say the computer has intelligence, its level is like that of a worm, even less than a worm. Computers lack the ability to recognize and adapt to their environment, so no matter how hard they try or how much data they learn, it's all in vain.

Many people hear terms like "artificial intelligence" (AI), "machine learning," and "deep learning" and think of the intelligent robots from science fiction. But when they enter the "machine learning" field, they find it's full of strange, confusing methods, and in the end, not very useful. These buzzwords, including so-called "deep learning," are hardly related to human thinking. Machine learning is just some ordinary statistical methods to fit function parameters. Statisticians laugh at it.

Artificial intelligence emerged as a trend in the 80s. People were optimistic that computers would soon surpass human intelligence. Japan even declared it would mobilize the entire nation to produce so-called "fifth-generation computers" and develop intelligent programming languages (like Prolog). In the end, people realized that surpassing human (animal) intelligence was much more difficult than they had imagined. The grand promises couldn't be kept, and AI entered a long winter. Recently, due to the popularity of "big data," "self-driving cars," and "Internet of Things," the "AI hype" has been rekindled. However, today's AI hasn't made much progress since the 80s. People still know very little about their own brains and senses, yet they blindly believe that concepts stolen from statistics and renamed "machine learning" can create machines as intelligent as their own heads. These people underestimate the wonder of their own bodies.

Visual and cognitive abilities are unique to animals (including humans), and they are remarkable abilities that allow animals to accurately perceive the complex world around them and make plans suitable for their survival. A car that can travel across an entire country needs to adapt to various complex environments: weather, road conditions, traffic, unexpected situations... Therefore, it needs animals' cognitive abilities. I'm not saying machines will never be able to possess this ability, but if you don't appreciate, study, and understand this ability, it's unlikely that so-called "machine learning" can accomplish these tasks. My prediction is that we won't be able to create cars with abilities approaching human capabilities until we fully understand how animal brains and senses work.: Indeed, there are some people who drive cars carelessly or even drink and drive, causing many accidents. However, it is not fair to conclude that "humans are terrible drivers" based on this. The majority of people still follow the law and prioritize safety. Many people have driven for decades without ever having an accident. Furthermore, we must distinguish between "attitude" and "ability." People who drink and drive are not technically inadequate, but have attitude issues. Computers, of course, do not have attitude problems, but their technology is far from matching human capabilities. Even those who drink and drive, their abilities are far superior to that of computers. I cannot imagine how current computer technology could surpass the driving skills of good drivers and professional race car drivers.

If you still don't understand, maybe the following picture will bring you back to reality:



How can a machine know what is happening next to a car or what potential dangerous situations might occur? How can it know to quickly avoid that car? It cannot. A machine without cognitive abilities is unable to handle the complex and ever-changing real world.

Nowadays, people's fascination, enthusiasm, blind optimism, and exaggeration towards autonomous car technology feel similar to the thought patterns during the Cultural Revolution and the "Great Leap Forward" era. The only difference is that "Mao Zedong" is now Google or Tesla, and "every acre produces ten thousand" is now "self-driving cars will cross the US in two years"... Instead of blindly experimenting with autonomous driving technology, why not focus on doing something practical and effective in the short term to improve people's lives?