---
layout: post
title: "machine-vs-human-3"
---

I. Personal Opinion, Unrelated to My Company's Position

Due to the recent slowing down of GitHub server access speed in China, the images in this article may still take a considerable amount of time to load despite significant size compression. This article exposes misconceptions and misrepresentations in the AI field that I encourage everyone to share and its subsequent articles, just remember to mention the author and source.

This is the third installment of this series, in this part, I will discuss how the so-called "superhuman recognition rate" in AI comes about, and the disastrous consequences of blindly trusting machine vision.

### The Mist of "Superhuman Accuracy"

I've discovered that the reliability and calculation method of accuracy in neural networks during testing data are quite problematic. "Neural networks perform image recognition, the so-called "accuracy" is not obtained from actual data, but from specialized testing data. For example, ImageNet contains 120,000 images, which were downloaded from Flickr and similar photo websites. Therefore, the actual accuracy and recognition effect are worth questioning. All the data is from online photos, but online data is certainly not comprehensive. The angle and lighting of the photos cannot represent the diversity of reality. Moreover, both the training and testing data were selected under ideal conditions, without considering various natural phenomena: reflection, refraction, shadow, etc.

For instance, the following is a small part of the commonly used image recognition datasets such as ImageNet and others. You can see that most of them are taken under sufficient lighting conditions. Both the training and testing images are of this type, so when encountering real-world scenes with insufficient light or shadows, the accuracy may not be as high as stated in the papers.

Such an evaluation of "accuracy" is akin to optimizing a compiler for a very small benchmark and then boasting about its performance. However, when people encounter real code, they may find that the performance is not satisfactory. Neural network training requires expensive hardware conditions, and most people may not have the opportunity for complete model training and actual testing. Therefore, everyone can only trust industry experts to claim "superhuman accuracy," but cannot verify its actual effectiveness."

"Top-5 accuracy scam"

Not only is the "generality" of testing data questionable, but the standard for calculating "accuracy" is also suspicious. In the field of AI, when announcing neural network accuracy to the public, they often secretly use so-called "top-5 accuracy," which means that each image is given five chances to classify, and if one of them is correct, it is considered correct, and then the accuracy is calculated. Based on top-5 accuracy, they conclude that some neural network models have surpassed human-level image recognition accuracy." if they mention "top-5", it's okay. Most of the time they only talk about "accuracy", without mentioning the words "top-5". When comparing with people, they always say "surpassed human level", but never mention "top-5", without explaining what standard. Why do I have such strong objections to top-5? I'll explain now.

To be specific, what does "top-5" mean? It means that for one image, you can give five possible classifications, as long as one of them is correct, the classification is considered correct. For example, for an image that is actually a car, I say:

1. "That's an apple?"
2. "Oh no, that's a cup?"
3. "Is that a horse instead?" Stuart still wrong, so it's a mobile phone then?

Amazingly still wrong, so I'll finally guess it's a car!

Five chances, I named 5 unlikely words, one of them was correct, so I'm classified correctly. Ridiculous? Keep going, classify many images, then calculate your "correct rate".

Why five chances? In the ImageNet competition (ILSVRC), they gave two different explanations for two kinds of competitions. One said it's to allow machines to identify multiple objects in an image without being marked incorrect due to one incorrectly identified object. The other said it's to prevent the output of synonymous words that don't perfectly match the label from being marked incorrect.

The reasons are different, but the mathematical definition is basically the same. In short, there are five chances, only one correct one needed. Seldom reasonable? However, this is a vague and incorrect standard. This allows neural networks to output five labels (apple, cup, horse, phone, car) as in the example above, with the first four being not objects in the image but still considered correct.

You might think my example is too exaggerated, but accuracy calculation standards should not have such loopholes. As long as there are loopholes in the standard, there will be incorrect cases that are overlooked. Now let's look at a more practical example.

The image above is an example of top-5 actual output results given by a machine learning course on Coursera. You can see that, even though some top-5 output labels are synonyms, there are also many labels that are completely wrong. For instance, the "calculator" image's top-5 includes a computer keyboard and an accordion. The "tiger" image's top-5 includes two breeds of dog names (boxer, Saint Bernard).

Furthermore, you can see that the test images were carefully selected and cropped, containing few if any multiple objects. So the first argument, "perhaps outputting an object that exists on the image but is not the correct answer," is likely rare.

Therefore, ILSVRC's reasons for using top-5 are unconvincing. The problem it aims to solve is not as prominent, but it has opened a backdoor, potentially overlooking many incorrect cases. For example, in the case of the "calculator" image, if the first-ranked label is not abacus but computer keyboard or accordion, as long as abacus appears in the top-5 list, this image is considered identified correctly. Thus, top-5 is an incorrect standard. I. To address issues of multiple objects in images or output being synonyms, there are better solutions that won't count incorrect results as correct. Every undergraduate student who has learned basic data structures and algorithms should be able to think of better solutions. For instance, you can use a synonym dictionary, and consider outputs as correct if the tags and the "correct tags" are synonyms. For images with multiple objects, you can label them with multiple tags during annotation, and consider the algorithm's output as correct if it is among these "correct tags."

II. However, ILSVRC did not adopt these solutions, but instead used top-5. This basic yet significant problem in AI, and the solutions in the industry seem quite primitive to me. Don't you find it strange? I believe they have their own intentions: top-5 increases the neural network's accuracy rate significantly, making it appear as if it "surpasses human capabilities."

III. Top-5 accuracy is always much higher than top-1. By how much? For example, the top-1 error rate for ResNet-152 is 19.38%, while the top-5 error rate is only 4.49%. A top-1 accuracy rate can barely be considered usable, but switching to top-5 suddenly allows the claim of "surpassing human capabilities." People say that human top-5 error rate is roughly 5.1%.

IV. Top-5 accuracy is unfair to humans. Many people may not have realized this, but the top-5 comparison method is unfair to humans. If there are familiar objects in an image, people can usually identify them correctly in one attempt, without needing five chances. Using "top-5 accuracy," it's like giving multiple chances to underperforming and top-performing students to answer exam questions. Consequently, it becomes difficult to distinguish between underperforming and top-performing students. The "top-5 accuracy" significantly blurs the line between good and bad, making everything seem similar, and even making underperforming students appear better than top performers.: Here's a more detailed explanation. If a person's top-5 error rate is 5.1% when identifying images, then his top-1 error rate is likely to be around 5.1% as well. If a person makes a mistake once, he might not have even seen the object in the image. If he makes a mistake every time and you give him five chances, he still won't get it right because he doesn't know what that thing is called.

Now, for a certain neural network (ResNet-152), the top-5 error rate is 4.49%, and the top-1 error rate is 19.38%. However, based on the top-5 error rate alone, you conclude that the neural network surpasses human capabilities. Isn't that absurd?

To take a step back, even if you could use top-5 error rates, like the 4.49% difference between them, which is only 0.61%, should be negligible. Since experiments have errors and randomness, and test data varies, differences of less than 1% cannot indicate a problem. If you carefully examine the reported recognition rates in various papers, you'll notice that the numbers are not the same. The accuracy difference for the same model can be over 3%. However, they compare neural networks and humans, always using the neural network's best result and focusing on the "advantage" of a fraction of a percentage point, then proudly declare that they have "surpassed human capabilities."

Moreover, have they conducted a fair experiment with humans? Why haven't they ever published "neural network vs human top-1 comparison results"? The "human top-5 accuracy rate" number comes from where? Which people participated in this test, and what were they like? I only saw a description of human performance in Andrej Karpathy's website. He tested his recognition accuracy on ImageNet and found that he didn't recognize many things, so he trained himself on ImageNet's images again and retested, resulting in a significantly improved accuracy.

Can the recognition accuracy of just one person represent the entire human race? And do you know who Andrej Karpathy is? He is Fei-Fei Li's student and currently serves as Tesla's AI manager. Fei-Fei Li is the initiator and creator of ImageNet. Having an "insider" conduct the experiment and publish the results is not a fair or scientific approach. Have you ever seen medical or psychological researchers conduct experiments on themselves and publish the results? First, the sample size is too small, and at least several dozen intellectually normal people should participate and average the data. Second, this person is an insider, and their performance may lack objectivity.misunderstood, I don't deny that Andrej Karpathy is intelligent and straightforward in his speech. I appreciate his teaching of Stanford CS231n, through which I first understood what neural networks really are, and how back-propagation works. I also thank Li Fei-Fei for preparing this course and selflessly putting it online. However, in such a large field with so many people, isn't it irresponsible for only one researcher to proclaim such a big slogan as "surpassing human-level vision" and to conduct experiments himself?

In the AI field, various optimizations are made for training neural networks, even specifically for the top-5, every performance and precision point is squeezed out. However, when it comes to showing human beings how accurately machines can recognize, it is done carelessly, without organizing reliable experiments, and the accuracy numbers are not clear. Compared to fields such as neuroscience, medicine, these areas conduct experiments on humans in a certain way and report results to the public. The practices in the AI field are questionable in terms of science.

This is the origin of the "AI image recognition surpasses human-level" rhetoric. The "superhuman recognition rate" and "90+% accuracy rate" in the AI industry are all based on "top-5 accuracy rate" as a standard, and the human recognition rate numbers used for comparison are not reliable. Once you use "top-1 accuracy rate" or a more fair standard, and use objectively and fairly selected human experimenters, machine accuracy may be far from human-level.

### Embarrassing top-1 accuracy

Let's take a look at top-1 accuracy. The top-performing model in the industry, ResNet-152, has a top-1 error rate of 19.38%. The ImageNet champion in 2017, SENet-154, had a top-1 error rate of 18.68%. Of course, this doesn't take into account any actual lighting, shadows, or distortion issues, it only uses standard, ideal ImageNet "test images". In real-world situations, accuracy will definitely be lower. In deep learning, improving top-1 accuracy has become quite challenging, hovering around 80% or so. Some algorithm engineers told me that the recognition rate seems to have reached a bottleneck, and increasing the model's scale is the only way to make a slight improvement. However, larger models have more parameters and require greater computational power to train. For instance, the size of SENet-154 is 1.7 times that of ResNet-152, and the size of ResNet-152 is 2.4 times that of ResNet-50. Yet, the top-1 accuracy only improves slightly.

I also found an interesting discovery. If you calculate the difference between ResNet-50 and ResNet-152, you'll find that ResNet-152, despite its model size being 2.4 times that of ResNet-50, only reduced its absolute top-1 error rate by 1.03%. From 22.37% to 21.34%, it relatively decreased by 4.6%, which is not much. However, if you look at its top-5 error rate, you'll notice a significant improvement, as it dropped from 6.36% to 5.54%. Although the absolute value decreased by only 0.82%, the relative decrease was (6.36-5.54)/6.36 = 12.9%, which seems like a considerable improvement.

This might be why the AI industry focuses on top-5 as the second reason. Since the error base is small, even a slight decrease appears substantial in comparison. Then, looking at the historical improvements in top-5, you'll feel that deep learning recognition rates have made substantial progress!

However, if you focus on top-1 accuracy, you'll notice little change. Despite the model's size increasing several fold and the computational requirements growing significantly, top-1 accuracy remains almost unchanged. Therefore, deep learning's top-1 accuracy seems to have reached a bottleneck, and without fundamental breakthroughs, it's unlikely that larger models will surpass human performance.

### AI industry's credibility issues and the drama of autonomous driving

The AI industry is grappling with credibility issues, particularly concerning autonomous driving. Companies are making bold claims about their self-driving capabilities, but the reality often falls short of expectations. For instance, Waymo, Alphabet's autonomous vehicle subsidiary, reportedly achieved a 99.999% safety record in its early testing phase. However, this statistic was based on a limited dataset, and the actual safety record in real-world conditions is far from perfect.

Another example is Tesla's Autopilot system, which has been involved in numerous accidents, despite Elon Musk's claims that it is "better than a human driver." The inconsistency between these claims and the actual performance of these systems raises concerns about the reliability and safety of AI technology in autonomous vehicles.

Moreover, the lack of transparency and standardization in testing and reporting autonomous driving performance adds to the credibility issues. There is no universally accepted benchmark for evaluating self-driving capabilities, making it difficult for consumers and regulators to compare the performance of different companies' offerings.

These credibility issues can have significant consequences, particularly in the case of autonomous driving. Self-driving cars have the potential to save lives and reduce traffic congestion, but they also pose significant risks if they are not reliable and safe. It is crucial that the AI industry addresses these credibility issues and establishes clear and transparent testing and reporting standards to build trust and confidence in autonomous driving technology.

In conclusion, deep learning's top-1 accuracy seems to have reached a plateau, and the credibility issues surrounding autonomous driving highlight the importance of transparency and standardization in testing and reporting AI performance. These challenges underscore the need for continued research and innovation in the field to overcome these barriers and unlock the full potential of AI technology.: The accuracy rate is not high enough, it's not a big problem for humans if we acknowledge its limitations and use it appropriately. However, the most serious issue is human trust. AI experts often exaggerate the effects of image recognition and apply it beyond its capabilities.

The AI industry has never clearly explained to the public what standards their so-called "superhuman recognition rate" is based on, instead, they are misleading and deceiving the public with claims like "AI has surpassed human vision" in various media.

Can you give an autonomous car five chances to identify what object appears in front of it? Do you have a few lives to spare for its experiments? Tesla's Autopilot system might have a high top-5 correct rate: "That's a whiteboard.... Oh wait, that's a truck!" "That's a loaf of bread.... Oh wait, that's the highway median!"

I'm not joking, click on the "truck" and "median" links above, they lead to two fatal crashes caused by Tesla Autopilot. In the first accident, Autopilot identified a truck as a whiteboard and crashed into it from the side, instantly killing the driver. In the second incident, it changed lanes without recognizing the median strip in the middle of the highway, failed to slow down, and instead accelerated, causing the driver's death and a subsequent fire.

Neural networks can identify a truck as a whiteboard and still be considered "top-5 classification correct," but Autopilot lacks the visual understanding ability, which is why such tragic incidents occur. You can find a list of accidents caused by Autopilot here. There have been quite a few lives lost, but the research on "autonomous driving" continues in chaos. In March 2018, an Uber autonomous vehicle struck and killed a female pedestrian in Arizona. The car's dashboard camera footage of the incident was released online.

The report showed that Uber's autonomous driving system detected the woman six seconds before the accident but initially classified her as an "unknown object," then as a "car," and finally as a "bicycle," failing to brake and hit her at a speed of approximately 40 miles per hour.... [news link]

Before this incident, Uber had had its autonomous driving experiment license revoked by the California government. They then turned to Arizona, as Arizona's governor warmly welcomed policies to "embrace tech innovation." The result? A life was taken. When Americans saw Uber's autonomous car kill a person, they all commented, "Why not experiment with autonomous driving cars in Arizona? After all, lives there don't matter, and you don't have to take responsibility for hitting someone!"

According to reports in December 2018, Uber plans to restart autonomous driving experiments, this time in Pennsylvania's Pittsburgh. They want to conduct autonomous driving experiments in Pittsburgh's busy downtown area because of its narrow streets, train tracks, and numerous pedestrians.... I think if they really go there to experiment, there might be a better show to watch.: The use of visual technology in autonomous driving is completely unreliable, posing a threat to other drivers and pedestrians. Yet, various autonomous driving companies are clamoring for the traffic departments to give them the green light. Some companies have even been denied US government approval for licenses and are berating regulatory bodies for not understanding their "high tech," being too conservative, and falling behind the times. Some companies go as far as to propose that they be allowed to forgo steering wheels, accelerators, and brakes in their autonomous vehicles, claiming that their cars no longer require human drivers, and that only when completely devoid of human control can autonomous vehicles run safely.

This drama unfolds as if autonomous driving is just around the corner, with everyone scrambling to seize the market and pressuring the government to relax policies. It's reminiscent of our earlier steel-making era, racing to surpass the US. These companies behave like spoiled children demanding their parents let them play with autonomous driving, making all sorts of demands and ultimatums. Tesla and Uber should be criticized, but the visual issues aren't just their problem; the entire autonomous driving industry is built on a shaky foundation.

We must clearly recognize that the so-called AI currently available doesn't possess the visual understanding abilities of humans. They are merely rough image recognition, far from human-level performance, making it impossible for them to achieve autonomous driving.

The L1-L4 levels of autonomous driving classification are meaningless. It's impossible to achieve these things, so what's the point of labeling them? They're just used as marketing slogans for these companies and as excuses to shirk responsibility. Before accidents, they're used for promotion: "We've achieved L2 autonomous driving, and we're now researching L3 autonomous driving. Once successful, we'll move on to L4!" After accidents, they're used to dodge responsibility: "We only offer L2 autonomous driving, so this accident was inevitable and unavoidable!"

If there's no visual understanding, and autonomous vehicles rely on image recognition technology, they cannot make correct actions in complex situations to ensure the safety of people. Robots and related technologies can only stay in fixed scenarios, in the precise positioning stage of "industrial robots," and cannot operate in complex natural environments. Recognition technology is still meaningful or not

Achieving true language understanding and visual understanding is extremely difficult, to say the least. Generation after generation of neuroscientists, cognitive scientists, philosophers, have put in great effort to understand what human "cognition" and "understanding" really are. However, our understanding of human cognition and understanding is still not sufficient to give machines true understanding capabilities.

True AI has not yet started, and many people who are involved in AI are busy with hype and propaganda, without caring about the essence. Unless someone truly cares about the core issues, researches the fundamental problems, otherwise, achieving true understanding abilities is just an empty castle in the air. I just remind everyone not to be blindly optimistic, not to be deceived. Instead of exaggerating the words, deceiving the masses, saying artificial intelligence is about to achieve, why not do something practical with the existing recognition technology? Be honest and face the serious limitations.

I'm not all against recognition technology, I'm just against exaggerating "recognition" as "understanding", equating it with "intelligence", and making false advertising, using it in areas beyond its capabilities. Honest use of recognition technology is still useful and interesting. We can use these things to make some useful tools, assist us in doing things. From speech recognition, speech synthesis, image search, content recommendation, business financial data analysis, anti-money laundering, police investigation, medical image analysis, disease prediction, network attack monitoring, various entertainment-oriented apps.... It can indeed bring us quite a few benefits, enabling us to do things we couldn't do before.

Furthermore, although all companies are exaggerating and making false claims about their "AI dialogue systems", if we give up "true dialogue", honestly acknowledge that they are not truly conversing and have no intelligence, they can still provide some convenience. Existing so-called dialogue systems, such as Siri, Alexa, are essentially voice control command-line tools. You say one sentence, the machine picks out the key words and executes a command. This is not a meaningful dialogue, but it can still provide some convenience. In particular, when driving and it's inconvenient to look at the screen, voice control "next song", "air conditioning wind less", "navigate to the nearest gas station".... These commands are still useful.: But don't forget, identification technology is not truly intelligent. It lacks the ability to understand, and cannot be used in fields such as autonomous driving, automatic customer service, food delivery, cleaning aides, chefs, hairstylists, athletes, and so on, which require true "visual understanding" or "language understanding" abilities. Machines also lack emotions and creativity, and cannot replace teachers, programmers, scientists, artists, writers, film directors, and so on, who require advanced knowledge. All those who tell you that machines can have "emotions" or "creativity" are deceiving you, just like the conversation systems today, which only make people think they have those functions, but in fact, they don't have them at all.

You may find that machine learning is very suitable for doing those non-intuitive, things that humans can't see through, or seem tiring, such as various data analysis. In fact, these are the problems that statistics have been trying to solve all along. However, when it comes to visual functions that are part of daily life for humans and higher animals, machines indeed have great difficulty surpassing them. If the machine learning field gives up its blind pursuit of "human-level intelligence" and stops using "superhuman vision" type masks to deceive the public, they should be able to make positive contributions in many areas.

(End of text)