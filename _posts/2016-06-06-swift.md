---
layout: post
title: "Swift Language Design Flaw"
---


In the 'Programming Wisdom' article, I analyzed and confirmed Swift's optional type design, but this does not mean that Swift's overall design is perfect without issues. In fact, I noticed a serious error in Swift 1.0's array mutability design. Swift 2.0 addressed this issue, but their solution did not hit the mark, resulting in other problems. This error has persisted to this day.

Swift 1.0 attempted to use var and let to specify the mutability of array members, but in reality, var and let only specify the mutability of array references, not the mutability of array members. For instance, Swift 1.0 tried to implement the following semantics:

var shoppingList = ["Eggs", "Milk"]

// It is possible to assign a value to an array member
shoppingList[0] = "Apples"

However, this approach led to unexpected behavior and inconsistencies, as array references and array members have different mutability scopes. Swift 2.0 introduced copy-on-write arrays to address this issue, but this solution introduced new complexities and performance overheads. Swift 3.0 introduced value types for arrays, which finally provided a more consistent and predictable behavior for array mutability.: shoppingList = ["Eggs", "Milk"]

// cannot assign to array member: shoppingList is a constant reference
shoppingList[0] = "Salad"

This is incorrect. In Swift 1.0, an array is a "reference type" like other objects. To understand this problem, you should clearly distinguish between an array reference and an array member. In this example, shoppingList is an array reference, while shoppingList[0] is accessing an array member. These two concepts are very different.

The var and let keywords are used to specify whether shoppingList, this reference, is mutable or immutable, determining whether shoppingList can point to another array object. The correct usage should be as follows:
[
 "Eggs",
 "Milk"
]

var shoppingList: [String]

// or

let shoppingList: [String] = [
 "Eggs",
 "Milk"
] var shoppingList = ["Eggs", "Milk"];

// Array reference can be assigned
shoppingList = ["Salad", "Noodles"];

// Array member can be assigned
shoppingList[0] = "Salad";: Cannot assign to an array reference, error occurs.

shoppingList = ["Salad", "Noodles"]

// let does not restrict array member assignment, no error
shoppingList[0] = "Salad"

This means you can use var and let to control the mutability of shoppingList this reference, but not to control the mutability of shoppingList[0] member access.

Once var or let is used to specify the mutability of an array reference, it can no longer be used to specify the mutability of an array member. In fact, var and let are only used to specify the mutability of stack data when defining local variables. If you understand that references are on the stack (stack), and Swift 1.0 arrays are on the heap, you will understand that the mutability of array members (a kind of heap data) must be specified in another way, and cannot be specified using var and let. Many old languages have clearly addressed this issue by using different ways to specify the mutability of stack and heap data. C++ programmers are familiar with the difference between int const * and int * const. Objective C programmers are familiar with the difference between NSArray and NSMutableArray. It's unclear why Swift's designers didn't see this problem and tried to use the same keywords (var and let) to indicate the mutability of data in stack and heap locations.

In fact, immutable arrays and mutable arrays should use two different types to represent them, like Objective C's NSArray and NSMutableArray, rather than using var and let to distinguish.

Swift 2.0 addressed this issue, but unfortunately, its solution was incorrect. Swift 2.0 made a bizarre change, turning array from a reference type to what's called a value type, which means the entire array is placed on the stack instead of the heap. This seems to have solved the above problem, as the array now being a value type means that shoppingList no longer refers to a pointer, but rather represents the entire array itself. Therefore, when the array is a value type, you can indeed use var and let to determine if its members are mutable.

let shoppingList = ["Eggs", "Milk"]

// Cannot assign values to array members, as shoppingList is a value type
// It represents the entire array, not a pointer
// All parts of this array are immutable. This seems like a viable solution, but it hasn't addressed the core issue. This is a workaround that brought about other problems. By making array a value type, every assignment or parameter passing of the array variable requires copying. You cannot let two variables point to the same array, so array can no longer be shared. For instance:

var a = [1, 2, 3]

// a's content is copied to b
// a and b are two different arrays, with the same content
var b = amakes it inconsistent with a programmer's mental model of arrays for large structures. Due to array being automatically copied without intention, errors are easily committed. Array copying requires significant time, even if the receiver doesn't modify it, thus having a large impact on efficiency. Sharing the same array and modifying it in the process is a significant functionality gap. No other modern languages (Java, C#, etc.) make array a value type for this reason.

If you understand the essence of value types, you will realize that this concept is almost meaningless in modern languages with garbage collection (GC). Languages like Swift and Rust attempt to use value types to address memory management efficiency issues, but the actual performance improvement is negligible and brings unnecessary complications for programmers. Completely using reference types (such as Java, Scheme, Python) allows programmers to not consider value types and reference types, significantly simplifying and accelerating the programming thought process. Java not only has highly efficient GC but can also automatically put some heap data on the stack using escape analysis, allowing programmers to achieve the performance gains of value types without thinking about it. Compared to Swift, Rust, and C#'s value types, the complications they create are more significant, but they don't offer real performance advantages.

Swift 1.0 made a glaring mistake that experienced language experts could see right away: compiler experts are not language experts. Many veteran language experts would have recognized the mistake in Swift's initial array design. Why did Swift not find this problem until version 1.0 and still got it wrong in version 2.0? I suspect it's because Apple didn't hire enough qualified language experts to work on Swift or, if they did, their suggestions were not taken into consideration by the leadership. Swift's chief designer is Chris Lattner, who is a good compiler expert but likely only has intermediate-level skills in programming language design. Compilers and programming languages are indeed two very different domains. Apple's leaders believed that good compiler authors could design good programming languages, leading them to let Chris Lattner take on the role of chief designer.

The Swift team is not as clueless as the Go language team, as they do have some foundational knowledge in the language field. However, it's clear that their capabilities are not yet deep enough, and they exhibit a certain youthful arrogance, impulsiveness, and innovative spirit. Some designs are not born out of their own deep understanding but are rather "borrowed" from other languages, making them prone to making errors that experienced language experts would never make. They should have done things right the first time, but instead, they needed to go through multiple revisions. As a result, each new version introduces "incompatible changes," making code written in older versions obsolete. This trend continues in Swift 3.0. Although Apple's dominant market position may not mean the end of the world for Swift, it did indeed commit a cardinal sin in language design. A good language can lack certain features, but it should not introduce incorrect designs that lead to incompatible changes in the future. I hope Apple can recruit more experienced language design experts and humbly consider their suggestions. BTW, if Apple pays enough, I might consider working as their language design consultant ;-)

### Does Java have value types?

There is no value type in Java. Java uses reference types for arrays, which is different from modern languages like C# and Swift that treat arrays as value types. Java's design decision not to use value types for arrays stems from the fact that arrays in Java are not objects and do not have a heap allocation. Instead, arrays are primitive data types that are directly stored on the stack or in the heap, depending on their size. This design decision has implications for how arrays are passed, copied, and manipulated in Java.

In Java, when you pass an array as an argument to a method, a reference to the array is passed, not the array itself. This means that any changes made to the array inside the method will affect the original array. When you want to make a copy of an array, you need to create a new array and copy the elements from the original array into the new one. This can be done manually using a for loop or using the System.arraycopy() method.

The lack of value types for arrays in Java can have some advantages and disadvantages. One advantage is that it simplifies the language design, as there is no need to define value types for arrays and their associated copy semantics. Another advantage is that it allows for more efficient memory usage, as arrays do not require extra memory for storing the value type's metadata. However, the lack of value types for arrays can also make it more difficult to reason about array manipulation and can lead to unintended side effects when arrays are passed as arguments to methods.

Despite the absence of value types for arrays, Java still offers several features to help manage arrays efficiently. For example, Java provides the List and ArrayList classes, which can be used as alternatives to arrays when dealing with large collections of data. These classes offer built-in methods for adding, removing, and manipulating elements, as well as features like automatic resizing and dynamic array creation. Additionally, Java's garbage collector helps manage the memory used by arrays and other objects, ensuring that memory is freed when it is no longer needed.

In summary, Java does not have value types for arrays, but it offers other features and classes to help manage arrays efficiently. The decision not to use value types for arrays simplifies the language design and allows for more efficient memory usage, but it can also make it more difficult to reason about array manipulation and can lead to unintended side effects. someone asked me about the content above, "You mentioned that Java only has reference types, but according to Java's official documentation, Java also distinguishes between value types and reference types." Due to the interest of this question, I wrote an additional article to answer it.