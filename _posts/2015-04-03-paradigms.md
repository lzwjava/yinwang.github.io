---
layout: post
title: "Object-Oriented Programming (OOP)"
---


This is a topic that always sparks debate: is "Functional Programming" (FP) better or is "Object-Oriented Programming" (OOP) better. Since there are two schools of thought, there are those who are passionate about each, criticizing and looking down upon each other. Then there is the "pragmatic grandfather" faction, whose members say that no matter what the paradigm is, a good tool for solving problems is a good tool! Personally, I don't belong to any of these three factions.

### Object-Oriented Programming (Object-Oriented)

If you look beyond the surface, you'll find that "Object-Oriented Programming" itself doesn't introduce much new. So-called "object-oriented languages" are essentially "procedural languages" (such as Pascal), with added abstractability. So-called "classes" and "objects" are essentially records (or structures) in procedural languages. At their core, they are a mapping table (map) from names to data.

You can extract corresponding data using names from this table. For example, point.x is extracting data corresponding to the name x from the record point. This is more convenient than arrays because you don't need to remember the index for storing the data. Even if you add new data members, you can still access existing data using the original names without worrying about index misalignment issues. "Object thinking" (differing from "object-oriented"), in essence, is an abstracted way of accessing data. A classic example is the plane point data structure. If you store a point as:

struct Point {
 double x;
 double y;
}

Then you can directly access its X and Y coordinates using point.x and point.y. You can also store it in "polar coordinates" style:

[Polar coordinates: r, angle]

Here, r represents the distance from the origin, and angle represents the angle in radians from the positive x-axis. In object thinking, you would define a Point class with methods to access its polar coordinates, such as getR() and getAngle(). This encapsulates the data and provides a more abstract and flexible way to work with the point. I. Struct Point {
II. double r;
III. double angle;
IV. }

// You can access its radius and angle using point.r and point.angle.
// Now the problem arises if your code defines Point at the beginning as the first kind of XY way, using point.x and point.y to access X and Y coordinates, but later you decide to change Point's storage method to polar coordinates, without modifying the existing code containing point.x and point.y. What can be done?

// This is the value of "object-oriented thinking," which allows you to change the meaning of point.x and point.y "indirectly" (or abstractly), so that the user's code remains completely unchanged. Although your actual data structure may not have x and y as members, since .x and .y can be redefined, you can simulate them by changing .x and .y definitions. When you use point.x and point.y, the system actually runs two pieces of code (so-called getters), whose purpose is to calculate x and y values from r and angle. This way, your code feels like x and y are actual members, but they are in fact temporarily calculated.

// In Python-like languages, you can directly change the meaning of point.x and point.y through defining "properties." In Java, it's a bit more complicated, and you need to use point.getX() and point.getY()-style writing. However, their ultimate goal is the sameâ€”they provide an "indirect" (abstract) layer for data access.: This abstract idea sometimes is a good one, it might even relate to quantum mechanics' so-called "unobservability." Do you think there are ten electrons in this atom? Maybe they are just like the point.x illusion given to you, maybe the universe doesn't even have electrons at all, maybe every time you see an electron, it is temporarily generated to fool you? However, the value of object thinking ends there. You have seen the so-called "object-oriented thinking" almost never fails to extend from this idea. Most features of object-oriented languages are actually provided by procedural languages long ago. Therefore, I believe there is no such language as "object-oriented language." Just like a person contributing a little code to a company does not make the company name after him.

"Object thinking" as a data access method, has some merits. However, "object-oriented" (with the added "object" part), is stretching and forcing the relationship, over-exerting the potential. Many object-oriented languages claim "everything is an object," putting all functions into supposed objects, calling them "methods," and ordinary functions are called "static methods." In reality, only a very small number of times do you need the embedded methods that are tightly coupled with data within objects. Otherwise, you are just expressing the transformation of data operations, which can be fully expressed by ordinary functions, and it is simpler and more direct.

The way of putting all functions into methods is backward, as functions are not part of objects. Most functions are independent of objects, they cannot be called "methods." Forcing all functions to be placed in objects they don't belong to, calling them all "methods," leads to overly complex logic in object-oriented code. It's a simple idea, but it requires many detours to express clearly. It's like sticking your head in your own butt.

This is why I like to joke that object-oriented programming is like "flat earth theory." Of course, you can say the earth is flat. For local, small-scale phenomena, it's not a problem. However, for general, large-scale situations, it's not natural, simple, or direct. To this day, you can still endlessly search for evidence, twist various physical laws, and justify the flat earth theory illusion, but it makes your theory extremely complex, often requiring frequent patching and difficult to understand.

Object-oriented languages not only have inherent design flaws, but the designers of object-oriented languages are often half-baked, without rigorous language theory and design training yet still acting arrogant, resulting in various quirks. For example, in JavaScript, every function can also act as a constructor, so every function implicitly contains a this variable. When you nest multiple objects and functions, you can't access the outer this, so you need to "bind" it. In Python, variable definition and assignment are not distinguished, so when you need to access a global variable, you have to use the global keyword. Later, when you want to access a "middle-layer" variable, there's no way, so they added a nonlocal keyword. Ruby had four similar lambda-like things, each with its own quirks... Some people ask me why some languages are designed that way, I can only say, many language designers don't really know what they're doing. I. Object-Oriented Programming (OO) Domain is fond of creating sects. "Object-oriented" was popularized years ago by seizing various pretexts and became a sect, brainwashing many people. What exactly is a "object-oriented language"? This fundamental question still lacks a definitive answer, indicating that object-oriented is mostly meaningless. Whenever you point out a shortcoming of an OO language X, someone will tell you that X is not a "pure" OO language and you should look at language Y instead. Once you find that Y also has similar issues, someone else will tell you to look at Z, and so on. In the end, they tell you that Smalltalk is the only "pure" OO language. Isn't it funny that they say the language that almost no one uses is the "pure" OO language? This is like politicians kicking a ball and passing the buck.

II. Functional Programming (FP)

Functional programming languages have always been low-key until recently due to the emergence of bottlenecks in parallel computing and the strong advocacy of Haskell, Scala, and other FP community. Some blindly believe that functional programming can miraculously solve the problems of parallel computing, ignoring the real issues that exist independently of the language. Brainwashed followers of functional programming, they deny everything about other languages and look down on other programmers. Particularly new programmers often see functional programming as a magical weight loss pill, thinking that they can tell experienced programmers with over ten years of experience three things once they start with a functional language, as if those who don't use functional programming don't understand anything.

#### Functional Programming Advantages

1. Pure Functions: In functional programming, functions are considered "pure" if they return the same output for the same input and do not modify external state. This predictability and consistency can lead to fewer bugs and easier testing.
2. Immutability: Functional programming emphasizes immutability, meaning that once data is created, it cannot be changed. This can help prevent unintended side effects and make the code easier to reason about.
3. Higher-Order Functions: Functional programming languages support higher-order functions, which can make the code more concise and easier to manipulate.
4. Parallelism and Concurrency: Functional programming is well-suited for parallel and concurrent programming due to its lack of side effects and focus on immutability.
5. Declarative Programming: Functional programming allows for a more declarative approach to programming, where the programmer specifies the desired result rather than the steps to achieve it. This can make the code easier to understand and maintain.

However, it's important to note that functional programming is not a silver bullet and has its own challenges, such as a steeper learning curve and limitations in handling certain types of problems. It's essential to understand the trade-offs and choose the right tool for the job. I. Functional programming provides its own value. The greatest value of functional programming over object-oriented programming is undoubtedly the correct understanding of functions. In functional languages, functions are "first-class citizens." They can be passed around like values such as 1, 2, "hello", true, and objects... at any location, through variables, parameters, and data structures, and can be called at any location. These are things that procedural and object-oriented languages cannot do. Many so-called "object-oriented design patterns" exist because object-oriented languages do not have first-class functions, resulting in each function needing to be wrapped in an object to be passed to other places.

II. Another contribution of functional programming is their type systems. The thinking about types in functional languages is often very strict. The type systems in functional languages are usually more rigid and simpler than those in object-oriented languages, helping you to perform strict logical reasoning about the program. However, type systems are a double-edged sword. If you take them too seriously, they can bring unnecessary complexity and over-engineering. I'll talk about this in more detail below.

III. "White elephants"

In this context, a "white elephant" refers to something that is revered as sacred and expensive but has no practical use. Functional languages have many excellent features, but they also have many unnecessary ones, which are similar in nature to white elephants.

The "advocates" of functional programming often believe that this world should be "pure" and should not have any "side effects." They view all "assignment operations" as low-level primitive behavior. They are very fond of tail recursion, type inference, fold, currying, maybe types, and so on. They take pride in being able to write code using these features. However, these things, while they can provide self-satisfaction and create an illusion of superiority, do not necessarily result in truly excellent and reliable code. pure function

A half-full pot of water likes to resound with a clear "ding". Many people who like to boast as "functional programmers" often do not truly understand the essence of functional languages. Once they see procedural language writing, they turn up their noses. For instance, the following C function:

int f(int x) {
int y = 0;
int z = 0;
y = 2 * x;
z = y + 1;
} many functional programming enthusiasts may wrinkle their brows at those few assignment operations, but they don't see that this is a genuine "pure function". It shares the same essence as functions in languages like Haskell, and might even be more elegant.

Those who blindly despise assignment operations don't understand the concept of "data flow". In fact, whether it's assigning values to local variables or passing them as arguments, it's essentially like putting something into a pipe or sending an electric signal along a wire. The only difference is the direction and style in which the pipe or wire is placed in different language paradigms!

##### Neglect of Data Structures

The functionally-inclined overlook another crucial, fatal issue: the true nature and importance of data structures. Some issues with data structures are "physical" and "essential", not something that can be magically disappeared by changing languages or styles. Functional language advocates blindly believe in and use lists (lists), but they don't fully grasp their essence and the time complexity they bring about. The problems with lists are not just about programming complexity. No matter how cleverly you use them, many performance issues cannot be resolved because the topology of lists is fundamentally unsuitable for certain tasks! I. Lisp List as a Linked List
From a data structure perspective, Lisp's list is a singly linked list. You can only access the next node by going through the previous one, and each time you indirectly address, it requires time. In this data structure, simple functions like length or append have a time complexity of O(n). To bypass this data structure's shortcomings, the so-called "Lisp style" advises against repeatedly appending, as the complexity is O(n^2). If you need to repeatedly add elements to the end of a list, you should first repeatedly cons, then reverse it.

Unfortunately, when you have recursive calls, the cons + reverse approach becomes confusing and prone to errors. Sometimes the list is positive, sometimes it's reversed, sometimes part of it is reversed.... This method works once, but with a few layers of recursion, you'll get lost. Despite this, some people enjoy creating challenges for themselves and embrace the "difficulties" intentionally created.

It's ironic that Lisp programmers who love using lists prefer half-empty water bottles. Real Lisp masterminds, however, know when to use records (structures) or arrays instead. In a Scheme (a modern Lisp dialect) compiler course I took at Indiana University, the professor was R. Kent Dybvig, the world's most advanced Scheme compiler Chez Scheme's creator. Our compiler's data structures (including AST) were all represented as lists. By the end of the semester, Kent told us: "Your compiler can now generate code as good as mine, but Chez Scheme doesn't just generate efficient target code, its compilation speed is 700 times faster than yours. It can compile itself in 5 seconds." He then revealed one reason for Chez Scheme's speed. One reason was that its internal data structures were not lists. At the beginning of compilation, Chez Scheme had already converted the input code into array-like, fixed-length structures. Industrial experience also taught me that arrays are indeed faster than linked lists in certain situations. Whether to use linked lists or arrays is an art.

II. The Real Value of Side Effects
The disregard for data structures and the blind rejection of side effects in the name of functional programming languages have a significant relationship. Overuse of side effects is harmful, but side effects themselves are fundamentally useful. I like to explain this to people in the following way: when computers and electronic circuits were first invented, all the wires were "pure," as logic gates and wires had no memory data capabilities. Later, someone invented flip-flops, which gave us "side effects." Side effects allowed us to store intermediate data, so we didn't have to transmit all the data through different wires to the required locations. A language without side effects is like a world without WiFi, 4G networks, or Bluetooth; all data must be transmitted through actual wires, which must be correctly connected and organized to achieve the desired results. We prefer WiFi, 4G networks, and Bluetooth for a reason. A language should not be "pure." Implications of Side Effects in Important Data Structures. One example is hash tables. Functional language advocates blindly reject the value of hash tables, claiming they can achieve the same effect with pure tree structures. However, the facts are that these pure data structures cannot match the performance of side-effect-based data structures. So-called pure function data structures require preserving old structures in every modification, leading to excessive data copying and reliance on garbage collection (GC) to eliminate these old data. It's important to note that memory allocation and deallocation both require time and energy. Blind reliance on GC leads to excessive memory allocation and deallocation in pure function data structures, making it impossible to match the performance of side-effect-based data structures. Side effects are advanced features of electronic circuits and physical support. Blind faith in and use of pure function style is in fact wasting existing physical support operations.

##### Fold and Other

Excessive use of fold and currying in code may seem cool to write, but reading it can be unnecessary pain for others. Many people don't even understand the essence of fold, yet they love using it because they think it's the "essence" of functional programming, allowing them to show off their intelligence. However, they fail to see that the essence of fold is just a "general template" for recursion on lists, requiring you to fill in three parameters to generate a new recursive function call. Therefore, every fold call essentially contains a recursive function definition on lists.

The problem with fold is that it defines a recursive function without giving it a clear name. The result is that every time you see a fold call, you need to re-understand its definition and figure out what it's doing. Moreover, fold only shows the required parts of the recursive template and hides the recursive body within fold's "frame." Compared to directly writing out the entire recursive definition, this hiding approach is actually more difficult to understand. For instance, when you see this Haskell code:

foldr
[...]

The implication is that side effects are essential for efficient data processing, while pure functions have their place in functional programming but may not always be the best choice for high-performance applications. Understanding the trade-offs between side effects and pure functions is crucial for effective and efficient programming. empty parentheses
0
[
1,

This text appears to contain empty parentheses "()" and an array "[1,]" in Chinese characters. However, since you asked for an English translation without any Chinese characters or punctuation, there is no translation possible for this text as it is already in its English form. The text only consists of special characters representing empty parentheses and an array in the format used in many programming languages. I suppose you know what it's used for? You might figure it out in a second based on experience, as it's essentially summing the numbers in [1,2,3]. Even though it only took a second, you still had to think about it. If fold contained more complex functions instead of +, you might spend a minute trying to understand it. It doesn't take much effort to write, but every time I read this code, I have to see the + and 0, which have no relation to my intention. What if someone accidentally wrote the wrong thing instead of + and 0? Why do I have to figure out the relative positions and meanings of +, 0, and [1,2,3]?

This way of writing is actually not as good as writing a simple recursive function and giving it a meaningful name (like sum), so when you see it being called, like sum [1,2,3], you know what it's doing without thinking. Defining a sum name adds a little work when writing the code, but makes reading it easier. Writing simply or coolly with fold increases the mental effort required to read the code. You should know that code is read more times than it is written, so using fold is usually a losing proposition. However, people brainwashed by functional programming can't see this. They care too much about showing off to others, so I'll use fold too!

Similar to fold is currying, Hindley-Milner type inference, etc. They seem cool, but once you carefully reason about them, you'll find that the trouble they cause is greater than the problems they solve. Some features claim to solve problems that don't even exist. Here are some functional programming language features and their pitfalls summarized: 1. fold: It's like a "recursive template," which means recursive function definitions are inserted at the call sites without giving them names. This results in the need to understand nearly the entire recursive function definition every time the code is read.

2. currying: It seems cool, but the parameters for partial application must be passed from left to right in sequence. The order of parameters has become a problem. In most cases, it's better to create a new lambda internally and call the old function instead, as this allows any arrangement of parameter order.

3. Hindley-Milner Type Inference (HM): To avoid writing parameter and return types, the programmer is given many restrictions by the type inference engine, resulting in many valid and reasonable code being unable to be written. It's actually better to have the programmer write out the parameter and return types themselves, as the workload is not significant and it can accurately help readers understand the parameter scope. The fundamental problem with HM type inference is that it uses unification algorithm. Unification, in fact, only represents the "equivalence relation" (equivalence relation) in mathematics, while subtyping in programming languages is not an equivalence relation because it does not have symmetry.

4. Algebraic Data Types (ADT): So-called "algebraic data types" are not as general as common type systems (like Java's). Many algebraic data type systems have a so-called sum type, which brings about excessive type nesting and is not as effective as general union types. Blind worshippers of algebraic data types often believe that "mathematics is a beautiful language." However, the facts are that mathematics is a historically inherited, flawed language. The language of mathematics has not undergone systematic, global design. Instead, mathematicians often write symbols randomly on the blackboard and define them as representing certain concepts, and then they are fixed.

5. Tuple: In languages with algebraic data types, there is often a construction called a tuple, such as in Haskell, which can be written as (1, "hello"), representing a type of (Int, String). This construction is often regarded as too sophisticated, leading people to use it in places beyond their abilities. In fact, tuples are just a structure without a name (similar to C's structure). Temporary use of tuples seems convenient because no structure type definition is needed. However, because tuples have no name, the members cannot be accessed by name, and it becomes tedious once there are a few more members. Tuples are usually only suitable for sizes not exceeding 2, and it is necessary to ensure that no new domains will be added later. Therefore, tuples are usually only used for simple cases and must be certain that no new domains will be added later. 1. Lazy evaluation (lazy evaluation). It seems elegant mathematically, but in reality, it has serious logical loopholes. Since bottom (dead loop) becomes an element of any type, taking each value may cause a dead loop. This makes code performance unpredictable, as evaluation is too lazy, so it may do unnecessary work temporarily and waste CPU time otherwise. Since evaluation is only done when needed, it cannot effectively utilize the computing capabilities of multiple processors.

2. Tail recursion. Most tail recursions are similar to loop statements, but unlike loop statements, they are not immediately clear about their intent. You need to carefully examine each branch's return position to determine if it is a recursion, and then judge whether this code is actually a loop. Loop statements, on the other hand, become apparent as a loop from the keyword (for, while), without having to look at the structure inside. Therefore, tail recursion equivalent to a loop is best written as a specific loop statement. However, tail recursion is useful in other situations, which are not equivalent to loops but rather "tree recursion." In these cases, using a loop often requires complex break or continue conditions, making the loop hard to understand. Therefore, both loops and tail recursion are necessary, don't always try to replace one with the other, choose the appropriate method based on the situation.

### Good Old Man

Many people avoid the "functional vs object-oriented" debate, so they become the "Good Old Man." These people have no principles and believe that any tool that can solve the current problem is a good tool. They like using shell scripts and tinkering with various Unix tools because they can obviously solve their "immediate problems."

However, this mentality is harmful. Its harm goes beyond the dichotomy of functional or object-oriented. Good Old Men, in their "flexibility," never think about how to eliminate these problems from their roots. Their so-called problems are often due to the design flaws of existing tools. Due to their "accommodating" nature, they never consider how to eliminate these problems from their roots. They patch up the heap of historical garbage, trying to build reliable software systems using poorly designed tools. The cost is significant. Not only is it labor-intensive, but it may not even solve the problem. I avoid saying "each has its own advantages" every time someone asks me about "functional vs object-oriented," because I would be easily seen as a neutral "good father" without principles. I see through the essence of various languages and their relationship. I don't see languages and programs as surface-level things during programming, but rather as something akin to circuits. I see data flow and bottlenecks, and these are unrelated to specific languages and paradigms.

In my mind, there is only one concept, which is called "programming" (programming), without any limiting qualifiers (like "functional" or "object-oriented"). My field of study is called "Programming Languages," which is not limited to a specific language or type of language, but rather all languages. In my eyes, all languages are just combinations of various features. Recently emerging so-called "new languages" are unlikely to have significant innovations. I don't like saying "invent a programming language," and I dislike using the word "invent," because regardless of how you design a language, most of its features already exist in existing languages. I prefer using the word "design," because although a language has no new features, it can still be more elegant in its details.

The most important thing in programming is to let the symbols we write be able to simply model the real or imagined "world." A programmer's most important ability is to intuitively see the correspondence between symbols and real objects. No matter how cool a language or paradigm looks, if it requires roundabout expressions to represent the model in a programmer's mind, it is not a good language or paradigm. Some things are inherently stateful, and if you insist on using a "purely functional" language to describe them, you will inevitably enter the maze of monads. In the end, you not only fail to efficiently express these side effects, but also make the code harder to understand than procedural languages. If you go to the other extreme and insist on using objects to express purely mathematical functions, you will make simple problems unnecessarily complex. Many of Java's so-called design patterns create these problems without solving any. about modeling, the model in your mind may not be the best one, and it may not have to be designed in that way. Some people don't have a clear and simple model in their minds, and they find certain languages "useful" because they can model their complex and twisted models with them. So you may not be able to explain why that language is not good, because obviously it is useful for them! As for simplifying models, that has gone beyond the scope of languages. I won't go into detail about that here.