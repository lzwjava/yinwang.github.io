---
layout: post
title: "church-turing"
---

Alonzo Church and Alan Turing were two influential figures in computer science, yet they held opposing views and had quite different reputations. Throughout my sixteen-year career in computer science, I always felt my thoughts oscillating between these two "camps." Church represented "logic" and "language," while Turing represented "physics" and "machines." In the first eight years, I knew nothing about Church, while in the second eight years, I seldom heard Turing's name mentioned. Whose views were right and whose were wrong is an unanswerable question. Completely embracing Church or completely embracing Turing seemed like a misguided approach. This is a confusing feeling, a kind of contradiction, but today I will try to summarize my understanding of it.

### The Quarrel Between Church and Turing

Every computer science student in the world must have heard of Turing and his achievements because the American Computer Science League (ACM) awards the "Turing Award" every year, which is considered the highest honor in computer science. Most computer science students learn about the principles of Turing machines in some courses (such as "Theory of Computation"). However, how many people know who Church was, what he contributed, and what their relationship was? I guess it's not even half.

If you look up the biographies of mathematicians, you'll find that Church was actually Turing's doctoral advisor. However, according to Andrew Hodges' book "Alan Turing: The Enigma," Turing seemed to have no such mentor in his mind, as if his "innovations" deserved the fame all on their own, which were allegedly stolen by Church (note the author's words: robbed). The truth of the matter is unclear. I can only say that computer science seems to have been filled with religious disputes since its inception.Although Turing is more famous now, Quine's theory still plays a major role in practical programming. In my experience, Quine's theory simplifies many things while Turing machine is excessively complex. Quine's invention of lambda calculus and subsequent work is the foundation for almost all programming languages. However, as described by old computer engineers, the earliest computer architectures were not influenced by Turing, but rather the work of some electrical engineers working independently. Interestingly, computer scientists who inherited Quine's legacy were still called the "Turing Award" winners. Roughly calculated, in all the Turing Awards given so far, researchers in programming languages accounted for nearly one-third.

### From Turing Machine to Lambda Calculus

A Turing machine remains forever in the realm of theory, and is mostly used in "Theory of Computation." Theory of Computation, in fact, consists of two main concepts: "computability theory" and "complexity theory." These concepts are usually explained using Turing machines in standard textbooks on computational theory (such as Sipser's classic text). During my studies in a Master's program in computational theory, I discovered that almost all principles of computational theory can be described using lambda calculus or the principles of programming languages and interpreters. So-called "universal Turing machines" are essentially self-interpreting interpreters, called "meta-circular interpreters." In my program language theory course at IU, my final project was a meta-circular interpreter. This interpreter could fully interpret itself and could be arbitrarily nested (meaning it could interpret itself, then interpret itself again, and so on). However, my "meta-interpreter" was based on lambda calculus, so I later found a way to fully explain almost all the theorems in computational theory using lambda calculus.

I wrote two blog posts about this discovery: "A Reformulation of Reducibility" and "Undecidability Proof of Halting Problem without Diagonalization." I rewrote almost the entire chapter of proofs from Sipser's computational theory textbook using my method and then taught it to the students in my class. Since this representation method is simpler and more precise than the usual "Turing machine + natural language" method, it had a good effect, with some students expressing a sense of enlightenment. I told Amr Sabry, my advisor at the time, about this discovery. He laughed and said he already knew. He recommended I read a book called "Computability and Complexity from a Programming Perspective," by Neil Jones (who is also the proposer of the important concept of "Partial Evaluation"). This book does not use Turing machines, but rather a language similar to Pascal yet with some features of lambda calculus (called the "WHILE language"). Using this language, Jones was able to prove all classical computation theory theorems effortlessly and even prove some theorems that cannot be proven using Turing machines.

I had long wondered why such clear explanations could be given about complex computation theory when it seemed necessary to use Turing machines, and the explanations were so complicated and vague. Due to these proofs being from the hands of experienced computation theorists, I began to doubt my own thoughts. However, after seeing Jones's book, I felt greatly relieved. It turned out that everything was as simple as this all along!

Later, I found that Robert Harper, a professor from CMU, held similar views in a blog post called "Languages and Machines." He strongly advocated for lambda calculus and opposed Turing machines and all other machines as the foundation for computation theory.

### From lambda calculus to electronic circuits

The first time I met Neil Jones at POPL in 2012, he explained many things to me. When I asked about his book, he said to me, "I wouldn't recommend my book to you because most people find lambda calculus hard to understand." Lambda calculus hard to understand? I didn't think so. I thought Turing machines were too complicated. Then I realized that, after years of research, my understanding of lambda calculus had reached a deep level, and I no longer knew how beginners felt about it. The word "simple" has completely different meanings in the minds of people with different experiences.: Jones professor was right in essence, lambda calculus may not be suitable for most people because there isn't a good introductory guide for it. Lambda calculus originated from the hands of logicians, and logicians were excellent at representing simple "programs" with formulae that resembled holy books. This is understandable for old logicians, as they created those concepts before computers existed. However, if we still use those symbols with formulas now, it might seem outdated. Most people cringe when they see beta-reduction, alpha-conversion, eta-conversion, and so on, these formulas. How could we possibly use it to understand computer theory with such headaches?

In fact, the things those symbols represent cannot surpass the objects and changes in reality. At most, they can only imagine "multiple futures" or "time machines." With the advent of computers, those symbol formulas can be represented using data structures and programming languages. Therefore, lambda calculus is very simple in my mind. Each lambda is like an electrical module. It takes input from terminals and outputs a result. What you call those terminals is irrelevant; what matters is that the same terminal name must be consistent, which is the principle of "alpha-conversion." I won't say more here, but if you want to deeply understand my lambda calculus, you might want to read my other blog post "How to Write an Interpreter," look at the beginning of the slides about type inference in that post, or further explore how to derive Y combinator, or read "What is a program?" You can also read Matthias Felleisen and Matthew Flatt's "Programming Languages and Lambda Calculi."

So, maybe you've seen the shadows of Curry and Turing in my mind. I believe Curry's lambda calculus is simpler and more powerful than Turing machines as a descriptive tool, but I've also been infected by Turing's obsession with "physics" and "machines." I believe the logicians' explanations of lambda calculus are too complex, and interpreting it as physical "electrical components" simplifies my understanding of lambda calculus, connecting it to the "real world."

In the end, the seemingly contradictory thoughts of Curry and Turing were harmonized in my mind. These profound thoughts helped me solve many problems. Thank you, ancestors of computer science.