---
layout: post
title: "Albgo Zero and Strong Artificial Intelligence"
---


Recently, there has been quite a sensation about AlphaGo (Alphago) Zero. Different from AlphaGo, AlphaGo Zero does not rely on any human go records, but starts from the rules of go and "self-learns," deriving all strategies and winning against AlphaGo in battles. Someone asked me if AlphaGo Zero changed my view on artificial intelligence. My answer is: no.

I must admit that AlphaGo Zero is an important achievement. In my previous articles, I couldn't express my appreciation for it, and some people might have misunderstood me, thinking that I was dismissive. That's not the case. I respect DeepMind for their efforts, just like I respect anyone who makes significant contributions for us. The success of AlphaGo and AlphaGo Zero for me is like a new medicine that cures a disease we couldn't fight against before, and possibly opens up broader fields. Its significance is undeniable.

But does it change my view on artificial intelligence? No, it doesn't. The person asking this question might just be hearing rumors, yet not having read my 'Why I Don't Care About Artificial Intelligence' (briefly referred to as 'Intelligence') article, or with a mischievous intent, failing to understand the argument. Those who have read and understood my 'Intelligence' article might have realized that the arguments in it can be directly applied to AlphaGo Zero without any adjustments. AlphaGo Zero achieved what a mechanical system excels at – setting goals – and that's not surprising. There's no understanding or intelligence in this activity, just like calculating multiplication.

In fact, the emergence and success of AlphaGo Zero were expected. To create a machine that beats human chess champions, it shouldn't require human go records in the first place. It's just like creating a calculator, which shouldn't require many human account books. We should have been able to create a machine that surpasses all human accountants based on the rules of arithmetic, right? If, years after AlphaGo, no one had created something like AlphaGo Zero, I would have been quite surprised. For me, this thing should have been made in the AlphaGo Zero form from the start. I am able to confirm my statement that Go is inherently mechanical, as Alfalfa Dog Zero can play without relying on human Go scripts. This is because: Go is a mechanical activity by nature, just like hand calculating multiplication. It is not the highest level of human intelligence. I have held this view for a long time, and that's why I don't enjoy playing board games. This tedious mechanical activity hurts my brain, is not conducive to physical health, and is prone to error. It's a shame that a machine had to be made for this purpose.

I am glad that the team behind Alfalfa Dog has disregarded this dull yet revered domain. Alfalfa Dog Zero has solved the problems of Go, but Go does not represent human intelligence. Using the analogy of a "wonder drug," although this great new drug cures a stubborn disease, we cannot conclude that it is a panacea.

Every time I bring up these points, someone is very firm in their belief: "Although that may be so, I believe that strong artificial intelligence will definitely be achieved. Maybe not in a few decades, a few hundred years, or even a thousand years, but it is ultimately possible." Their notion of "strong artificial intelligence" is the complete realization of "human intelligence" by machines.

What is my take on this "strong artificial intelligence belief"? In general, I don't particularly like people who have the strong belief that "X will definitely be achieved." This belief robs them of reason. You say "will," you say "ultimately," but that is just a belief, not a fact. People with a scientific mindset should accept the possibility that "some things cannot be achieved." Otherwise, they might keep building perpetual motion machines based on their belief. As the saying goes: "Everything is possible, so it is also possible 'not possible'" :)misunderstood, I didn't say super intelligent AI is "perpetual motion machine"...... In fact, it's not even that :p

Why so? Because "perpetual motion machine" has a precise definition, while "super intelligent AI" doesn't. The concept of "super intelligent AI" is meaningless in itself, apart from providing fodder for artificial intelligence experts to philosophize like philosophers. No one knows what "human intelligence" is, so we have no business talking about "super intelligent AI," let alone creating it with machines to realize "human intelligence." Saying super intelligent AI will definitely realize it is the same as saying: "There's something we don't know what it is, but it will definitely realize!" Isn't that absurd?

So, engineers who want to build a perpetual motion machine will inevitably fail, but they at least know what they're building. In contrast, engineers who want to create super intelligent AI don't even know what they're building....

Let me give an example to illustrate how ridiculous this is. If we send a car through a time machine to the Stone Age, Stone Age people see this car running and have a lot of power, but they can't take it apart. They use it every day to transport various things, observing its operation, but they can't see how it works inside. They don't know there's an engine inside, let alone its structure, or that there's gasoline....

In short, the internal structure of the car is a black box. When will these Stone Age people be able to build their own cars? After a few decades, a few hundred years, a few thousand years, or a few hundred million years?: My answer is: We are not qualified to discuss this issue. Some indigenous people stumbled upon a car, just like the story in "God is a Bullet." Suddenly, someone started asking "When will indigenous people be able to make their own cars?" Why don't we talk about other things? There are so many interesting stories to tell, why does it have to be about indigenous people making their own cars? Maybe they destroyed themselves before making a car? :p

Regarding "strong artificial intelligence," my view is similar. Until we truly understand what human intelligence is, the term "strong artificial intelligence" does not have a precise definition. Therefore, discussing whether strong artificial intelligence can be achieved, when it can be achieved, is meaningless. We are not qualified to discuss this issue.

(If you like this article, consider purchasing it.)