---
layout: post
title: "Turing's Aura"
---


It seems that everyone in the world knows, Turing (Alan Turing) was a genius, the one who created computer science, the one who cracked the German Enigma code. He was called the "father of computer science." Due to his outstanding contributions, the highest honor in computer science is called the "Turing Award." However, based on his long-held views on Turing machines and other calculating models, as well as some historical data, I found that Turing's actual achievements were quite far from the reverence he received.

Due to the secrecy measures taken by various governments regarding espionage work during the war, combined with the pity generated by Turing's unfortunate birth, the name Turing seems to have acquired a hazy, disorienting aura. Many contributions that were not actually made by Turing were attributed to him, and ordinary contributions were excessively magnified. Turing's aura obscured many people who made more significant contributions to these fields.

#### Turing's Legacy

In 2012, on the centenary of Turing's birth, people held numerous grand events to commemorate this "father of computer science." Many media outlets also exaggerated his achievements. There was also a man named Andrew Hodges who seized this opportunity to sell his biography of Turing, "Alan Turing: The Enigma." This book became a sensation, and was later adapted into a film. I wrote this article primarily because of this biography. The author's partial glorification of Turing's contributions and disparaging treatment of other scholars left me feeling unequal. Turing's reputation in the computer field was already significantly exaggerated and idolized by many, and now with this biography and film, the misunderstanding has been further perpetuated. I believe it's necessary to clarify some facts.

Regarding cryptography, many people attribute the story of the Enigma machine's decryption entirely to Turing, disregarding others. In reality, decrypting the Enigma code was the result of collective efforts, with Turing being one of the contributors. The absence of any one of these individuals could have led to disastrous consequences. Many of their ideas preceded Turing's, influenced him, and designed things more advanced than Turing's, yet few are familiar with their names. Turing had his contributions, but it seemed as if he single-handedly saved everyone, which is unfair.

The initial breakthrough in cracking the Enigma code was not made by the British but by the Poles. The Poles not only intercepted and replicated the German Enigma machine but also discovered a subtle flaw, inventing a machine for decryption called BOMBA and a manual decryption method called Zygalski sheets. BOMBA could decrypt Enigma codes within two hours. The Poles silently intercepted German communications for six and a half years, and before the war broke out, they passed this technology on to their Allied friends.: The principle of BOMBA's work is to simulate several Enigma machines simultaneously (in parallel), enabling a faster key discovery. Initially, this worked, but later, the Germans improved the Enigma machine by increasing the number of selectable wheels from three to five, increasing the key space by 60 times. In theory, BOMBA would need to run 60 times more Enigma machines to decrypt this increased key space, but this exceeded Poland's resources and manpower. With the Germans advancing, Poland had to seek help from their allies.

Turing's most significant contribution was improving the Polish BOMBA and designing a better machine called BOMBE. BOMBE did not have a major leap in quality compared to BOMBA, but it simulated more Enigma machines in parallel and turned faster. Additionally, it introduced some optimization measures to exclude unfeasible paths earlier, significantly increasing speed. Turing's initial design required the ability to guess long texts in advance, making it essentially unusable. Later, a circuit called the diagonal board, invented by Gordon Welchman, enabled Bombe to be put into practical use. For more information on Gordon Welchman's story, you can refer to this BBC documentary.

Before Bombe could be put into use, there was a man named John Herivel who discovered a special technique called the Herivel tip. This technique was put into practical use several months before Bombe, decrypting many German messages, and earning significant merit. If the Herivel tip had not been discovered, the Allies might have been defeated in May 1940, and BOMBE would have had no chance to make a difference.

Meanwhile, at Bletchley Park, a large-scale programmable electronic computer called Colossus was born. It was designed by a engineer named Tommy Flowers. Colossus was not used to decrypt Enigma codes but to decrypt the Lorenz SZ-40, a more advanced cipher machine used for transmitting Hitler's highest commands.

Later, the Germans further improved their communication methods, using a four-rotor Enigma machine. This significantly increased the difficulty of decryption, making the standard Bombe machine incapable of cracking it. Later, Harold Keen designed a machine called Mammoth, and with the help of the US Navy, faster Bombe machines were manufactured, enabling its decryption.: Therefore, you saw, all these people's work combined, improved the situation during World War II. The Polish BOMBA included the most important thoughts of the BOMBA. Turing's work was more about quantitative improvement than qualitative leap. Nowadays, many people like to follow the trend and exaggerate Turing's role in it, which is not right. If you are interested in the technical details of the Enigma machine, you can refer to these two videos:[Video1][Video2].

Theoretical Computer Science:

Turing is known as "the father of computer science," and the highest honor in the computer science community is called the "Turing Award" (Turing Award). However, if you delve deeper into computer theory and programming language theory, you will find that Turing did not have a lasting and beneficial impact on theoretical computer science. In a way, he actually helped out. Turing's theories were too complex, causing great confusion and hindering the development of computer science. Moreover, his attitude towards publishing papers made me question him, I feel that Turing himself was the ancestor of some unhealthy trends in today's computer academic world.

#### Turing Machine and Lambda Calculus

Most computer science professionals will associate Turing with the Turing Machine when they hear his name. A somewhat knowledgeable person may know that the Turing Machine and Lambda Calculus are equivalent in computational power. However, being equivalent in computational power does not mean they have the same value, using either one randomly is irrelevant. There is a common principle in scientific research: if multiple theories can explain the same phenomenon, take the simplest one. Although Lambda Calculus and the Turing Machine can express the same theory, Lambda Calculus is simpler, more elegant, and more practical.: The field of Theoretical Computer Science (Theory of Computation) is actually complicated due to Turing machines. The design of a Turing machine is complex and lacks principles. Its read-write head, tape, state, and operations make what was originally simple semantics exceptionally complex. The Turing machine performs read-write operations simultaneously, which is the most forbidden error in programming, similar to C language's i++. Turing machines are so complex and confusing that it's hard to tell what they're supposed to do and it's hard to express oneself clearly using them. That's why everyone gets a headache during "Theory of Computation" courses. If you dig a little into history, you might find that the prototype of the Turing machine was actually a typewriter used by Turing's mother. Modeling all computation with a typewriter is certainly feasible, but it's too complex.

On the contrary, lambda calculus is simpler, more elegant, and practical. It is a very principled design. Lambda calculus not only clearly shows what you want to express, but it also has a direct "physical implementation." You can naturally consider a lambda calculus expression as an electronic circuit module. For real programming language design and system design, lambda calculus has tremendous guiding and inspiring significance. In fact, many people who understand lambda calculus are baffled by the significance of Turing machines besides making some theories seem profound and incomprehensible.

#### A Historical Reversal

Turing machines, compared to lambda calculus, are actually a historical regression. In 1928, Alonzo Church invented lambda calculus (he was 25 years old at the time). Lambda calculus was designed as a general computational model and not born to solve a specific problem. In 1929, Church became a professor at Princeton University. In 1932, Church published a paper in Annals of Mathematics correcting several common issues in logic, using lambda calculus in his arguments. In 1935, Church published a paper using lambda calculus to prove that there are undecidable problems in number theory. In 1936, Church published a two-page note indicating that his 1935 paper could be used to prove that Hilbert's "Entscheidungsproblem" was undecidable.

In May 1936, a student at Cambridge studying for a master's degree named Turing also wrote a paper using a machine he designed (later called a Turing machine) to prove the same problem. Turing submitted his paper, but it was published a year later than Church's earliest conclusions. The editor had never seen anything like Turing's machine and found it far less elegant than lambda calculus. Just like everyone's first impression of Turing machines, the editor found it hard to believe that this typewriter-like operation could contain "all computation." The editor couldn't determine the correctness of Turing's argument and had to consult someone. Church was probably the only person in the world who could verify the correctness of Turing's paper, so the editor wrote a letter to Church: "This young man named Turing is very smart. He wrote a paper using a machine to prove the same result as yours. He will send you the paper. If you find his results are correct and useful, please help him get the scholarship, join Princeton University, and study under you." I. Turing's Student of Church

Turing became Church's student in this manner, yet Turing was arrogant and probably never regarded Church as a teacher. Instead, he always felt that Church had gotten ahead of him, ruining his chance at a storied academic legacy. Unlike other students of Church, Turing could not grasp the essence of lambda calculus, yet he believed his machine was the greatest invention. After entering Princeton, Turing did not hesitate to seek guidance, but only with the intention of publishing his own paper, hoping to pique interest in his "machine." However, he suffered significant setbacks. After all, an unknown person creating a strange machine that claimed to encompass all computation in the universe was not taken seriously in the scientific community.

1937 marked the publication of Turing's paper (titled "Computable Numbers"). Church still held Turing in high regard and named Turing's machine the "Turing machine." Unfortunately, the academic community showed little response to the publication of the paper, with only two individuals requesting a copy from Turing. Unsatisfied, Turing then went on to promote his Turing machine everywhere, trying to convince people that it was a great invention. With a hammer, everything looks like a nail. Later, at every location and every project (as detailed in the next section), he tried to connect the problem to his paper and Turing machine, proving their value, even claiming that everything others invented was inspired by the Turing machine... After a long period of rumors and misunderstandings, he eventually succeeded.

Turing's methods from that era were not much different from the common practices in today's computer science academic world. I wanted to publish my idea A, but someone had already published B, which solved the problem that A aimed to solve, and did so in a simpler and clearer way. What should I do? First, I declare that I had never read B's paper, allowing me to be labeled as an "independent discovery." Then, I prove that A and B are equivalent in essence. Lastly, I dig into the limitations of B and highlight the advantages of A in certain edge cases... By repeatedly manipulating this process, seeking the advantages of A, I would eventually publish successfully. Once published, people would sing my praises, even calling worthless things valuable. They would build upon A and eventually elevate me to the status of a master. The earlier, simpler, and more beautiful B, however, was soon forgotten. Victory!

Now, I must address the distortion in "The Turing Biography." Church's paper was published a year earlier than Turing's, and Church used a simpler and more elegant computational model than Turing's machine. Church's achievements should have naturally received more respect, but the author instead said: "... and Turing was robbed of the full reward for his originality" (see Chapter 3, "The Turing machine"). It seems as though Church stole Turing's "originality" unfairly. But in reality, it was Turing who stole Church's originality. Today, when people mention Hilbert's Entscheidungsproblem and computational theory, they think of Turing, not Church, and few know that Church was the first to solve this problem or that he used a more beautiful method.1. Due to Turing selling his theories everywhere, turning bad things into good, forcing others' inventions onto his theories, claiming they were inspired by the Turing machine, leading many people to be deceived, believing it is indeed superior to lambda calculus. Moreover, many people, including many so-called "theoretical computer scientists," have not properly understood lambda calculus. They misinterpreted the words, regarding the Turing machine as a "physical" and practical "machine," while lambda calculus is just a theoretical model.

2. In fact, it's the opposite: lambda calculus is very practical, its essence is similar to electronic circuits. Almost all real-world usable programming languages, the semantics of all of them can be explained using lambda calculus. However, the Turing machine is not very practical, it's clumsy to use, so it can only be a model in theoretical computer science. Furthermore, lambda calculus can completely replace the Turing machine in the field of theoretical computer science, as it not only can express all the theories that the Turing machine can express, but it can also express them more simply and precisely.

3. Many theoretical computer scientists like to use the Turing machine, as if using it as a model makes their theories seem profound and incomprehensible. Regular computer science textbooks, which often use the Turing machine as their computational model, use laborious methods to derive various computability and complexity theories. Especially classic textbooks like Michael Sipser's Computational Complexity, which are difficult to understand and confusing, sometimes making me doubt whether the author understands those things himself.

4. Later, I found that all the theories that the Turing machine can express can be represented using simpler lambda calculus (or any currently popular programming language). The state of each Turing machine corresponds to an "AST node" in lambda calculus (or some programming language), but expressing these theories with lambda calculus can be much clearer and easier. When I was a teaching assistant at Indiana University for a computer science course, I quietly told the students this way of thinking, and they generally expressed that my thinking method was easier to understand and closer to practical programming.

5. Here's a simple example. I can express Hilbert's "undecidability problem" unsolvability in one line of lambda calculus expression: λx.λy.(x y x). This shows that the Turing machine's ability to express theories can be expressed much more clearly and easily using lambda calculus. I was under the impression that I was the only one who knew this secret, until one day I told it to my PhD advisor Amr Sabry. He laughed and said, "Ha ha! I knew it long ago. You can refer to Neil Jones's book, called 'Computability and Complexity: From a Programming Perspective'. This book is now freely downloadable.

This book's author explains the theories that are usually described using Turing machines in a very simple programming language. He found that describing computation theory in a programming language not only simplifies and clarifies, but also allows for a more precise description of theorems that Turing machines cannot describe. Obtaining this book made me feel like finding a treasure, as I discovered there was someone with a similar perspective to mine in the world.

At a conference, I had the opportunity to meet Neil Jones and discuss ideas with him. When the topic of his model's relationship with Turing theory came up, the professor humbly said, "Turing's model still has its value...." However, he was unable to clarify what this value was. I was clear in my mind that he was being modest to avoid causing religious conflicts or appearing arrogant. The professor in front of me, although he had never received a Turing Award and was rarely heard of, understood the fundamental nature of computation at a level higher than Turing himself. overall, a Turing machine is not worthless in essence, but due to lambda calculus being able to explain more clearly what a Turing machine can represent theoretically, the value of a Turing machine is relatively close to zero. Church noted in his review of Turing's paper in 1937 that the advantage of a Turing machine lies in its ability to make it understandable for people who don't understand much mathematics or lambda calculus and the like. But don't I find it more painful for people who don't understand much mathematics to understand a Turing machine? And even if it has some benefits for the understanding of "outsiders" or "dumb people," its value seems rather insignificant, haha:P

##### Electronic Computing Machines

Many "theoretical computer scientists" love to say that the computers we use now are just "Universal Turing Machines." Even if you don't know who Turing is at all, they will always say: "You were inspired by Turing because your thing is equivalent to a Turing machine, it is complete according to Turing..."

So let's take a look at Turing and his theory, and see if they had a significant impact on the development of electronic computing machines. If a person has had a major impact on an industry, we can say "we couldn't have done it without him." However, the fact is that even if Turing didn't exist, computer technology would still have developed as it is today, without any influence whatsoever. Take a look at history, you might be surprised to find that Turing's theory didn't inspire the design of any computer, and Turing himself designed the only computer (ACE), which ended in tragic failure.

###### What is a Universal Turing Machine (UTM)

## 图灵机

图灵机是一种理论上的计算机模型，它可以计算任何可计算问题。它被认为是计算机科学中最基本的理论之一，因为它可以证明其他计算机模型的等价性和完备性。图灵机本身并不是一种实际存在的计算机，而是一种理论上的概念。

### Turing Machine

A Turing machine is a theoretical computer model that can compute any computable problem. It is considered one of the fundamental theories in computer science because it can prove the equivalence and completeness of other computer models. A Turing machine itself is not a practically existing computer, but rather a theoretical concept. ACE's failure was due to Turing's excessive emphasis on his invention, the Universal Turing Machine (UTM). So let me first demystify this supposedly omnipotent, able-to-be-talked-about-about-anything UTM.

To put it simply, UTM is an interpreter, just like Python or JavaScript interpreters. A computer's processor (CPU) is also an interpreter, designed to interpret machine instructions. Therefore, any programmable machine with an instruction set is a UTM according to Turing's theory. So did Turing's theory inspire all these machines? Feel free to argue with me :)

You should know that the concept of an interpreter existed in Church's lambda calculus before Turing's UTM. Therefore, UTM is not a new thing, and it is indeed ugly and complex compared to the interpreter in lambda calculus. And Church was not the first to propose the concept of an interpreter. As for general concepts like this, it's hard to trace who "invented" it. Maybe it wasn't invented by one person, but rather a collective effort throughout history.

The concept of an interpreter is all-encompassing and omnipresent. Any "programmable" machine inherently contains an interpreter. An engineer, unaware of the concept of an interpreter, might still accidentally design a programmable machine. So if you attribute all of these to Turing or Church, it's a stretch.

#### The Story of Turing and ACE
[Turing and ACE's Story]

Turing's work on the Universal Turing Machine (UTM) and the concept of a general-purpose computer laid the groundwork for Alan Turing's development of the Automatic Computing Engine (ACE). ACE was designed to be a more practical and efficient version of the UTM, capable of performing calculations at a much faster rate than its predecessor.

However, Turing's focus on the UTM and his theoretical work led him to neglect the practical aspects of building ACE. As a result, the project faced numerous challenges, including funding issues and delays in its development. Despite these obstacles, Turing continued to work on ACE, hoping to see his vision of a general-purpose computer come to fruition.

Meanwhile, other researchers and engineers, such as John Atanasoff and Clifford Berry, were also working on building early electronic computers. Their efforts, combined with Turing's theoretical work, eventually led to the development of the first practical computers, such as the Electronic Numerical Integrator and Computer (ENIAC) and the Colossus.

Turing's contributions to the field of computer science were significant, but it's important to remember that the development of computers was a collaborative effort involving many brilliant minds. Turing's focus on the UTM and his theoretical work were crucial, but the practical aspects of building a working computer were equally important.

In the end, the story of Turing and ACE serves as a reminder of the importance of both theoretical and practical work in the field of computer science. While Turing's UTM laid the foundation for the concept of a general-purpose computer, it was the practical efforts of engineers like Atanasoff and Berry that brought the first working computers to life. In fact, the earliest electronic computers were not Turing machines, but rather the result of collaboration between electronic engineers and some mathematicians. According to old engineers' accounts, Turing's work and theories had hardly any positive impact on the practical design of electronic computers. Many engineers didn't even know who Turing was or what a Turing machine was. They just designed and manufactured those circuits based on real requirements. That's why today's electronic computers are hardly related to Turing machines or Turing's other theories.

The first two electronic computers, ENIAC and EDVAC, were both designed and manufactured in the United States. The design report for EDVAC was participated in and signed by von Neumann. Regarding EDVAC's design, there is an interesting introduction in "The Essential Turing" which says: "von Neumann was well acquainted with Turing's groundbreaking invention—UTM. EDVAC's design was undoubtedly inspired by UTM since it put instructions and data in the same storage space. However, the design report of EDVAC did not mention Turing or UTM at all, let alone cite Turing's seminal paper 'Computable Numbers'...."

This is a rather ambiguous statement. Von Neumann and the EDVAC team copied Turing's research findings? If that's the logic, then my washing socks and underwear in the same bucket while doing laundry was also inspired by Turing, just because UTM only had one tape? The world is full of things that are supposedly inspired by UTM. This reminds me of certain companies that make a living solely by patent litigation.... Von Neumann, as a mathematician of his generation, had far more significant research achievements than Turing. Would he care about copying Turing's work? I suspect they probably didn't even consider Turing and his papers. Moreover, Church had similar ideas to UTM earlier, and they were better and simpler. Before they stole Church's limelight, they are now stealing Neumann's? Hmph, I can't stand these people who only have one good idea in their lives....

So, when they say Americans made EDVAC, Turing became envious, resentful, and jealous. Eventually, his chance came. A few months after EDVAC was born, the National Physical Laboratory (NPL) in the UK contacted Turing. They wanted to catch up with American computer technology development, so they wanted to recruit Turing to help design an "English version" of EDVAC. The machine Turing designed was called ACE (Automatic Computing Engine). Initially, Turing gave NPL a very grand blueprint: ACE could be so powerful that one machine would suffice for the entire UK, which we could call it "the National Computer of the UK".... However, no matter how grand the slogans, they could not escape the reality check. The ACE project eventually ended in failure.

The "Turing Biography" blamed NPL and others for myopia and bureaucracy for ACE's failure, but the primary responsibility for ACE's failure was actually Turing himself: he lacked the basic design skills for a practical computer but set ambitious goals. Turing's design differed greatly from all practical computers, then and now. As you might have expected, his initial design ideas were based on his UTM, but he removed some impractical designs, such as using a tape to store data. This improvement seemed reasonable, but he also added some designs that left engineers speechless, under the name of "minimalism." For instance, ACE's hardware only provided AND, OR, NOT, and other logical operations as "basic operations," while all other arithmetic operations, including addition, subtraction, multiplication, and division, were all implemented through code. Turing Master, do you know about an essential concept called "efficiency"? I'll give you an English translation of the text without Chinese characters or punctuation:

This is not even the beginning.... Later, he became even more eccentric, eventually bringing up "thinking machines." He wanted ACE to become a machine that could think like a human and even write its own code. According to Turing's own words: "In ACE's work, I am more interested in modeling human brains than in actual computing applications." He had already turned ACE into his own personal toy, rather than a tool for solving people's practical needs. Anyone who opposed this idea, he would mockingly say, you're afraid my machine is too smart, it took your job? In truth, Turing knew very little about the actual workings of human brains, being at a level comparable to a middle school health textbook. Yet he loved to tell people, the human brain is just another UTM. Look, it has input, output, state transitions, just like a UTM.... The so-called "Turing Test" was proposed at that time. Of course, because he mentioned "thinking machines," he was later called the ancestor of AI. However, the Turing Test couldn't possibly prove that a machine had acquired human intelligence, it only tested some superficial aspects. Later, "thinking machines" became a common euphemism, used to raise large research funds, and all ended up in a dead end.

Turing designed this machine, but NPL at the time didn't have the ability to manufacture it. So they turned to two engineers who had previously implemented computers: F. C. Williams and Maurice Wilkes (later the designers of the EDSAC computer). Williams and Wilkes both expressed dislike for ACE's design and pointed out that Turing's personality and research style didn't match theirs, so they both refused NPL's invitation. Finally, NPL established an electronic department, and ACE's engineering could finally begin. However, according to senior engineers' discussions, the idea of creating an "electronic brain" and "intelligent machine," as proposed by Turing, was not practically feasible or capable of making a significant impact in the short term. Turing was very angry about this and publicly opposed NPL's decision.

Eventually, engineers and management couldn't take it anymore, given Turing's reputation outside, they couldn't dismiss him, so they proposed an unprecedented plan: NPL would sponsor Turing to return to Cambridge University for a sabbatical to do pure mathematical research. So ACE finally started construction in Turing's absence.... 1950, ACE ran its first program. However, the ACE implemented by engineers was completely different from Turing's design, with hardly any similarity between the actual machine and Turing's design. A year later, Turing wanted to return to NPL to continue influencing ACE's design, but NPL's leaders suggested he continue his research at the University of Cambridge and gave him a position. Turing accepted this offer, and everyone let out a sigh of relief....

Turing's only computer design, ACE, ultimately ended with Turing completely withdrawing from the entire project. Looking back today, if Turing had stayed, NPL might have built ACE according to Turing's intentions. However, due to Turing's impractical design and arrogant personality, NPL lost the help of one of its most excellent people. In 1949, Maurice Wilkes successfully built EDSAC according to the EDVAC concept, faster than ACE and more practical.

If you're interested in ACE and other early computers, you can refer to more detailed information. You can also read "The Turing Years," although it has ridiculous views, excessive flattery towards Turing, but the communications between Turing and others, the basic facts, he should not be ashamed to alter. I say these things for what purpose? I'm not intending to deny the contributions of Turing. Like many computer workers, his work was certainly meaningful. However, the significance of his work is not as great as many people boast, and it didn't even include much innovation.

I feel that the halo that many people have placed on Turing has concealed too many other people who are worth learning from and respecting, leading people to have a misconception about the concept of computer science. Computer science was not created by Turing alone, Turing was not the founding father of computer science, and he was not even the most crucial person in the process of cracking the Enigma code and the birth of electronic computers.

Countless computer scientists and electronics engineers have built today's computer science. Their intelligence and contributions should not be hidden by Turing's halo, they deserve the same respect. I hope everyone will stop idolizing Turing, and stop idolizing anyone. Don't miss the opportunity to learn from others because of idolizing certain people.