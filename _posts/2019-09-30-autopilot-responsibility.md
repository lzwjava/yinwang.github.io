---
layout: post
title: "Autopilot Responsibility"
---

: The responsibility and risk analysis of self-driving cars

When it comes to "self-driving cars," the most familiar name is probably Elon Musk of Tesla. He often exaggerates Tesla's Autopilot capabilities, leading people to misunderstand its abilities. After an accident caused by Autopilot resulted in fatalities, Musk often deflects responsibility online by grabbing at various excuses such as "the driver did not take control in time."

Many people find his statements absurd and angry but struggle to clearly explain where he went wrong, even regulatory agencies are powerless against the distortions of each self-driving car company. In this article, I attempt to use logic and probability tools to analyze the responsibility and risk issues of self-driving cars. Although the mathematics used may not be very detailed, you can gain insights into the approach to analyzing such issues. Logic reasoning and probability analysis can be used not only in scientific research but also in law and various social phenomena.

From Musk's interviews, you can see that his statements generally contain the following content:1. Autonomous driving is about to be realized soon, the visual recognition capability of Autopilot is exponentially increasing, so autonomous driving is right in front of us.
2. Cars currently produced by Tesla already have the hardware capable of "full self-driving" (FSD) installed. Once we update the software in the car in the near future, you will own a car with autonomous driving capabilities. Buying a Tesla car now is an investment rather than a depreciating asset.
3. Statistics show that the accident rate of Autopilot is significantly lower than that of human drivers. Autopilot is safer than human drivers, which is an indisputable fact. If you deny this fact, you are endangering public safety.

However, various evidence shows that Autopilot has hardly any self-driving capabilities. Yet, Elon Musk continues to tout these fallacies. Many tech-savvy nerds believe his "accident rate" and cheer for his so-called "high tech," with some even parroting the claim that "Autopilot is 6 times safer than human drivers." These people do not understand that statistics are useless for analyzing liability in accidents or assessing the risks of Autopilot. Moreover, their statistical methods and ways of interpreting statistics are incorrect.

In this article, I want to emphasize the following points:

1. Autonomous driving is not yet a reality, and Autopilot's capabilities are far from it.
2. Tesla's claims about Autopilot's safety are misleading and potentially dangerous.
3. The hype around Autopilot and other self-driving technologies can obscure the real issues and risks.
4. It is essential to approach the development and implementation of self-driving technology with caution, transparency, and a critical perspective.
5. The focus should be on improving safety and reducing human error in driving, rather than relying solely on technology.
6. Regulations and standards must be in place to ensure the safety and reliability of self-driving technology.
7. The public should be educated about the limitations and risks of self-driving technology, as well as the importance of human oversight and intervention.
8. The automotive industry, governments, and other stakeholders must work together to address the challenges and opportunities of self-driving technology, with a focus on the greater good and the long-term benefits for society.1. Statistics numbers in a broad sense have no relation to "responsibility" and "risk" analysis.

2. Tesla Inc. is legally liable for any car accidents caused by Autopilot.

3. Requiring the driver to "take over at any time" is an attempt to shift responsibility, which is completely against logic.

4. The interpretation of accident fatality data in the autonomous driving industry is one-sided and incorrect. Deaths caused by accidents are not particularly severe problems compared to other causes of death. Autonomous driving technology cannot significantly reduce accident fatalities. Elon Musk and many self-driving companies often use the term "accident rate" to argue that autonomous driving is safer than human drivers, as statistics show their accident rates are lower. In reality, this is similar to saying: "I have lived for so long and served so many customers, I haven't killed anyone, so the probability of me killing you is very low, lower than the national murder rate, so it's not a big deal that I killed you now."

Leaving aside whether Autopilot's accident rate is truly that low. Even if it is, does that mean that if someone is killed, they are not responsible or exempt from blame?

What is Tesla's responsibility in this matter can be analyzed using the logical causality relationship "counterfactual analysis" (counterfactual analysis). If the driver had not used Autopilot but driven himself, would this accident still have occurred? If it would not have occurred, we have a causality relationship: Autopilot caused the accident. Irrespective of whether other people using Autopilot have had accidents and what proportion they occupy, this causality relationship is irrelevant. The causality relationship equals responsibility.

If Autopilot caused the accident, even if only one accident has occurred, Tesla, the designer of the Autopilot, should be held responsible. Many people confuse "responsibility" and "accident rate," which is why they continue to support Elon Musk and Tesla's flawed arguments. Some people believe that "autonomous driving may reduce the national accident rate," and therefore consider the problem of a few accidents caused by Autopilot to be insignificant, without realizing that "accident rate" and "responsibility" and "risk of accident recurrence" are completely different things.

Furthermore, if you see through the exaggerated "machine vision ability" hype, you will realize that the statement that "autonomous driving will reduce the accident rate" is not even feasible. I why do I emphasize "responsibility"? Because if a person accidentally causes a car accident and injures himself while driving himself, he can accept it because it's his responsibility. However, if Autopilot makes a mistake and causes an accident, injuring himself, for the car owner, this is unacceptable and he must hold Tesla company accountable.

Isn't this logic clear to everyone? This is similar to getting into an accident while driving yourself, versus an accident caused by a taxi driver. You would sue the taxi driver, not yourself. Simple, isn't it?

Every time there is an incident related to Autopilot, Tesla company always spreads news afterwards that the driver was not paying attention, his hands were not on the steering wheel ready to "take over," so it's not Autopilot's responsibility. Whether the driver was paying attention or not, a life was lost with no evidence, but all of this became an excuse for Tesla to evade responsibility.

If you find that Autopilot makes a mistake, could you really take control in time, and make the correct response in such a short time? Even if both your hands were on the steering wheel, if the car got too close to an obstacle without slowing down, would you realize it made a mistake and decide to take over? It's likely that by the time you took control, it was already too late. Therefore, requiring the driver to take control at all times is an unreasonable demand, and should not be used as Tesla's excuse to evade responsibility.

### Personal Risk Analysis for Autopilot
[

For the record, here's a more polished translation:

Why do I stress the importance of "responsibility" in the context of Autopilot? Because if a person accidentally causes a car accident and injures himself while driving, he can accept the consequences because it's his responsibility. However, if Autopilot makes a mistake and causes an accident, injuring the driver, the driver would find this unacceptable and would hold Tesla accountable.

Isn't this a logical conclusion? This is similar to getting into an accident while driving yourself, versus an accident caused by a taxi driver. In the former case, you would sue the taxi driver, not yourself. Simple, isn't it?

Every time there is an incident related to Autopilot, Tesla quickly disseminates news that the driver was not paying attention or his hands were not on the steering wheel, ready to "take over." The driver's attentiveness is irrelevant to the outcome - a life was lost with no definitive evidence. However, Tesla uses these circumstances as a shield to evade responsibility.

If you detect an error in Autopilot's judgement, could you really take control in time and make the correct response? Even if both your hands were on the steering wheel, if the car got too close to an obstacle without slowing down, would you realize the mistake and decide to take over? It's unlikely that by the time you took control, it would already be too late. Therefore, requiring the driver to take control at all times is an unrealistic demand, and should not be used as Tesla's excuse to evade responsibility.

### Personal Risk Analysis for Autopilot

For the record, here's a more polished translation:

I'd like to emphasize the concept of "responsibility" in the context of Autopilot. If a person accidentally causes a car accident and injures himself while driving, he can accept the consequences because it's his responsibility. However, if Autopilot makes a mistake and causes an accident, injuring the driver, the driver would find this unacceptable and would hold Tesla accountable.

This logic is clear, isn't it? This is similar to getting into an accident while driving yourself, versus an accident caused by a taxi driver. In the former case, you would sue the taxi driver, not yourself. Simple, isn't it?

Each time there's an incident involving Autopilot, Tesla swiftly spreads the news that the driver was not paying attention or his hands were not on the steering wheel, ready to "take over." The driver's attentiveness is irrelevant to the outcome - a life was lost with no definitive evidence. However, Tesla uses these circumstances as a smokescreen to evade responsibility.

If you detect an error in Autopilot's judgement, could you really take control in time and make the correct response? Even if both your hands were on the steering wheel, if the car got too close to an obstacle without slowing down, would you realize the mistake and decide to take over? It's unlikely that by the time you took control, it would already be too late. Therefore, requiring the driver to take control at all times is an unrealistic demand, and should not be used as Tesla's excuse to evade responsibility.

Personal Risk Analysis for Autopilot:

1. Autopilot's ability to detect and respond to obstacles: Autopilot's sensors and algorithms may not be able to detect all obstacles, especially those that are small, stationary, or difficult to distinguish from the road or surrounding environment.
2. Driver's reaction time: Even if the driver notices an error in Autopilot's judgement, it may take too long for them to react and take control of the vehicle.
3. Autopilot's reliance on the driver's attention: Autopilot requires the driver to remain attentive and ready to take control at all times. If the driver becomes distracted, Autopilot may not be able to prevent an accident.
4. Autopilot's limitations in complex driving scenarios: Autopilot may struggle to navigate complex driving scenarios, such as construction zones, intersections, and roundabouts.
5. Autopilot's potential for false positives and false negatives: Autopilot may misidentify obstacles or fail to detect them altogether, leading to potential accidents.

Therefore, while Autopilot can enhance driving safety, it is not a foolproof system and still requires the driver's attention and intervention when necessary. It's important for Tesla to acknowledge the limitations of Autopilot and work to improve its safety features, rather than relying on the driver's responsibility to prevent accidents.: Why do a few thousand other car accidents each year not attract much attention, while Autopilot causing a few accidents receives so much news and media coverage? Because if a car accident is caused by Autopilot, the probability of the same accident happening to all Tesla owners using Autopilot significantly increases. The risk of harm to oneself from Autopilot is high.

The core issue here is, who is actually driving the car - human or Autopilot? Human and software have significant differences not only in technical abilities but also in effects for probability risk analysis. In essence, humans are "independent random variables," while Autopilot is not.

Every person is different and independent. Some people drive carefully, some drive average, and a few are reckless. These people have no inherent connection, they are "independent random variables." What does "independent" mean? It means that one person crashing their car by mistake does not mean that others will have the same accident, as each person drives differently. In probability theory, whether these people have a car accident or not is completely independent events.

However, Autopilot is a software system, and all cars with Autopilot installed have identical behavior, so the many Tesla cars using Autopilot are not independent variables but "correlated variables," as they are connected through the Autopilot system's design. If Autopilot causes an accident due to a misjudgment, all cars using Autopilot are likely to have the same accident.

The random variables being "independent" or not led to vastly different risk analyses for human drivers versus Autopilot causing an accident. if you have studied probability theory, then the posterior probability (posterior probability) of an Autopilot owner having an accident will significantly increase due to the occurrence of an Autopilot causing an accident, while the posterior probability of a non-Autopilot car owner having an accident will not significantly increase due to another car of the same model having an accident. Mathematically, it can be expressed as:

P(other Autopilot accidents | Autopilot causing an accident)

far greater than

P(other non-Autonomous car accidents | one non-Autonomous car accident)

The randomness of accident causes results in different posterior probabilities. facing "Autopilot has a certain probability of taking your life," this fact renders meaningless Autopilot's overall accident rate, no matter how low it may be, even if it is lower than the national car accident rate as Elon Musk claims. This is because of "responsibility": a car owner can allow themselves to take their own life, but they cannot allow Autopilot or anyone else to take their life, especially if it is due to Autopilot's foolishness. Secondly, this is due to "personal risk": regardless of the national accident rate, one's risk while driving is generally only related to their own level of caution, meaning that one's probability of getting into an accident is largely independent of the national accident rate. However, using Autopilot increases one's risk, which is influenced by Autopilot's capabilities, making it roughly equivalent to the average accident rate of Autopilot.

### Carefully examine the statistics

Is Autopilot's accident rate really low? You can research this yourself. For instance, some people only consider the ratio of accidents to Autopilot miles driven, but ignore the number of accidents avoided by the driver's timely intervention.

How many miles can Autopilot drive without interruption? Given the current visual technology, it is unlikely to be very far. Intelligent drivers do not let Autopilot enter even slightly complex situations, only using it for "highway lane control," so the low Autopilot accident rate may be due to the fact that most users do not use it in complex situations. Therefore, although Autopilot's statistics appear to be "several tens of billions of miles," it is likely that it has never made the correct response in complex situations.

Additionally, Tesla is a relatively expensive car, and its buyers are generally more responsible people, so the accident rate should not be compared to all cars, but rather to similar luxury cars like Mercedes-Benz and Porsche. We shall carefully examine the overall statistical figures in the automobile industry. In the US, there were 3.7 million car accident fatalities in 2017. It seems like a large number, but considering the death rate per mile, only 1.16 people die on average per billion miles. From 1975 to 2017, the number of deaths per billion miles dropped from 3.35 to 1.16. Therefore, even without Autopilot, driving is becoming increasingly safer.

Let's compare it to other causes of death. In the US in 2017, there were 281 million total deaths, of which 64.7 million were due to heart disease, 59.9 million to cancer, 16 million to respiratory diseases, 14.6 million to strokes, 16.9 million to unintentional injuries (including car accidents), 8.3 million to diabetes, 5.5 million to the flu, and 4.7 million to suicide.

Of the 16.9 million unintentional injury deaths, 3.7 million were due to car accidents. Therefore, another 13.2 million people died from other unintentional causes. Even suicide claimed 4.7 million lives. So you may have realized that the 3.7 million car accident fatalities are not such a terrifying number, but rather one of the safer domains.

Do you know who dies in car accidents and how they drive? If you drive carefully, I don't think your risk is that high, and it might even be smaller than the probability of suicide.

Elon Musk called cars a "two-ton death machine" (two-ton deadly machine) in an interview and even said "it's hard to believe we let people drive these things." Blindly emphasizing car accident fatalities and claiming to reduce accident rates through autonomous driving is a common justification in the autonomous driving industry. They are not solving a very important problem, and their solution is not even a practical distraction.: Therefore, Tesla not only cannot achieve self-driving technologically, but also has issues with character and integrity. I have never seen a car company as eager to shirk responsibility as Tesla. Generally, they actively cooperate with investigations, courageously assume responsibility, promptly make corrections, only then can they possibly gain the public's trust.