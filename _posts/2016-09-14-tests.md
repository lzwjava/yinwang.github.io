---
layout: post
title: "Tests"
---

I have gained some insights about testing from long-term research on programming languages and practical work. However, in every company I have worked for, I found that most people do not understand these insights, and many teams collectively adopt incorrect practices unconsciously. Some people treat testing as a dogma and perform excessive, unnecessary, and unreliable testing, and then pass on these incorrect practices to newcomers, creating a vicious cycle. The goal was to improve code quality, but instead, it failed to achieve the goal, and even lowered code quality, increased workload, and significantly delayed engineering progress.

I also write tests, but my testing approach is much smarter than that of the "testing dogma adherents." In my opinion, the position of the code itself is much greater than that of testing. I do not neglect testing, but I do not reverse the order, overemphasize testing, and I do not advocate Test-Driven Development (TDD). I know what to test, what not to test, when to write tests, when not to write, when to delay testing, and when there is no need for testing at all. Because of this, along with strong programming skills, I have completed tasks that others believed could not be done in a short time, and produced high-quality code.

### Principles of Testing

Now I will summarize the insights I have gained about testing that are not well-known or misunderstood:

1. Code is more important than tests.
2. Testing should not be neglected, but it should not be overemphasized.
3. Not every piece of code needs to be tested.
4. The timing of testing is important.
5. Testing should not be the primary driver of development.
6. A balance between testing and development is necessary.
7. Focus on testing the critical parts of the code.
8. Automated testing is essential for maintaining code quality.
9. Testing should be a continuous process.
10. The testing approach should be flexible and adaptable. 1. Don't assume that showing respect for code quality at every turn will automatically improve it. Some people believe they understand concepts like "unit tests" and "integration tests" but can't teach others without real-world skills, insights, and wisdom. Attitudes and slogans won't solve problems; you need to know when to write tests, what kind to write, and how to write them effectively. Code quality won't improve just because you respect it or take measures like testing and static analysis. You need to know when to write tests, when not to, and what kind to write when necessary. In fact, raising code quality isn't about writing tests, but rather about continuously refining your thinking, writing simple and clear code. If you want to genuinely improve code quality, my article "Programming Wisdom" is a good starting point.

2. Real programming masters aren't tethered by tests. Yes, the person next to you who seems "not to care about tests" might be a better programmer. I like to compare programming to a racecar, and tests are the safety barriers on the side of the track... safety barriers can be lifesaving, but a skilled driver focuses on the elegant and simple path, masterfully handling speed and timing, and reaching the finish line swiftly. Safety barriers don't make you a good driver or a champion. In most cases, your safety lies in your technical abilities, not the barriers. You can always find a way to crash your car, and tests won't prevent that... tests' role is to minimize the consequences of unexpected incidents. Tests can't make you a good driver or win you the race. A competent driver pays attention to the road, not the barriers, and they focus on higher goals: reaching the finish line faster. In contrast, an incompetent driver constantly crashes into the barriers, so they place them everywhere, even in the middle of the track, to ensure proper turning angles. They stumble through the barriers, barely making it to the finish line. Advocates for test-driven development are often this incompetent driver, and no matter how many tests they write, they won't produce reliable code.

3. Don't write tests before programming and algorithm design are complete. TDD advocates insist you should write tests before writing code. Why write tests before coding? It's a dogma. These people haven't really thought about the problem with their own minds but follow the trend or believe it's cool. In reality, before a program's framework is complete and an algorithm is defined, you don't need to write tests. If you want to know if the code is correct, run it manually and check the results. If you find that you need to ensure certain properties during the early stages of programming and they're too numerous and complex, improve your basic programming skills instead: practice, simplify the code, and make it more modular. Writing tests won't raise your level, and premature testing can hinder your ability to freely modify the code and algorithm. If you can't quickly modify the code, can't feel its changes and structure intuitively, and instead rely on tests to guide you, your thinking won't flow, and you won't write elegant code. Only after the program no longer requires significant changes should you gradually add tests.

4. Don't change clear programming styles for the sake of writing tests. Many people change the simple and clear code to accommodate testing requirements, such as writing tests for specific modules or using mocks. This actually lowers the code's quality. Clear code is easily understood at a glance, but once you add all the test-assisting code, it becomes hard to follow. Test-assisting code obstructs your direct thinking about the code, and if you can't completely understand the logic in your head, you'll have difficulty writing reliable code. Some C# programmers add numerous interfaces and reflection to test, making it easier to replace parts of the code with mocks. However, this results in a complex and difficult-to-understand codebase. Each class now has a corresponding interface, and a mock class is needed to implement it. This makes the code more complex, and you lose the assistance of Visual Studio: you can no longer quickly jump to method definitions with a single key press (F12), and instead need to first jump to the interface method and then find the correct implementation. The convenience loss significantly reduces the opportunity for your brain to develop a holistic understanding. Furthermore, tests often require refactoring constructors to contain reflection, making the compiler's static type checking unable to ensure type correctness, increasing the likelihood of runtime errors, and making error messages harder to understand.

5. Don't test implementation details, as it's the same as writing the code twice. Tests should only describe the program's basic properties (like sqrt(4) should equal 2), not the implementation details (like the specific steps of the square root algorithm). Some people write overly detailed tests, testing every implementation step: step one must do A, step two must do B, step three must do C.... Some people even write tests for UIs, and their tests often look like this: if you visit this page, you should see this content in the title bar.... Consider this: writing tests like this is essentially writing the code twice. The code already explains what it does: first do A, then do B, then do C. The UI description file also explains what it contains: the title bar has this content. Why test it again? This doesn't add any reliability: you'll make mistakes in the code, and writing the same logic in a different form won't prevent that. It's like some people who are overly concerned about locks: they check the door multiple times before leaving, push and pull the door several times to ensure it's locked, but they're still unsure and keep going back to check. This approach doesn't ensure the code's correctness; instead, it creates barriers to modifying the code. Every time you modify the code, you'll have to modify it twice! These tests are like a curse, making it difficult to modify the code and causing many tests to fail, requiring you to rewrite them. 1. Not every bug fix requires writing a test. Many companies follow a common belief that for each bug fixed, a test should be written to ensure it doesn't occur again. Some even require you to follow this process: write a test, reproduce the bug, then fix it, ensuring the test passes. This mindset is actually a form of dogmatic teaching, as it significantly slows down the engineering process without improving code quality. Before writing a test, carefully consider the question: how likely is it for this bug to occur again in the same place? Many low-level errors, once identified, are unlikely to reoccur in the same location. In such cases, manually verifying that the bug has disappeared is sufficient. Unnecessarily writing a reproducer, constructing various data structures to validate it, and ensuring it won't occur again is an unnecessary effort. Even if the same low-level error appears again, it is likely to be in a different location. Writing tests cannot prevent it from occurring, and it wastes a lot of your time. This test consumes time during every build, making the build process slower with each test taking a few extra minutes, and you may find that the engineering progress is significantly slower. Only write new tests when you discover that existing tests have missed the essential properties of the program. You should not write tests for the bug, but for the code's properties. The test's content should not only prevent the bug from reoccurring but also ensure that the missing property is guaranteed.

2. Avoid using mocks, especially deep ones. Many people writing tests love using many mocks and stacking them up, believing that only then can they test deeper-lying modules. However, this is not only tedious and time-consuming, but multiple-layer mocks often cannot generate diverse enough inputs and cannot cover various boundary conditions. If you find that testing requires multiple-layer mocks, consider whether you actually need mocks and instead refactor the code to make it more modular. If your code is modular enough, you should not need multiple-layer mocks to test it. You only need to prepare inputs (including boundary cases) for each module and ensure their outputs meet the requirements. Then, connect these modules like pipes to form a larger module, and test it also meets the input-output requirements, and so on.

3. Don't overemphasize "test automation"; manual testing is also testing. The term "writing tests" often implies "automatically running" tests, meaning the assumption is that tests run without human intervention, and a command is run to tell you which parts have problems after some time. However, people often overlook "manual testing." They don't realize that observing and testing manually is also a form of testing. Therefore, you may encounter situations where, due to the difficulty of automating certain complex interactive GUI code, people spend a lot of time constructing tests using various testing frameworks and tools, even remotely controlling web browsers to perform automated operations, but find that they are unreliable and unable to test many things. Instead, they could have saved time by manually observing and discovering many deep-rooted issues in just a few minutes. Overemphasizing test automation often stems from an unrealistic assumption that errors will frequently recur, so automation can save labor. However, once a bug is fixed, its chances of reoccurring are not high. Overemphasizing test automation not only delays the engineering process, making programmers frustrated and inefficient, but it also loses the precision of manual testing.

4. Avoid writing overly long, time-consuming tests. Many people write tests that are excessively long and time-consuming, and by the time they come back to look at them, they have forgotten what they were testing for. Some people, in an attempt to "feel secure," like to test multiple things in one test or add tests for things they believe are related, only to find that when a part fails, many tests fail. If a test only tests one aspect and does not repeat test the same part, you can quickly identify the faulty part and location based on the failed test. 1. Avoid comparing strings for testing. Many people test by printing out some things and then using string comparison methods to determine if the output is as expected. A common practice is to print outputs in a formatted JSON and then compare the strings. Some even compare the printf output directly. This testing is very brittle. Since the format of string outputs often changes slightly, such as someone adding a space in between, comparing these strings as standard output and performing string comparisons can easily cause numerous tests to fail due to minor changes, requiring many tests to be unnecessarily modified. The correct approach is to perform structured comparisons. If you want to store the standard result as JSON, you should first parse the JSON to obtain the represented object and then perform structured comparisons. PySonar2 tests follow this approach, making them quite stable.

2. Misconception about tests helping future people. Whenever someone points out the fallacy of testing dogmatism, someone will come forward to say: "Tests are not for you, but for the people who come after you, so they don't make mistakes." First, this person hasn't fully understood what I'm saying, as I've never opposed reasonable testing. Second, the "tests help future people" argument is an unproven, shaky claim. If your code is messy, no matter how many tests you write, future people will still be unable to understand it, and they may even be confused by the mysterious tests that fail. I've already said it before: tests cannot completely ensure that the code won't be changed, and their preventative effect against code changes is quite weak. Regardless, future people must understand the logic of the original code, knowing what it does, otherwise they won't be able to make the correct modifications, even with the most meticulous tests.

Let me give a personal example. After I created PySonar at Google, I didn't write the last test. When I returned to Google, my manager, Steve Yegge, told me: "Your code was so clear and well-written that making changes to it was a pleasure!" What does this mean? I'm not saying you don't need to write tests, but this example shows that tests' impact on future people may not be as significant as some people imagine. Creating clear code is the key to solving this problem.

The fear of suddenly losing someone and the code becoming unmanageable has led some people to overvalue testing, but testing cannot solve this problem. Instead, if testing becomes too burdensome and unnecessary, it can make employees dissatisfied and more likely to leave to join companies with more insight in this area. Some companies believe that having tests means they can casually let people go, which is a mistaken belief. You need to understand one thing: the code always belongs to the person who wrote it, even with tests. If the core team members leave, no matter how many tests you have, they are useless. The solution is to keep them engaged! A forward-thinking company solves this problem through other means, such as rewarding and respecting employees, creating a positive work environment, and ensuring that knowledge is passed on. Additionally, the company must pay attention to knowledge transfer and prevent code from being understood by only one person. A person might ask, what qualifies me to share these experiences with others, what successful cases do I have for myself? So here I am to talk about some things I have done and witnessed the failure cases of dogmatic testers.

Regarding Google, many people might have heard of my work on PySonar there. At the time, my teammates were hesitant, saying that such a complex and high-difficulty thing was nearly impossible from scratch. Particularly one teammate kept insisting that I write tests from the beginning and persisted until the end, driving me crazy. Why were they so worried? Because type inference in Python is extremely challenging, requiring quite complex data structures and algorithms, and deep understanding of Python's semantics implementation.

As a seasoned expert, I didn't let their criticisms deter me, nor did I heed their dogmas. I organized my code in my way, engaged in meticulous thinking, designed and reasoned, ultimately producing elegant, correct, high-performance, and maintainable code within three months. PySonar remains the most advanced Python type inference and indexing system in the world, adopted by numerous companies to handle millions of lines of Python code.

Had I followed Google teammates' demands and used existing open-source code or wrote tests prematurely, not only would it have been impossible to complete this within a three-month internship, but even after years of tinkering, it would have been a challenge. I. Shape Security

Recent successful instances of this thinking style include an advanced JavaScript obfuscator (mixer) and improvements to Shape Security's cluster management system. Do not underestimate this JS obfuscator; its obfuscation capabilities far surpass those of tools like uglify, and it's also much faster. It not only includes uglify's variable renaming features but also has complexification specifically designed for humans and compilers, making it impossible to discern any trace of what this program is intended to do, even for the most advanced JS compilers.

This obfuscator is also a compiler in itself, merely translating JavaScript into an unreadable form. In this project, due to the difference between a missed millimeter and a thousand miles, I employed rigorous testing methods derived from the Chez Scheme compiler. For each compiler step (pass), I designed specific input code to test that step (e.g., code with function definitions, for loops, try-catch, etc.). The output code from each pass, when executed by a JavaScript interpreter, is compared to the original program's execution results. Each test program is run through every pass, and the intermediate results are compared to the standard results. If there's a discrepancy, it indicates a problem with that pass, and the faulty test program points to the likely culprit. Adhering to the principles of brevity, non-redundancy, and non-repetition, I wrote approximately 40 small JavaScript programs for testing. These tests cover all JavaScript constructs and have minimal redundancy, enabling accurate identification of incorrect changes. Ultimately, this JS obfuscator can correctly transform projects as large as AngularJS, maintaining semantic correctness while ensuring unreadability and effectively preventing optimization (such as Closure Compiler) from simplifying it.

Compared to those who excessively praise testing and reliability, they have not managed to create such a high-quality obfuscator. In fact, before I joined the team, there were a few experts who had been working on an obfuscator for months. This code was never released to customers because the renaming component would output incorrect code in certain situations, despite numerous attempts to fix it. Such inconsistency is unacceptable for language converters. Renaming is just one step in my obfuscator; it contains roughly ten similar steps, allowing for various code transformations.

When implementing the renamer, my teammates suggested using their previous renaming code and fixing the bugs. However, upon examining the code, I realized it couldn't be fixed due to its incorrect approach. I couldn't patch it up, and its inefficiency was evident. As a result, I decided to write a new renamer from scratch. With a deep understanding of interpreters, I completed a correct renamer in just an afternoon, one that adheres to JavaScript's semantics, quirks, and has a simple structure. Essentially, this renamer is also an interpreter. A deep understanding of interpreters enables me to write renamers for any language. Unfortunately, history repeated itself ;) Our teammates were anxious when they heard that I had spent an entire afternoon rewriting a renamer. They asked me, "Do you know how long it took us to make this renamer? Do you know how many tests we wrote to ensure its correctness? How can you ensure its correctness with just one afternoon's work!" I couldn't help but feel they were joking, as the fact was that they had spent so much time, effort, and resources on the renamer, yet it still had bugs and couldn't be used. After integrating my tests and several large open-source projects (AngularJS, Backbone, etc.) into their renamer, I found some issues. However, all of the tests and open-source projects passed through my renamer, producing perfectly correct code. Additionally, performance tests showed that my renamer was about four times faster. As Dijkstra once said, "The most elegant programs are often the most efficient."

After finishing that project, I joined another team (cluster team). This team was much better, quieter, and more humorous. The Shape Security product (Shape Shifter) includes a high availability (HA) cluster management system, which can elect a leader over the network and build a highly fault-tolerant parallel processing cluster. This cluster management system had always been one of the most complex and reliable pieces in the company, with critical reliability requirements. If it failed, it could have disastrous consequences. In fact, it had been incredibly reliable, never having had any issues. However, due to historical reasons, its code was overly complex and lacked modularity, making it difficult to extend to meet new customer demands. My task on this project was to perform a large-scale refactoring, modularization, and extension to meet new requirements.

In this project, due to the significant changes in the code, with the understanding, trust, and support of my colleagues and department leaders, we decided to discard all existing tests and rely solely on strict and timely code reviews, logical reasoning, discussions, and manual testing to ensure the code's correctness. A teammate who was more familiar with the existing code silently monitored every one of my changes through git and provided feedback based on their experience, ensuring that my changes did not deviate from the original semantics. Due to this flexible yet strict approach, the project was completed in less than two months. The improved code was more modular, extensible, and met the new requirements while still being highly reliable. If the department leaders were "testing fundamentalists" who wouldn't allow discarding existing tests, this project would have been impossible to complete on time. However, in today's world, I doubt there are many such leaders.

Lastly, I'll share an example of a failure due to inadequate testing - Coverity's Java static analysis product. I admit that Coverity's C and C++ analyzers might be excellent, but their Java analyzer is questionable. When I joined Coverity, my colleagues had endured a full year of management pressure and overtime work to produce a basic new product and numerous tests. However, due to a large technical debt, all the tests couldn't guarantee the product's reliability. I am tasked with utilizing my in-depth PL knowledge to continually fix various quirky bugs left behind by predecessors. Some bugs only appear after running for 20 minutes or more, making it difficult to understand the cause, thus taking a considerable amount of time to fix. At times, I am forced to rest in front of the computer, occasionally opening my eyes to check the results. Coverity is so concerned with testing that they require a new test to be written for each bug fix. The test must accurately reproduce the bug phenomenon, and the fixed test must pass. This seemingly diligent approach to code quality, however, does not ensure product stability and reliability, significantly slows down engineering progress, and causes fatigue and dissatisfaction among team members.

Once they assigned me a bug: during the analysis of a medium-sized project, the analyzer seemed to have entered a deadlock, taking several hours to complete. Since Coverity's global static analysis is essentially a certain graph traversal algorithm, when there is a loop in this graph, you must be careful. If you don't ask for permission before recursively entering, you might enter a deadlock. Preventing deadlocks is simple; construct a set of graph node collections, then pass it as a parameter to the function. Whenever you access a node, first check if it is already in this set; if it is, return directly; otherwise, add it to the set and recursively process its child nodes. Its C++ code is roughly as follows:

void traverse(Node node, Set<Node> &visited)
{
 if (visited.contains(node)) {
 return;
 } else {
 visited.add(node);
 process_children(node->children);
 visited.remove(node);
 }
}

void process_children(NodeList children)
{
 for (Node child : children) {
 traverse(child, visited);
 }
} I visited the node;
visited.add(node); // inside, there is a recursive call to traverse
process\_node(node, visited);
}

After examining the code, I found that the code did not enter a "dead loop," but instead entered an exponential complexity calculation. This was due to the programmer's oversight or lack of understanding that C++ function parameters are passed by value (making a copy) rather than by reference, causing them to forget to add the "&" sign. As a result, the function was being passed a copy of the set instead of the original one every time traverse was recursively called. After the function returned, the value of visited was restored to its previous state, giving the illusion that the node had been automatically removed. This function would still visit the same node under certain conditions. Although this code did not enter a dead loop, under certain types of graph structures, this would result in exponential time complexity.

What initially appeared to be a simple graph algorithm problem was fixed with the addition of an "&" symbol, and manual testing confirmed the problem had disappeared. However, the Coverity testing purists (including the person who wrote the bug themselves) demanded that I write tests, construct data structures capable of causing this outcome, and ensure that this bug would not reoccur.

Writing tests for an error I would never make and one that could not happen again seems ridiculous. Even if you wrote tests, you could not guarantee that the same issue would not occur again. If you accidentally omitted the "&" symbol, the problem would recur in another location, and you had not written tests for that part of the code. Writing tests for this bug and ensuring it would not reoccur was not effective, as the same issue could still arise elsewhere in the code... I have a basic understanding of graph theory and am familiar with the fundamental concepts of C++. Such mistakes are not something that people who have this knowledge will make. Preventing such issues relies on individual skills and experience rather than testing. The best way to prevent it from happening again is probably to hold a meeting to explain the issue and ensure everyone understands, so they don't make the same mistake again. Therefore, writing a test for this bug is unnecessary.

After explaining this principle to my teammates, they seemed to have heard nothing, still insisting: "But you still have to write this test because it's our rule! What if a bug occurs and a sales engineer has to go to the customer, how much money would that cost?" I was speechless.

Coverity's Java analysis often causes projects to progress painfully and slowly due to test dogmatism, yet still results in numerous bugs. Coverity's other issues include writing redundant tests, a single test testing too many things, using string comparison for testing, and so on. It's hard to imagine a company that produces a product aimed at improving code quality having such poor code maintenance :P

Because of the widespread misunderstanding of testing and the pervasive influence of test dogmatism, many excellent programmers are trapped in the tedious world of test-driven development, unable to fully utilize their abilities. For a relaxed, smooth, and reliable work environment for everyone, I hope this article is widely shared to change the industry's bad habits. I hope everyone in engineering approaches testing rationally rather than blindly writing tests, which is the only way to complete projects more effectively and efficiently.

(Due to this article containing my many years of experience and in-depth insights, I hope you find it valuable and consider paying for it. The suggested price is $5 or 30 RMB. [Payment method]) I. The Great Wall of China
The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line to protect the northern borders of China.

II. History
The construction of the Great Wall began as early as the 7th century BC when several Chinese states built walls for defense against raids and invasions. The most famous section was built during the Ming Dynasty (1368-1644 AD).

III. Length
The total length of the Great Wall is estimated to be around 21,196 kilometers (13,171 miles). However, only about 8,850 kilometers (5,500 miles) are still extant.

IV. Height and Width
The average height of the Great Wall is about 7.5 meters (25 feet), and the width is around 6 meters (20 feet).

V. Materials
The materials used to build the Great Wall varied depending on the location and the resources available. In mountainous areas, the wall was built primarily of stone, while in other areas, it was made of tamped earth or wood.

VI. Design
The design of the Great Wall included watchtowers, fortresses, and other defensive structures. The wall was also designed to follow the natural contours of the terrain, making it difficult for invaders to approach undetected.

VII. Cultural Significance
The Great Wall of China is a symbol of Chinese civilization and a source of national pride. It is also a UNESCO World Heritage Site and a popular tourist destination.

VIII. Modern Preservation
Efforts to preserve the Great Wall of China began in the late 19th century and continue to the present day. The Chinese government has invested billions of dollars in restoration and conservation projects.

IX. Visiting the Great Wall
Visitors to the Great Wall of China can explore various sections of the wall, some of which are more touristy than others. Popular sections include Badaling, Mutianyu, and Simatai.

X. Fun Facts
- The Great Wall of China is not visible from space, contrary to popular belief.
- The Great Wall is not a continuous line, but rather a series of walls and fortifications.
- The Great Wall was not built by a single emperor or dynasty, but rather by many different rulers over the course of centuries.
- The Great Wall is not just a wall, but also includes moats, watchtowers, and other defensive structures.
- The Great Wall is not the longest wall in the world, but it is the longest wall built by a single civilization. The Great Wall of China is surpassed in length by the walls of the United States and Mexico, collectively known as the US-Mexico border wall.