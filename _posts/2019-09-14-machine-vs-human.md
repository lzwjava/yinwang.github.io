---
layout: post
title: "I. Machine and Human Visual Ability Gap (1)"
---


This article represents my personal opinion and is unrelated to my current company's stance. Due to the recent slowing down of GitHub server access speed in the mainland, the images in this article may still take a considerable amount of time to load, despite being significantly compressed. This article exposes misconceptions and false advertising in the AI field, in order to prevent the spread of ignorance, I encourage everyone to share this article and its subsequent parts, just remember to mention the author and source.

Many people believe that artificial intelligence is almost here, often because they confuse "recognition" and "understanding." Currently, so-called "artificial intelligence" is only doing recognition: speech recognition, image recognition, but true intelligence requires understanding ability. How far are we from understanding? It's likely that the real work hasn't even started yet.

For a long time, I have been pondering the difference between understanding and recognition. Understanding and recognition are very different, yet they are often confused. I deeply understand the importance of understanding, but I find that few people know what "understanding" means. The AI field has been in chaos due to the confusion of recognition and understanding.

Recently, due to advances in fields such as image recognition, people have developed a lot of science fiction-like, blind faith in AI, marking the largest "AI boom" since the 1980s. Many people believe that AI has finally achieved it, their minds have been clouded by the "black technology" promoted by various companies, yet they cannot see the huge chasm between existing AI methods and human intelligence. So, I want to introduce the differences I have learned about the machine and human visual ability gap, hoping that some people will regain a clear mind after reading this. I have expressed my views on misconceptions in the field of natural language processing in a previous article titled "The Limitations of Artificial Intelligence." At that time, due to my limited understanding of computer vision, I did not include any content related to the visual aspect. After familiarizing myself with various approaches in machine vision, I intend to elaborate on the visual aspect in this article. These two articles together can be said to encapsulate my understanding of AI in language and vision.

The distinction between "image recognition" and "visual understanding" is often blurred in the AI field. The popular so-called "AI" today is primarily "image recognition," while an animal's visual system possesses powerful "visual understanding." Visual understanding and image recognition have fundamental differences.

Deep learning vision models (CNN type) only fit a function from "pixels=>labels" from a large amount of data. It might be able to guess the "name" of an object from a pile of pixels in an image, but it doesn't know what that object "is," and cannot perform actions on the object. I deliberately used the word "guess" because it truly is guessing, unlike humans who precisely know.

Image recognition and speech recognition are at the same level, remaining at the syntactic (literal) level without touching the semantic. Speech recognition is the conversion from "speech=>text," while image recognition is the conversion from "image=>text." Both output text, but "text" and "understanding" are on different levels. Text is a surface symbol, and it only becomes meaningful once it is understood. How can one be said to "understand an object" then? At least, you should know what shape it is, what components it has, where each part is located and where the boundaries are, roughly what material it is made of, and what properties it has. Only then can you effectively take action towards it and achieve the desired effect. Otherwise, this object is just a box with a label on it, unable to make precise judgments or operations.

When facing various daily objects, is it the name of the object that appears in your mind? For instance, when you pick up a knife to cut fruit, with no one around to talk to, does the word "knife" appear in your mind? Generally, it doesn't. What appears in your mind instead is "common sense." Common sense is not text, but a kind of abstract yet concrete data.

You know this is a knife, but your mind does not extract "knife" as a character, but rather "what is a knife." Your visual system tells you what its structure is like. You know it is made of metal, you see the blade, the edge, the handle, it might be foldable. Experience tells you that the edge is sharp and can cut things, touching it may cause injury, the handle is the place to hold. If the knife is foldable, you need to figure out which end to act on to open it, where is its axis?

You successfully pick up the knife and start cutting fruit. However, your mind still hasn't produced the words "knife," "blade," or "handle." While cutting fruit, your "language center" in your brain might be humming a recent favorite song lyric, which has nothing to do with the knife. Language is just a tool for communication with others, but we don't need language when we do things ourselves. Completing the action of cutting fruit requires the understanding of the object's structure derived from vision, not language.

You don't need to know the name of an object to use it correctly. Similarly, just knowing the name of an object doesn't help you use it. If the first thing that comes to mind when you see an object is its name, then you are definitely a simple-minded person, unable to manage your own life. Modern "machine vision" is essentially the same. The machine may be able to determine the name of the object in the image, but it doesn't know what it is and cannot operate it. Imagine a robot that cannot understand the structure of objects, it can only use image recognition technology to identify regions on your head, labeling them as "forehead," "hair," "ears".... Would you let it give you a haircut?

This is what I mean by the difference between "visual understanding" and "image recognition." You will realize that this difference is enormous.

Visual recognition cannot be lacking in understanding.

If we lower the standards and only require recognizing the name of an object, then pixel-based image recognition, such as convolutional neural networks (CNN), cannot recognize objects like humans do. Human recognition of objects is not like a neural network's "take a picture, recognize" two-step action, but a dynamic, continuous process: observe, understand, observe, understand, observe, understand....

Perception receives information, with understanding interspersed in between, and understanding in turn guides the direction and sequence of observation. Understanding is interwoven in the process of recognizing objects, "observing/understanding" becoming an indivisible whole. Humans see part of an object, understand what it is, then continue observing its surroundings, repeating this process to finally determine what the object is. A machine lacks a comprehending component in its recognition process, which is why it cannot match human abilities in image recognition.1. The process of observing and understanding this happens so quickly, that it's completed in the blink of an eye, making many people unaware of the existence of the "understanding component". To slow things down, let's use a slow-motion replay and see what really happened. If you have never seen this before, do you know what it is?

2. A person who has never seen this before would still know that it is a "vehicle". Why? Because it has wheels. Why do you know that's a wheel? Think carefully about it. All these analyses are the result of "visual understanding", and these understandings depend on the experiences accumulated throughout your life, which I call "common sense".

3. In fact, you don't need to analyze so much to recognize this thing. You do these analyses because someone asked you "how do you know that?" People recognize objects based on what is called "intuition". When you see this picture, your brain naturally generates a 3D model. A moment later, you realize that this model fits the mechanical principles of a vehicle because you have seen cars, trains, and tractors before. Your brain immediately conjures up the possible motion sequences of this object, making you feel as if you are watching it move with its wheels. You even imagine one wheel pressing against a rock, lifting up with the axle, while the entire vehicle remains balanced, suggesting it can handle rough outdoor environments.

4. There's an often overlooked detail: the axle of the wheel must be connected to the body of the vehicle. If the wheel is not connected to the vehicle or if its position is incorrect, it appears impossible for the wheel to move the vehicle as a whole. This relationship between the wheel axle and the vehicle body is a concept called "topology" (topology).

5. Topology is a challenging branch of mathematics, but it seems that humans naturally understand some simple topological concepts. In reality, higher animals also intuitively grasp some topological concepts, recognizing which things are connected and which are separate. Hunting animals instinctively know that a prey's tail is connected to its body, so biting its tail can capture it. Topology has another important concept in it, which is "hole." Intuitively, most animals, including mice and rabbits, understand the concept of a hole. Their predators, such as cats, also understand the concept of a hole. If I give my cat a cardboard box to play with and dig a hole on its surface, the cat won't go in. I have to dig two holes for it to enter. Why is that? Because the cat knows that if there is only one hole on the box and it goes in and the hole is blocked, it won't be able to get out!

How can a machine understand the concept of a hole? How does it understand "continuity"?

In short, when a person sees an object, they see a 3D model in their mind, understanding its topological relationships and geometric properties. When a person encounters a new object, they can infer what it is and how to use it based on this understanding. This ability allows people to accurately identify objects. A machine without understanding abilities cannot do this.

### Differences between Human Vision and Machine Vision

There are fundamental differences between the human eye and a camera. The central part of the human retina, called the "fovea," has a high density of photoreceptors, while other parts have fewer photoreceptors and are blurry. However, the human eye can rotate, controlled by brain nerves, and agilely tracks interesting parts: lines, planes, three-dimensional structures.... Human visual system can precisely understand the shape of objects, topology, all in 3D. The brain sees not pixels, but a 3D topological model.1. The order of observation for the eyes is not to record each "pixel" in a line from top to bottom to make a 6000x4000 pixel image, but to focus on key points. The eyes can observe along straight lines or curves, rotate, or jump around. The brain controls the eyes' movements based on its understanding ability to observe the required key points. Due to the high resolution in the central part of the retina, the brain can obtain very high precision information. However, not every place is looked at with the same level of detail, so the amount of information collected by the eyes may not be large, and the amount of information the brain needs to process will not be much.

2. The human visual system can understand concepts of points, lines, and surfaces. It can tell if a surface is continuous or has holes, is concave or convex, and can distinguish inside from outside, far from near, up from down, and left from right. It can understand the texture of an object's surface and imagine what it would feel like if touched. It can imagine the rough shape of the object's backside and rotate or distort its mental model. If there is a defect in the object, it can even guess what it looked like before.

3. The human visual system is more interesting than a camera. Many people have seen optical illusion images, which reveal what the human visual system is doing behind the scenes. For example, the following image is static, but you feel as if there are many dark dots at the intersections of the white lines. However, if you focus on a specific intersection, the dark dots disappear. This illusion is very classic and is called the Herman grid, and it is widely studied in neuroscience. Later, I will mention this again.

Originally a static image, you feel it is rotating.

Originally, the upper and lower parts were the same color, but the lower part appears lighter. If you cover the highlighted part in the middle with your finger, you will find that the colors of the upper and lower parts are actually the same. Another illusion similar to this, is the famous "Abelson checkerboard illusion." In the given figure, A and B squares have the same color, yet you feel that A is black and B is white. You can try cutting these two blocks out from the image and comparing them closely if you don't believe it. If you're curious about why this is, you can refer to this article.

In the following picture, you will perceive a black inverted triangle, but it doesn't actually exist.

Many optical illusions demonstrate that the human visual system is not as simple as a camera. It possesses certain special functions and mechanisms that give rise to these illusions. This makes human vision different from that of machines, enabling us to extract structural information about objects rather than just seeing pixels.

Extracting topological structure features of objects is why humans can understand abstract paintings, comics, and toys. Although there are no cats and mice in the world that look like this, a child who has never seen the "Tom and Jerry" cartoon can still recognize a cat and a mouse, and even a house in the background. Try letting an untrained deep learning model identify this image instead.

Even more abstract toys, humans can still recognize them as certain characters. With heads and limbs turned into squares, they still seem quite "familiar." Don't you find this amazing?1. The brain interprets the concept of "topology" to enable humans to correctly process various objects without being affected by specific pixel interference. Understanding topology structures helps humans recognize objects very accurately, even in situations with incomplete, blurry, or distorted information, and even in poor weather conditions with reflections and shadows.

2. Regarding reflections, have you ever thought about how a machine could identify a mirror or glass in a scene? If there are reflective objects in the scene, such as mirrors, calm water surfaces, or polished items, will CNN-type functions that rely on pixel filtering training be useful? Keep in mind that the pixels they see may be a large area formed by mirror reflection, making it impossible to recognize this situation based on local texture.

3. How can autonomous vehicles or robots determine if the road ahead has puddles or is iced over? How can they distinguish the reflected images from real objects? For instance, how can they tell that the reflection in the following image is not a real tree on the road? Keep in mind, the pixel texture of the reflection may be very similar to the real scene.

4. Humans perceive the existence of mirrors, glass, water, and ice on the ground through their understanding of light and common sense. Can a machine that doesn't understand the properties of light and water detect their presence? Can it identify them based on pixel analysis? Keep in mind, their presence in certain places can be life-threatening.

5. It's fascinating that understanding the reflection and refraction of light seems to have become hardwired into every animal's visual system. I noticed this point because there are two large mirrors on the wardrobe door between my bedroom and living room. My cat in the bedroom can see me in the living room holding a toy leash. When he comes running, he doesn't collide with the mirror but instead immediately exits the bedroom door and turns towards me. Every time I see his agile movements, I wonder, how does he know the mirror exists? How does he know the cat in the mirror is himself and not another cat?: The human brain constructs 3D models of objects

Light and shadow, let's talk about shadow now. Anyone who has drawn before knows that the outline drawn at the beginning has no depth sensation. Then you add shadows in the right places, and depth sensation appears. Animals' visual systems have a processing center for shadow analysis, and it seems we are born with this ability. "Stereoscopic vision" is so deeply ingrained in our brains that once depth perception is established, it is difficult to see pixels as flat again.

With the combination of light and shadow, humans and animals can obtain a lot of information. For example, in the picture above, we not only perceive that this is a three-dimensional egg, but we can also infer that the surface below the egg is a plane, possibly a table, because a shadow has been cast on it.

Does a neural network know what a shadow is? How does it know that the shadow is not a real object? Can it extract useful information from the shadow?

A neural network doesn't know what a shadow is. It has been discovered that Tesla's autonomous driving system based on image recognition, Autopilot, is confused by shadows. It takes a tree shadow on the road for an obstacle and tries to avoid it, almost colliding with an oncoming car. I discussed this issue in an early article.: Another topic about painting. In the beginning of learning to paint, many people find perspective particularly difficult. Perspective refers to "near objects are larger, far objects are smaller." Initially, the walls of a house are all rectangular and the same height. However, you need to make the far edge shorter, and the proportions related to it need to be drawn correctly, just like in a photograph, so the walls become a triangle shape. The roof, windows, etc., also need to be adjusted accordingly. You need to paint it this way for the viewer to feel it's correct, otherwise they will feel something is off and unreal.

This is indeed a difficult thing, most people (including me) never learn to paint perspective in their lifetime. Although I can see with my eyes that the far edge is shorter, my brain seems to "automatically correct" me, making me believe they are all the same length. So if I paint just relying on my eyes, I will paint all the edges the same length. I seem to be unable to learn to paint!

Painting perspective is such a difficult thing that in the 16th century, German painter Albrecht DÃ¼rer designed a special device for it.

You might not have thought about it, but the reason we find it difficult to paint perspective is because it's a crucial function of the human visual system. Although our eyes see objects as nearer objects are larger and farther objects are smaller, our brain automatically adjusts their lengths in our "mental representation," so we know they are the same length.

This might be why humans can reconstruct correct 3D models from the perspective cues in visual images. In your mental model, the walls of a house are the same height, just like in reality. With an accurate 3D model, we can correctly control our movements in the space around the house.1. This "3D automatic correction" feature that makes it difficult for us to learn painting, seems to have been embedded in every human and higher animal's visual system. We don't need to learn to have this ability, it has always been functioning. On the contrary, we need to put in a lot of effort to turn off this function.

2. Why is it so difficult for people to paint perspective effects? Because when people paint, they are not painting what they see with their two eyes on their heads, but rather what they see with their "mind's eye" - the 3D model in their mind. This 3D model is consistent with the real one, with all the walls of the house in the model being the same height, resulting in incorrect painting. Only professional painters have the ability to close their "mind's eye" and paint directly what their eyes see.

3. I speculate that every advanced animal's visual system has similar mechanisms to construct a 3D model from optical reconstruction similar to reality. Lack of 3D modeling ability in machines is unable to understand the objects they see accurately.

4. Many self-driving cars use laser radar to construct 3D models, but compared to the 3D models formed by human vision, they are too rough. Laser radar relies on active emission of laser and generates a "point cloud" with low resolution, forming only a rough 3D outline, unable to recognize objects and unable to understand their structure. We should carefully consider why humans can construct such precise 3D models with just passive reception of light, understand the structure of objects, and precisely control their actions to operate these objects.

5. Current deep learning models are all based on pixels and lack abstract ability, unable to construct 3D topological models, and even the position relationship is unclear. Lack of the "structural understanding" ability of the human visual system may be the reason why deep learning models need so much data and computation to identify objects, while children can recognize objects with just a few views without needing much data and computation.: The human brain extracts features of objects, thus a vast amount of information can be ignored, making the data humans need to process possibly much smaller than that required by deep learning models. In the field of deep learning, there has been a blind focus on increasing computational power, manufacturing larger scale computation chips such as GPUs and TPUs.... Have we ever considered how much computational power the human brain actually has? It might not require that much.

From the various phenomena we have seen, we may have understood that the human visual system is quite amazing. Current machine vision research has not yet figured out how these abilities of the human visual system are achieved. In the next installment, we will clarify exactly how much the AI field understands about the construction of the human neural system.

Please see the next article: The Gap between Machine and Human Visual Abilities (2)

(This series of articles contains many unique insights. If you find it helpful, please consider supporting it financially.)