---
layout: post
title: "Self-Driving Car Liability"
---

: Why can't autonomous vehicles make mistakes at all?

Someone told me that I have unreasonably high expectations for Google's autonomous vehicles. People are not perfect, so Google's products don't need to be perfect, they just need to be useful. There are so many terrible drivers - drunk drivers, texting while driving, falling asleep at the wheel, making judgement errors... which have caused countless accidents. Autonomous vehicles are far from perfect, but if they can work correctly in 99.9% of situations and significantly reduce accident rates, that's a great benefit for humanity.

Firstly, the current situation is that Google's autonomous vehicles can only come out in very limited conditions: daytime, good weather, and simple traffic. Even under these ideal conditions, there are still over 270 incidents per year that require human intervention. So, it's worth questioning whether Google's autonomous driving technology can surpass even the most basic human driving skills. Putting that aside, let's assume that autonomous vehicles can surpass most human drivers and can judge correctly in 99.9% of situations. However, it's still not feasible. Autonomous vehicles must be able to make correct judgments in 100% of situations and not make any mistakes to be accepted by people. Why is that?

The reason is ethical and legal principles. Legally, responsibility is not determined from a macro perspective. In other words, the law does not excuse Google for accidents caused by their autonomous vehicle in the 0.1% of situations where it makes incorrect judgments, just because it makes correct judgments in 99.9% of situations. The legal principle is simple: whoever causes an accident, bears the responsibility, regardless of whether they are human or machine. Yes, autonomous vehicles don't need to be perfect to be usable, but if they cause an accident due to a judgment error, responsibility falls completely on Google, not the vehicle owner. Since the vehicle owner is not the driver, Google's software is the driver, so if the autonomous vehicle causes an accident, Google bears the full responsibility.

Let's imagine a scenario. Suppose Google's autonomous vehicle makes correct judgments in 99.9% of situations, but in that 0.1%, it makes a judgment error causing an accident. Now you are one of the unlucky victims, riding in the Google autonomous vehicle when it crashes due to a software judgment error, resulting in both your legs being amputated, leaving you disabled for life. You sue Google. Google tells the judge that our autonomous vehicle is reliable in 99.9% of situations, significantly reducing overall accident rates, and making a huge contribution to humanity. This unfortunate person encountered the 0.1% judgment error situation, so Google is not responsible. Do you find this acceptable? ;): 0.1% error probability falling on an individual means 100% misfortune. If you were originally a safe driver, it's even more unfortunate because you wouldn't have made such errors yourself. Even if automatic cars reduce societal accident rates drastically, it's meaningless to you since you're the damaged goods. Morally, we must have different standards for machines and people. Automatic cars' judgment ability isn't sufficient to surpass most drivers; it must surpass all. Errors some people make while driving, an automatic car cannot. Because the person maimed by a faulty automatic car can say, "I wouldn't have made such a mistake if I were driving myself. Others might, but I wouldn't! Google's self-driving car is seriously responsible for this."

Understood? Reducing accidents on a macro level isn't enough. Automatic driving technology must surpass the world's safest drivers, making no errors whatsoever. Currently, although there are many accidents in the world, responsibility is apportioned among various parties. But if Google's self-driving car enters the market and replaces most drivers, all responsibility for accidents caused by the self-driving cars will fall on Google. Therefore, this business model is extremely difficult and impractical.