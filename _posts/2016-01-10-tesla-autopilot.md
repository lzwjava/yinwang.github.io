---
layout: post
title: "Tesla Autopilot"
---


The following content is an additional section in the article "Design Flaws of Tesla Model S." Due to the lapse of time since writing and its timeliness, I am now releasing it separately as a standalone article.

Two months ago, Tesla introduced "autopilot" (self-driving) functionality to Model S through a "software update." This feature allows Model S to automatically drive along clearly marked lanes, adjusting speed based on the vehicles ahead.

This seems like a new and cool feature, almost on par with Google's self-driving cars (in reality, they're quite different). However, shortly after its release, several videos appeared on YouTube (video1, video2, video3, video4, video5). They showed that autopilot might make incorrect judgments and perform risky maneuvers, nearly causing serious head-on collisions.

Notably, video1 demonstrated that on a road with clear lane markings and good weather, autopilot suddenly veered to the left, attempting to switch to the opposite lane, narrowly avoiding a severe head-on collision. Upon closer examination of the autopilot's turning moment, it was due to a tree shadow on the road surface that the autopilot mistook for an obstacle and tried to steer the car onto the wrong lane! 1. From this simple video, we can observe:
Autopilot does not perform basic "shadow removal" and cannot distinguish shadows from obstacles. When the sunlight is strong and shadows are prominent, autopilot may consider shadows as obstacles. Shadow removal has been extensively researched in computer vision, suggesting that Tesla may not have conducted fundamental computer vision research. Lack of ability to distinguish shadows from obstacles, such self-driving systems are completely unacceptable.

2. There is a clear "no passing" double yellow line in the middle of the road, and there is a car coming from the opposite direction. Autopilot, to avoid "obstacles," turned left across the double yellow line, risking a collision. This indicates that autopilot cannot understand even basic traffic rules and correct emergency operation methods. Alternatively, this software may not have recognized the double yellow line, or even the concept itself. For an experienced driver, if there is an obstacle ahead, the correct action should not be to sharply turn away, but to emergency brake. From the video, we can see that the car did not brake or slow down (maintaining 37-38 mph), but instead sharply turned. Moreover, the operation was not performed in advance, but only when the tree shadow appeared in front, and there was no consideration of anticipatory distance. This shows that the designers of autopilot do not understand even basic driving common sense.

It is sad to see that many comments on these videos mainly blame the car owner as an "idiot": "This is the car owner's fault!" "Autopilot can only be used on highways" "It can only be used when there is a clear boundary line on the road" "It cannot be used in areas with many curves" "It can only be used when you can see 300 meters of road ahead" "Why didn't you read the instructions?" ... Elon Musk also clearly told reporters in an interview: "If the user causes an accident while using autopilot, it is the user's responsibility!" He repeatedly stated: "autopilot is still in beta version..." which means, be careful!

I hold a different view on these statements. First, Tesla should not have pushed a "beta version" or "test version" feature like autopilot to all Model S systems. In fact, a function related to human life safety should not have a "beta version" or "test version" label. Tesla forced this immature system onto users and then shifted responsibility to users if an accident occurred, which is an irresponsible approach. No one is willing to use themselves as a "beta tester" for Tesla's life. other than that, even if the user did not carefully read the autopilot's usage instructions, if they used autopilot in places where they should not (such as areas with unclear road markings), and if there was an accident, Tesla should bear full responsibility. Reasons are as follows:

1. As users, they have no duty to read and fully understand autopilot's limitations. In the software industry, there is a harmful habit of blaming users. If the software design has issues and the user fails to remember its flaws or effectively bypass them, the blame falls on the user when problems arise. Tesla wants to bring this unethical culture from the software industry into the life-critical automobile industry, which is unacceptable.

2. Tesla's autopilot implementation is immature and has numerous limitations. It does not work in bad weather, unclear road edges, poor lighting or shadows, construction cones, high-speed exits, etc. In such stringent conditions, any car manufacturer can create a similar autopilot. My cheap Honda car, for instance, has a lane departure warning (LDW) function that alerts when the vehicle drifts from the lane. With a simple image processing camera, it can be done. In college at Indiana University, we had a course where we wrote code to control a golf cart (an electric vehicle) to automatically drive along the road markings. This was not difficult because the conditions for correct operation were excessively stringent. Other car manufacturers are well aware of this function's limitations and do not exaggerate this "lane detection" technology or turn it into autopilot. They only use it as an auxiliary, advisory feature. These car manufacturers understand that, as a user, they cannot and should not remember all the conditions under which autopilot can correctly operate.

3. Users may not have sufficient ability to judge whether autopilot conditions are met. For example, the road markings are still there, but they are worn and the color is light. Can autopilot work or not? Nobody knows. Assigning the task of judging whether these conditions are met to the user is like asking the user to help Tesla's engineers debug code. This is clearly unreasonable. If autopilot can automatically alert users and exit autonomous driving mode when it detects road conditions are not met, that would be more reasonable.

4. Users may not have enough time to respond to changing conditions. Autopilot may initially drive under good conditions (good weather, clear road markings), but the road conditions may change rapidly during high-speed driving. For instance, the video 5 seems to depict such a situation. The change in road conditions comes suddenly, and the driver is not prepared. By the time they react and try to disable autopilot, the accident has already occurred. In such cases, a fair-minded judge should rule against Tesla.: 1. The sleek "high tech" image of autopilot can easily lead to blind trust from people, resulting in negligence and accidents. Since it's called "autopilot," it means it can automatically drive for a certain period without human intervention. If users believe it can drive autonomously, they might do other things before reaching a highway exit (for example, GPS shows an hour left before reaching the exit). Who gave you the right to call it "autopilot" then? I've seen pilots turn on autopilot and go to the bathroom on a plane. If you have to constantly focus on autopilot to prevent errors, it might even be more tiring than driving yourself. Driving only requires focusing on the road, but now with autopilot, you have to watch the steering wheel as well...

2. Tesla releasing "beta version" autopilot to all Model S owners is irresponsible towards social safety. You should understand Murphy's Law: if something can go wrong, it will. Autopilot's functionality is not mature, has many limitations, and is not easy to use correctly. This not only threatens Model S owners but also others. A car is not a toy; releasing a new feature, beta version, and letting people test it can cost lives. I think Tesla's autopilot should be treated like an unlicensed driver and should be legally banned. Due to the complexity and potential danger of autopilot, users should undergo DMV testing and have a "can correctly use Tesla autopilot" notation on their driver's license to use it.

3. Legal disclaimers and user agreements regarding human life safety are invalid in law. You'll find "disclaimers" everywhere in the US. For instance, when you attend a school-organized recreational activity, you're asked to sign a "waiver" stating that if there's an accident or mishap, you can't sue the school. Such waivers are generally ineffective in law. If the school is at fault and causes you injury, you can still sue them despite signing the waiver. I bet Tesla's autopilot has a similar disclaimer, stating that if an accident occurs while using autopilot, Tesla is not responsible. Since autopilot directly controls your car, if an accident does occur, this waiver is as ineffective as others. You can still sue them.

Since they realized the issue and knew they couldn't escape responsibility, Tesla recently updated the software forcefully, limiting autopilot's functionality, claiming it's to prevent users from "abusing" and "going crazy" with autopilot. Tesla is the crazy one, accusing users of "abusing" and "going crazy." This is infuriating.

While limiting autopilot's functionality, Tesla also introduced beta versions of the "autosteer for trailers" and "summon" (call) features. These features seem cool, but they come with numerous conditions. You can only use them in specific places and under certain conditions. If you violate these conditions and cause an accident, Tesla claims no responsibility. These functions that enable a car to move on its own, similar to autopilot, also pose safety risks to society. For instance, if someone uses the self-driving and summon features in inappropriate places, accidents may occur. This is not a user issue, but rather Tesla should not have released these immature technologies to gain popularity.