---
layout: post
title: "Test-driven development (TDD)"
---


Many companies, including Google and my current company Coverity, prefer "test-driven development." The principle is to write automated "unit tests" (unit test) while writing code. After code modification, these tests can be run in batches, preventing unexpected errors.

This is not a bad idea. I used many tests in Kent's compiler course. They are indispensable in compiler development. Compilers are extremely precise programs, even minor changes can cause significant errors. Compiler projects usually contain a large number of tests.

However, test building should be done after the program's main body has taken shape. If the program is of a creative design and the main body has not yet formed, premature testing can significantly reduce development efficiency. When I developed Python static analysis at Google, I hardly used any tests. Although my team members urged me to write tests, I knew it would lower my development efficiency because I had to scrap and rewrite the code several times in the few weeks it took. If I had written tests at the beginning, they would have been an obstacle to my extensive code modifications.

Another side effect of testing is that it instills a blind dependence on testing in many people. After changing the program, running the tests without errors gives them the false confidence that their code is correct. However, tests cannot ensure the correctness of the code, even if they have completely "covered" it. Coverage only means that the code has been touched by the tests, but it cannot determine under what conditions it was touched. If the actual conditions differ from the test conditions, problems will still occur in actual operation. Test conditions are usually at the level of "combination explosion," so it is impossible to test all cases. The only reliable method is to use strict "logical reasoning" to prove correctness. I am not suggesting you use ACL2 or Coq, or any such theorem proving software. While their logic is very rigorous, using them to prove complex software systems requires top-notch programmers and a great deal of time. Due to theoretical limitations, the program's correctness may not be provable at all. So the "logical reasoning" I'm talking about is local, human, basic reasoning.

Many programmers write code based on appearances rather than precisely analyzing the logic of their programs. They modify the program frequently with a "band-aid" approach. When a problem arises, they look for where it went wrong without fully understanding, make a change to prevent it from occurring again, and run all tests again. They may even add new tests to ensure that this area does not cause problems in the future.

The result is that the program contains numerous "special cases" and "band-aid solutions." One bug is squashed, but another emerges. Busy as a bee, but still unable to satisfy "all cases." Programs that can "satisfy all cases" are often much simpler than those that can only "satisfy special cases." This is a strange phenomenon: the more things a program can do, the less code it requires. Perhaps this is the "beauty" of a program, which is similar to the "beauty" of mathematics.

A beautiful program cannot come from constant patching. It must perfectly grasp the essence of things, or it will have countless unfixable edge cases. Programmers are like painters in this regard. A painter who spends all day at home will not create anything good. Programmers are the same; they must go out and observe things, seek "inspiration," and not just write code. When modifying code, they must use their "spiritual eye" to see the code's underlying meaning. This is why many brilliant programmers rarely use debuggers. They look at the code with their eyes and, closing their eyes, visualize the information flowing within it, allowing them to make corrections with ease.