---
layout: post
title: "Misunderstandings about Parser"
---


I have long been aware of the common misunderstandings about parsers, but I have never been motivated enough to express my views on the subject. However, I now feel that it is necessary to clarify this issue. I sense that many people's misunderstandings of parsers are quite deep, and if not addressed, these misconceptions may be recorded in distorted history textbooks, leaving future generations unaware of the truth.

### What is a Parser

First, let me clarify this concept. Generally speaking, a parser refers to the process of converting some text format (string) into a certain data structure. The most common parser is one that converts program text into a compiler's internal data structure called an "abstract syntax tree" (AST). There are also simpler parsers used for processing CSV, JSON, XML, and other formats.

For example, a parser for arithmetic expressions can convert a string like "1+2" that contains the characters 1, +, and 2 into an object. This object is generated like a Java constructor call such as new BinaryExpression(ADD, new Number(1), new Number(2)).: The reason we need to make this conversion from strings to data structures is because compilers cannot directly operate on "1+2" type strings. In reality, the code's essence is not a string, it is a complex data structure, just like a circuit. "1+2" is just a encoding of this data structure. This encoding makes it convenient to store the code on the disk and edit it with text editors. However, it is important to note that text is not the code itself. After reading the text from the disk, you must first "decode" it to conveniently operate on the code's data structure. For instance, if the Java code generates an AST node named node, you can use node.operator to access ADD, node.left to access 1, and node.right to access 2. This is very convenient.

For programming languages, this decoding action is called parsing, and the code used for decoding is called the parser.

### The Role of Parser in Compiler

It seems that the parser plays a crucial role in the compiler then? Parser is indeed indispensable, but it is not as important or difficult as some people imagine. The significance and technical complexity of parsing have been greatly exaggerated by some. Some people equate "compiler" with tools like LEX, YACC, ANTLR used to construct parsers, as if the compiler and parser are identical. Some people even start worshipping someone after they hear they have written a parser. These are shallow displays. I call a parser the "zero step of a marathon journey," as once it is finished, you obtain the Abstract Syntax Tree (AST), and real compilation techniques begin. A compiler consists of numerous steps: semantic analysis, type checking/inference, code optimization, machine code generation, etc. Each step analyzes or transforms certain intermediate data structures (such as AST), without requiring knowledge of the code's string form. Therefore, once the code has passed through the parser, it is possible to completely forget about its existence in the subsequent compilation process. A parser holds a supportive role in a compiler, similar to ZIP in JVM, or JPEG in Photoshop. Although indispensable, a parser is secondary compared to the most crucial processes within the compiler.

Due to this reason, better college-level programming language (PL) courses rarely cover parser-related content. Students often directly use Scheme or similar code data-form languages, or directly use AST data structures to construct programs. In Kent Dybvig's compiler masterclass, students bypass the construction of parsers and start learning the most valuable semantic transformations and optimization techniques instead. In fact, Kent Dybvig himself doesn't consider parsers as part of the compiler. Since the AST data structure is the program itself, and the program's text is just one of its encoded forms.

### Misconceptions in Parser Technology Development

Given that parsers hold a secondary role in compilers, why do people invest so much effort into researching various fancy parser technologies? LL, LR, GLR, Lex, YACC, Bison, parser combinators, ANTLR, PEG, etc., the list of parser creation tools seems endless, with each new tool promising to handle increasingly complex syntax.

People blindly design complex syntax and then use increasingly complex parser technologies to parse them, which is the reason parser technology continues to develop. The desire for complex syntax is a pervasive and harmful misconception in the programming language field, leaving numerous historical relics that have solidified our language design philosophies. Many people designing languages seem to be less concerned with making them useful, and more with confusing or intimidating those who use them. some people assume that mathematics is beautiful language, so they blindly hope that programming languages look like mathematics. Therefore, they imitate mathematics and create various strange operators, set their precedence, allowing you to write code like 2 << 7 - 2 * 3 without adding parentheses to sub-expressions. Many people also like to make syntax "terse", even at the cost of fewer brackets, semicolons, braces, etc. However, the result is complex, inconsistent, ambiguous, hard-to-extend syntax, and confusing, borderline code.

Some go even further, blindly following the foolish ways of mathematics, designing languages like Haskell and Coq. In Haskell, you can define new operators in the code and specify their "associativity" and "precedence". This requires the parser to be able to read and add new parse rules during parsing. Coq attempts to be more "powerful", allowing you to define "mixfix operators", meaning your operators can connect more than two expressions. This lets you define constructs like if...then...else... as "operators".

Designing such complex and confusing syntax serves no real purpose. Not only does it unnecessarily complicate learning for programmers and make code hard to understand, but it also poses a serious challenge for parser authors. However, some people just love creating problems, as a joke goes: "Where there's a problem, there's a need to complicate it!"

If your language syntax is simple (like Scheme), you don't need any advanced parser theory. In essence, you only need to know how to parse matching parentheses. With a little effort, I could write a parser for a language similar to Scheme in an hour or so, using just a few hundred lines of Java code.

However, some people are never satisfied with the difficulty level, so they keep creating increasingly complex syntax, even going as far as making their language look different from others to show "innovation". Of course, this requires more complex parser technologies, which appeals to those who enjoy tinkering with complex parser theories.misconceptions about compiler theory in programming communities are largely due to the rote learning of compiler theory courses in universities. Many teachers themselves do not understand the essence of compilers, so they can only teach "textbook knowledge" and "industry practices." In most universities, compiler theory courses are held with a large, authoritative textbook such as "Dragon Book" or "Tiger Book," spending one third to two thirds of a semester's time on writing parsers. Due to the extensive time spent on parsers, many truly valuable contents, such as semantic analysis, code optimization, type inference, static checking, code generation, etc., are overlooked. As a result, many people who have completed compiler theory courses only remember the painful experience of writing parsers.

"Dragon Book" and similar textbooks hold such high esteem in people's minds, being hailed as "classics," yet apart from the initial large sections on parser theory, the rest of the content is mediocre. Most students' feedback is "I don't understand," but due to the lack of better alternatives, its classic status is hard to challenge. The new versions of "Dragon Book" I browsed through added sections on type checking/inference, but the authors' understanding of type theory was evidently limited, resulting in crucial parts being glossed over.

Appel's "Tiger Book" has a slightly higher authoritative level, but it too cannot avoid the "parser pit." I took a compiler course at Cornell using "Tiger Book" as the textbook, spending an entire month wrestling with parsers, unsure of why I was doing such mundane tasks. To make matters worse, the grading system was unreasonably strict, forcing me to drop the course in the end. I also had an odd experience collaborating with one of Appel's students on research.

Therefore, I have always considered myself a "PL (Programming Languages)" major rather than a "compiler" major. The compiler domain places more emphasis on the rote learning of compiler theory, while the PL major focuses more on the essentials. I. Introduction

If you want to truly understand compilation theory, it's best to start with texts from the PL (Programming Languages) field, such as EOPL (Elements of Programming Languages). The PL domain is quite different from the compiler domain. Don't expect the authors of compilers (like LLVM's authors) to design good languages, as they might not understand many aspects of language design. However, understanding PL theory makes compiler-related things seem much simpler. The intricacies of engineering are tedious, but once you grasp the fundamental principles, they become manageable.

II. My Experiences and Tips for Writing Parsers

Although I've told you that writing parsers for overly complex languages is tedious and uninteresting work, there are historical errors that have had far-reaching consequences. So, despite knowing better, you may have to write parsers for popular languages like C++, Java, JavaScript, and Python. In this section, I'll share some tips that might help you write parsers for these languages more easily.

Many people find writing parsers challenging due to the complex syntax resulting from incorrect language design ideas and misconceptions about the nature and true purpose of parsers. People often try to make parsers do things they shouldn't or have unrealistic expectations of parsers. Consequently, they find parsing difficult and error-prone.

1. Use existing parsers whenever possible. Maintaining a parser is a tedious and time-consuming task with a low return on investment. Once a language is modified, your parser must be updated accordingly. If you can find a free parser, it's best not to write your own. Many languages, such as Python and Ruby, now provide parsers in their standard libraries. You can write a small piece of code using that language to call the standard parser, then convert the parsed data into a common data exchange format like JSON. Finally, you can use a generic JSON parser to extract the desired data structures. Using someone else's parser is preferable, but avoid using their original data structures. If the parser's author changes their data structure in a new version, all your code will need modifications. My tip is to create an "AST converter," converting someone else's AST (Abstract Syntax Tree) structure to your own, and then writing other code on your AST structure. If someone else's parser is modified, you can only change the AST converter, with minimal impact on other code. Using someone else's parser comes with some minor inconveniences. For instance, Python-like languages come with built-in parsers that discard some information I need, such as function names' positions. I need to perform some hacks to recover the necessary data. Compared to writing a parser from scratch, these small modifications are still more manageable. However, if you can't find a good parser, you'll have to write one yourself. 1. Many people write parsers and care about so-called "one-pass parser". They try to construct the final AST structure by scanning the code text once. But if you relax this condition and allow multi-pass parsers, it becomes much easier. You can construct a rough tree structure in the first pass using simple methods, then write a recursive tree traversal process to transform certain structures that cannot be determined in the first pass, and finally obtain the correct AST. Trying to parse out the final AST in one pass can be considered as a premature optimization. Some people blindly believe that scanning the code only once is faster than scanning twice. However, since you need to perform complex operations in this one scan, the ultimate performance may not be better than quickly scanning once and then quickly traversing the tree structure generated.

2. Some people try to do things that are not the responsibility of the parser during the parsing process, such as performing some basic semantic checks. Some people let the parser check "use of undefined variables" and other semantic errors, reporting errors and terminating at that time. This approach confuses the roles of parser, causing unnecessary complexity. As I said before, a parser is just a decoder. The parser's job is to generate structured data structures from unstructured character strings. Semantic checks like "use of undefined variables" should be done after the AST has been generated, using separate tree traversals. People often confuse "decoding", "syntax", and "semantics" and write overly complex, inefficient, and hard-to-maintain parsers as a result.

3. Another common misunderstanding is blindly believing in "parser generators" like YACC and ANTLR. In fact, the best parsers, such as the EDG C++ parser, are mostly handwritten in ordinary programming languages rather than automatically generated. This is because parser generators require you to use a specific description language to represent syntax, which is then automatically converted into parser program code. In the conversion process, there is not a strong semantic connection between the special description language and the generated parser code. If the generated parser has bugs, it is difficult to trace back from the generated parser code to the syntax description and find the location and cause of the error. You cannot debug the syntax description because it is just a text file that cannot be run. Therefore, if you really want to write a parser, I recommend that you write it directly in some programming language using regular recursive descent or parser combinator methods. Only handwritten parsers can be easily debugged, and they can output clear, human-readable error messages.

4. Some people are obsessed with BNF format and blindly believe in the differences between "LL", "LR" and other syntactical types, so they often fall into misunderstandings and say "oh no, this syntax is not LL", and then adopt parsers like YACC that use LR parser generators, resulting in significant trouble. Although some syntactical rules may not be LL, their parsers can still be written using simple recursive descent methods. The key here is that the BNF format given in the language specification is not the only way to write parsers. BNF is just a basic reference, it gives you a clear understanding of the syntax, but the actual parser does not have to follow the BNF format strictly. Sometimes you can change the syntax format slightly and still correctly parse the original language. Since many languages have syntax similar to C, you can often write parsers just by looking at some example programs and using your own experience, without relying on the BNF. Recursive descent and parser combinator-written parsers can be very powerful, even surpassing the capabilities of so-called "context-free grammars", because you can do almost anything in the recursive function, so you can even pass context to the recursive function and make decisions based on the context about what to do with the current node. Moreover, because the code provides a lot of context information, if the input code has a syntax error, you can generate very human-friendly error messages based on this information.

5. Some people are fixated on BNF format and blindly believe in the differences between "LL", "LR" and other syntactical types, so they often fall into misunderstandings and say "oh no, this syntax is not LL", and then adopt parsers like YACC that use LR parser generators, resulting in significant trouble. Although some syntactical rules may not be LL, their parsers can still be written using simple recursive descent methods. The key here is that the BNF format given in the language specification is not the only way to write parsers. BNF is just a basic reference, it gives you a clear understanding of the syntax, but the actual parser does not have to follow the BNF format strictly. Sometimes you can change the syntax format slightly and still correctly parse the original language. Since many languages have syntax similar to C, you can often write parsers just by looking at some example programs and using your own experience, without relying on the BNF. Recursive descent and parser combinator-written parsers can be very powerful, even surpassing the capabilities of so-called "context-free grammars", because you can do almost anything in the recursive function, so you can even pass context to the recursive function and make decisions based on the context about what to do with the current node. Furthermore, because the code provides a lot of context information, if the input code has a syntax error, you can generate very human-friendly error messages based on this information.: So you saw, a parser is not a compiler, it's not even an important thing in compiling. There are many fascinating things in programming languages and compilers. Research on parsers is actually about solving non-existent or artificially created problems. The complexity of syntax led to complex parser technologies, causing unnecessary trouble and hassle in the computer world. The misunderstanding of parser writing, excessive engineering and premature optimization often lead to overestimation of the difficulty of writing parsers.


Being able to write a parser is not impressive, it's a tedious task. So if you can write a parser, don't think it's impressive. If you see someone wrote a parser for a certain language, don't show a laughable reverence.